{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f912fb5f-ae2b-439a-8f2f-0ac11345139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c23a282-08f6-4329-801b-b8ccf66dbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_layers = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c793c69-9c04-42fd-bac6-bb3c73839d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_layers = {f'layer{k}': str(v) for v, k in enumerate(returned_layers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855efd31-1cae-403f-80b8-3aef79db8a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2852a444-59d8-4ce0-aad0-13569d94f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "backbone = resnet.__dict__['resnet50']()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbdd2a21-60cf-4349-8ada-3b6743854b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels_stage2 = backbone.inplanes // 8\n",
    "in_channels_stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30b61687-955f-470a-b0ac-4c3c6fc5267c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[256, 512, 1024, 2048]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\n",
    "in_channels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf711eb-0c98-4c57-b222-ebe61884dcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15506b1d-9c7b-4031-8d9a-bf46244c1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd69fd60-241b-4044-abce-5ae9d1d6c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utils import IntermediateLayerGetter\n",
    "body = IntermediateLayerGetter(backbone, return_layers=return_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed7104b-099d-4f26-8967-400abca71333",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,1024,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03ef0f13-d770-4c70-a3cb-30850bc90a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon/anaconda3/envs/pytorch_retina/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "out = body(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "545be8a2-e86a-43b7-98ac-030a4075ef20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0', '1', '2', '3'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1dcd842-ff75-4a20-8d2e-547e7291b5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2048, 32, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['3'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc57ae90-4419-46d9-832a-6c7a53d1dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelMaxPool\n",
    "fpn = FeaturePyramidNetwork(\n",
    "    in_channels_list=in_channels_list,\n",
    "    out_channels=256,\n",
    "    extra_blocks=LastLevelMaxPool(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e41aae1-b784-4375-8fdd-a9a9d2b11462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_rcnn import fasterrcnn_resnet50_fpn\n",
    "from model import fasterrcnn_resnet101_fpn\n",
    "model = fasterrcnn_resnet101_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb747d76-0cd1-48d4-a630-12177b3fe0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 2048.0\n",
      "size 2048.0\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(2, 3, 2048,2048)\n",
    "target = ({'boxes': torch.tensor([[1103.1410,  216.3810, 1593.6943,  389.0794]]),\n",
    "  'labels': torch.tensor([1])},\n",
    " {'boxes': torch.tensor([[ 141., 1132.,  364., 1314.]]), 'labels': torch.tensor([1])},\n",
    " {'boxes': torch.tensor([[132.6265, 526.8099, 215.9036, 599.8016]]),\n",
    "  'labels': torch.tensor([1])},\n",
    " {'boxes': torch.tensor([[1073.2058, 1598.3280, 1293.8553, 1769.0895]]),\n",
    "  'labels': torch.tensor([1])})\n",
    "\n",
    "loss_dict = model(input,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fa9ff25-61ed-49d3-8bc3-f78211f6a91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4219862222671509"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(1).uniform_(0., float(1)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c3e55af-3955-4776-9195-bce4f1728d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mbeomgon-B360-M-AORUS-PRO     \u001b[m  Thu Sep  2 15:33:26 2021  \u001b[1m\u001b[30m465.19.01\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 2080 Ti\u001b[m |\u001b[31m 36'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  743\u001b[m / \u001b[33m11016\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m26M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m91M\u001b[m) \u001b[1m\u001b[30mbeomgon\u001b[m(\u001b[33m273M\u001b[m) \u001b[1m\u001b[30mbeomgon\u001b[m(\u001b[33m179M\u001b[m) \u001b[1m\u001b[30mbeomgon\u001b[m(\u001b[33m23M\u001b[m) \u001b[1m\u001b[30mbeomgon\u001b[m(\u001b[33m143M\u001b[m)\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49017329-2f19-4d8d-8140-e8eaa310cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70c2abfb-c62a-4658-acbe-87eb10415b92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fff88dd7bfb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGeneralizedRCNNTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mPerforms\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mtransformation\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mfeeding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mGeneralizedRCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class GeneralizedRCNNTransform(nn.Module):\n",
    "    \"\"\"\n",
    "    Performs input / target transformation before feeding the data to a GeneralizedRCNN\n",
    "    model.\n",
    "\n",
    "    The transformations it perform are:\n",
    "        - input normalization (mean subtraction and std division)\n",
    "        - input / target resizing to match min_size / max_size\n",
    "\n",
    "    It returns a ImageList for the inputs, and a List[Dict[Tensor]] for the targets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_size, max_size, image_mean, image_std, size_divisible=32, fixed_size=None):\n",
    "        super(GeneralizedRCNNTransform, self).__init__()\n",
    "        if not isinstance(min_size, (list, tuple)):\n",
    "            min_size = (min_size,)\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "        self.image_mean = image_mean\n",
    "        self.image_std = image_std\n",
    "        self.size_divisible = size_divisible\n",
    "        self.fixed_size = fixed_size\n",
    "\n",
    "    def forward(self,\n",
    "                images,       # type: List[Tensor]\n",
    "                targets=None  # type: Optional[List[Dict[str, Tensor]]]\n",
    "                ):\n",
    "        # type: (...) -> Tuple[ImageList, Optional[List[Dict[str, Tensor]]]]\n",
    "        images = [img for img in images]\n",
    "        if targets is not None:\n",
    "            # make a copy of targets to avoid modifying it in-place\n",
    "            # once torchscript supports dict comprehension\n",
    "            # this can be simplified as follows\n",
    "            # targets = [{k: v for k,v in t.items()} for t in targets]\n",
    "            targets_copy: List[Dict[str, Tensor]] = []\n",
    "            for t in targets:\n",
    "                data: Dict[str, Tensor] = {}\n",
    "                for k, v in t.items():\n",
    "                    data[k] = v\n",
    "                targets_copy.append(data)\n",
    "            targets = targets_copy\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            target_index = targets[i] if targets is not None else None\n",
    "\n",
    "            if image.dim() != 3:\n",
    "                raise ValueError(\"images is expected to be a list of 3d tensors \"\n",
    "                                 \"of shape [C, H, W], got {}\".format(image.shape))\n",
    "            image = self.normalize(image)\n",
    "            image, target_index = self.resize(image, target_index)\n",
    "            images[i] = image\n",
    "            if targets is not None and target_index is not None:\n",
    "                targets[i] = target_index\n",
    "\n",
    "        image_sizes = [img.shape[-2:] for img in images]\n",
    "        images = self.batch_images(images, size_divisible=self.size_divisible)\n",
    "        image_sizes_list: List[Tuple[int, int]] = []\n",
    "        for image_size in image_sizes:\n",
    "            assert len(image_size) == 2\n",
    "            image_sizes_list.append((image_size[0], image_size[1]))\n",
    "\n",
    "        image_list = ImageList(images, image_sizes_list)\n",
    "        return image_list, targets\n",
    "\n",
    "    def normalize(self, image):\n",
    "        if not image.is_floating_point():\n",
    "            raise TypeError(\n",
    "                f\"Expected input images to be of floating type (in range [0, 1]), \"\n",
    "                f\"but found type {image.dtype} instead\"\n",
    "            )\n",
    "        dtype, device = image.dtype, image.device\n",
    "        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device)\n",
    "        std = torch.as_tensor(self.image_std, dtype=dtype, device=device)\n",
    "        return (image - mean[:, None, None]) / std[:, None, None]\n",
    "\n",
    "    def torch_choice(self, k):\n",
    "        # type: (List[int]) -> int\n",
    "        \"\"\"\n",
    "        Implements `random.choice` via torch ops so it can be compiled with\n",
    "        TorchScript. Remove if https://github.com/pytorch/pytorch/issues/25803\n",
    "        is fixed.\n",
    "        \"\"\"\n",
    "        index = int(torch.empty(1).uniform_(0., float(len(k))).item())\n",
    "        return k[index]\n",
    "\n",
    "    def resize(self,\n",
    "               image: Tensor,\n",
    "               target: Optional[Dict[str, Tensor]] = None,\n",
    "               ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
    "        h, w = image.shape[-2:]\n",
    "        if self.training:\n",
    "            size = float(self.torch_choice(self.min_size))\n",
    "        else:\n",
    "            # FIXME assume for now that testing uses the largest scale\n",
    "            size = float(self.min_size[-1])\n",
    "        image, target = _resize_image_and_masks(image, size, float(self.max_size), target, self.fixed_size)\n",
    "\n",
    "        if target is None:\n",
    "            return image, target\n",
    "\n",
    "        bbox = target[\"boxes\"]\n",
    "        bbox = resize_boxes(bbox, (h, w), image.shape[-2:])\n",
    "        target[\"boxes\"] = bbox\n",
    "\n",
    "        if \"keypoints\" in target:\n",
    "            keypoints = target[\"keypoints\"]\n",
    "            keypoints = resize_keypoints(keypoints, (h, w), image.shape[-2:])\n",
    "            target[\"keypoints\"] = keypoints\n",
    "        return image, target\n",
    "\n",
    "    # _onnx_batch_images() is an implementation of\n",
    "    # batch_images() that is supported by ONNX tracing.\n",
    "    @torch.jit.unused\n",
    "    def _onnx_batch_images(self, images, size_divisible=32):\n",
    "        # type: (List[Tensor], int) -> Tensor\n",
    "        max_size = []\n",
    "        for i in range(images[0].dim()):\n",
    "            max_size_i = torch.max(torch.stack([img.shape[i] for img in images]).to(torch.float32)).to(torch.int64)\n",
    "            max_size.append(max_size_i)\n",
    "        stride = size_divisible\n",
    "        max_size[1] = (torch.ceil((max_size[1].to(torch.float32)) / stride) * stride).to(torch.int64)\n",
    "        max_size[2] = (torch.ceil((max_size[2].to(torch.float32)) / stride) * stride).to(torch.int64)\n",
    "        max_size = tuple(max_size)\n",
    "\n",
    "        # work around for\n",
    "        # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "        # which is not yet supported in onnx\n",
    "        padded_imgs = []\n",
    "        for img in images:\n",
    "            padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n",
    "            padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n",
    "            padded_imgs.append(padded_img)\n",
    "\n",
    "        return torch.stack(padded_imgs)\n",
    "\n",
    "    def max_by_axis(self, the_list):\n",
    "        # type: (List[List[int]]) -> List[int]\n",
    "        maxes = the_list[0]\n",
    "        for sublist in the_list[1:]:\n",
    "            for index, item in enumerate(sublist):\n",
    "                maxes[index] = max(maxes[index], item)\n",
    "        return maxes\n",
    "\n",
    "    def batch_images(self, images, size_divisible=32):\n",
    "        # type: (List[Tensor], int) -> Tensor\n",
    "        if torchvision._is_tracing():\n",
    "            # batch_images() does not export well to ONNX\n",
    "            # call _onnx_batch_images() instead\n",
    "            return self._onnx_batch_images(images, size_divisible)\n",
    "\n",
    "        max_size = self.max_by_axis([list(img.shape) for img in images])\n",
    "        stride = float(size_divisible)\n",
    "        max_size = list(max_size)\n",
    "        max_size[1] = int(math.ceil(float(max_size[1]) / stride) * stride)\n",
    "        max_size[2] = int(math.ceil(float(max_size[2]) / stride) * stride)\n",
    "\n",
    "        batch_shape = [len(images)] + max_size\n",
    "        batched_imgs = images[0].new_full(batch_shape, 0)\n",
    "        for img, pad_img in zip(images, batched_imgs):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "\n",
    "        return batched_imgs\n",
    "\n",
    "    def postprocess(self,\n",
    "                    result,               # type: List[Dict[str, Tensor]]\n",
    "                    image_shapes,         # type: List[Tuple[int, int]]\n",
    "                    original_image_sizes  # type: List[Tuple[int, int]]\n",
    "                    ):\n",
    "        # type: (...) -> List[Dict[str, Tensor]]\n",
    "        if self.training:\n",
    "            return result\n",
    "        for i, (pred, im_s, o_im_s) in enumerate(zip(result, image_shapes, original_image_sizes)):\n",
    "            boxes = pred[\"boxes\"]\n",
    "            boxes = resize_boxes(boxes, im_s, o_im_s)\n",
    "            result[i][\"boxes\"] = boxes\n",
    "            if \"masks\" in pred:\n",
    "                masks = pred[\"masks\"]\n",
    "                masks = paste_masks_in_image(masks, boxes, o_im_s)\n",
    "                result[i][\"masks\"] = masks\n",
    "            if \"keypoints\" in pred:\n",
    "                keypoints = pred[\"keypoints\"]\n",
    "                keypoints = resize_keypoints(keypoints, im_s, o_im_s)\n",
    "                result[i][\"keypoints\"] = keypoints\n",
    "        return result\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        _indent = '\\n    '\n",
    "        format_string += \"{0}Normalize(mean={1}, std={2})\".format(_indent, self.image_mean, self.image_std)\n",
    "        format_string += \"{0}Resize(min_size={1}, max_size={2}, mode='bilinear')\".format(_indent, self.min_size,\n",
    "                                                                                         self.max_size)\n",
    "        format_string += '\\n)'\n",
    "        return format_string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_retina",
   "language": "python",
   "name": "pytorch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
