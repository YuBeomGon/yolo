{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138ac810-4a3f-4af9-92a3-7b93727ce420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/FrancescoSaverioZuppichini/ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042502bc-59ee-4b85-8be9-e3c12112f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, transforms\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from config import BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d335c01-d4a2-4326-8093-532f6d950741",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b282bc86-ebc9-4580-96dc-6dccb1127dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece8b269-21b1-4ab3-a0a4-8f6b19a8d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 2048, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img =cv2.imread('../cat.jpg')\n",
    "img =cv2.imread('../images/paps.png')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f370def4-262e-4080-8e76-68ec73bc5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open('./cat.jpg')\n",
    "# # print(img.shape)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b79b5af4-b1be-429f-862a-9ea3c75e8d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1568, 1568])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize to imagenet size \n",
    "transform = Compose([transforms.ToPILImage(), Resize((1568, 1568)), ToTensor()])\n",
    "x = transform(img)\n",
    "\n",
    "x = x.unsqueeze(0) # add batch dim\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a018274-e1ab-43bf-81f3-ea7b7381b738",
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b c (h s1) (w s2) -> b (h w) (s1 s2 c)\".\n Input tensor shape: torch.Size([1, 3, 1568, 1568]). Additional info: {'s1': 128, 's2': 128}.\n Shape mismatch, can't divide axis of length 1568 in chunks of 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_retina/lib/python3.7/site-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mrecipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_transformation_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhashable_axes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mEinopsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_retina/lib/python3.7/site-packages/einops/einops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    204\u001b[0m         init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes = self.reconstruct_from_shape(\n\u001b[0;32m--> 205\u001b[0;31m             backend.shape(tensor))\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_retina/lib/python3.7/site-packages/einops/einops.py\u001b[0m in \u001b[0;36mreconstruct_from_shape\u001b[0;34m(self, shape, optimize)\u001b[0m\n\u001b[1;32m    175\u001b[0m                         raise EinopsError(\"Shape mismatch, can't divide axis of length {} in chunks of {}\".format(\n\u001b[0;32m--> 176\u001b[0;31m                             length, known_product))\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEinopsError\u001b[0m: Shape mismatch, can't divide axis of length 1568 in chunks of 128",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e56697c91016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m \u001b[0;31m# 16 pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b c (h s1) (w s2) -> b (h w) (s1 s2 c)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_retina/lib/python3.7/site-packages/einops/einops.py\u001b[0m in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rearrange can't be applied to an empty list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_on_zeroth_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rearrange'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_retina/lib/python3.7/site-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n Input is list. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'Additional info: {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEinopsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"b c (h s1) (w s2) -> b (h w) (s1 s2 c)\".\n Input tensor shape: torch.Size([1, 3, 1568, 1568]). Additional info: {'s1': 128, 's2': 128}.\n Shape mismatch, can't divide axis of length 1568 in chunks of 128"
     ]
    }
   ],
   "source": [
    "# patch_size = 128 # 16 pixels\n",
    "# patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)\n",
    "# patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34aac3d1-d502-4b53-83c7-e60f0e39c570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9604, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(\n",
    "            # break-down the image in s1 x s2 patches and flat them\n",
    "            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
    "        )\n",
    "                \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "    \n",
    "PatchEmbedding()(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c34dc58-2a73-4f23-8980-f7e7138d6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_size = 8\n",
    "# emb_size=64\n",
    "# conv1 = nn.Conv2d(in_channels=3, out_channels=emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "# print(conv1(x).shape)\n",
    "# conv2 = nn.Conv2d(in_channels=emb_size, out_channels=emb_size*2, kernel_size=8, stride=8)\n",
    "# conv2(conv1(x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c81da71-ad19-4d63-92c4-be816456ff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 384])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 384):\n",
    "        super().__init__()\n",
    "        self.image_size = 1568\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_size_2nd = 8\n",
    "        self.stride_2nd = 6\n",
    "        self.projection = nn.Sequential(\n",
    "            # using a conv layer instead of a linear one -> performance gains\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=int(emb_size/3), \n",
    "                      kernel_size=patch_size, stride=patch_size),\n",
    "            nn.Conv2d(in_channels=int(emb_size/3), out_channels=emb_size, \n",
    "                      kernel_size=self.patch_size_2nd, stride=self.stride_2nd), #128 patch size, 32 stride\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn(((self.image_size - patch_size*self.patch_size_2nd) \\\n",
    "                                                   // (patch_size*self.stride_2nd) +1 ) **2 + 1,\n",
    "                                                  emb_size))\n",
    "                \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)   \n",
    "#         print(x.shape)\n",
    "#         print(self.positions.shape)\n",
    "        x += self.positions\n",
    "        return x\n",
    "    \n",
    "PatchEmbedding()(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a823eb-bd4f-4659-a9ce-7e8facebb5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd29ec3-243a-4f90-8613-0ff26e2342b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 384])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 384, num_heads: int = 6, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "#         self.keys = nn.Linear(emb_size, emb_size)\n",
    "#         self.queries = nn.Linear(emb_size, emb_size)\n",
    "#         self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        self.scaling = (self.emb_size // num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "#         queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "#         keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "#         values  = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "#         # sum up over the last axis\n",
    "#         energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "#         if mask is not None:\n",
    "#             fill_value = torch.finfo(torch.float32).min\n",
    "#             energy.mask_fill(~mask, fill_value)\n",
    "        \n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)        \n",
    "            \n",
    "        energy /= self.scaling            \n",
    "#         att = F.softmax(energy, dim=-1) * self.scaling\n",
    "        att = F.softmax(energy, dim=-1)\n",
    "        if self.att_drop is not None:\n",
    "            att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "print(patches_embedded.shape)\n",
    "MultiHeadAttention()(patches_embedded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cf53a30-10ec-48b5-85ae-2484727372b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "011be298-e4f6-4eaf-b109-43cb2b0ff25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28c8712a-ea68-4aef-ab50-889b72c191af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertLayer(nn.Sequential):\n",
    "class BertLayer(nn.Module):    \n",
    "    def __init__(self,\n",
    "                 emb_size: int = 384,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__()\n",
    "        self.mlp = FeedForwardBlock(emb_size, forward_expansion, drop_p)\n",
    "        self.mha = MultiHeadAttention(emb_size=384, num_heads=6, dropout=0)\n",
    "        self.layernorm_mlp = nn.LayerNorm(emb_size)\n",
    "        self.layernorm_mha = nn.LayerNorm(emb_size)\n",
    "        self.dropout_mlp = nn.Dropout(drop_p)\n",
    "        self.dropout_mha = nn.Dropout(drop_p)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        skipped = x\n",
    "        x = self.layernorm_mha(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.dropout_mha(x)\n",
    "        x += skipped\n",
    "        \n",
    "        skipped = x\n",
    "        x = self.layernorm_mlp(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.dropout_mlp(x)\n",
    "        x += skipped\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83fe04f2-f36f-4fc2-ab79-2d69c37824d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03f2bdfe-b6d0-494c-ad95-e26d92992908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer() for _ in range(config.num_hidden_layers)])\n",
    "        self.embeding = PatchEmbedding()\n",
    "        \n",
    "    def forward (self, x : Tensor) -> Tensor :\n",
    "        x = self.embeding(x)\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            x = layer_module(x)\n",
    "            \n",
    "        return x\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b85ebca-cfd2-4938-ac5f-37c44e3e4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,3,1568,1568)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e5cbbb4-921a-4e23-9acc-fedec156e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BertEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1d534c7-d069-469b-bed0-0ee48584452a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 257, 384])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a7b27-cf3c-46a0-9270-7f2049aa7d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10b41f-3826-4c83-8779-48517870cf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3397d-f729-45c0-ac46-a4630424434a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e68a8-b0e5-400c-806f-f91bb538fb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "848d0722-2210-4388-b998-bbb3b31cd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9daa4dba-5672-4a21-a597-1fcfedce1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 384,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "813bbcaf-94a0-4e5e-8f01-578312ef1a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 384])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_embedded = PatchEmbedding()(x)\n",
    "TransformerEncoderBlock()(patches_embedded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "976a63eb-7629-4992-8864-51497acf2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 6, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e31aa3b0-41a7-47df-822a-7e93a2da5698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 384, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d5bb4c49-d532-4c6b-88b3-87a1045954aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 384,\n",
    "                img_size: int = 1568,\n",
    "                depth: int = 6,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d6126f77-97fb-42d2-8089-68e681d3d10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 98, 98]          98,432\n",
      "            Conv2d-2          [-1, 384, 16, 16]       3,146,112\n",
      "         Rearrange-3             [-1, 256, 384]               0\n",
      "    PatchEmbedding-4             [-1, 257, 384]               0\n",
      "         LayerNorm-5             [-1, 257, 384]             768\n",
      "            Linear-6            [-1, 257, 1152]         443,520\n",
      "           Dropout-7          [-1, 6, 257, 257]               0\n",
      "            Linear-8             [-1, 257, 384]         147,840\n",
      "MultiHeadAttention-9             [-1, 257, 384]               0\n",
      "          Dropout-10             [-1, 257, 384]               0\n",
      "      ResidualAdd-11             [-1, 257, 384]               0\n",
      "        LayerNorm-12             [-1, 257, 384]             768\n",
      "           Linear-13            [-1, 257, 1536]         591,360\n",
      "             GELU-14            [-1, 257, 1536]               0\n",
      "          Dropout-15            [-1, 257, 1536]               0\n",
      "           Linear-16             [-1, 257, 384]         590,208\n",
      "          Dropout-17             [-1, 257, 384]               0\n",
      "      ResidualAdd-18             [-1, 257, 384]               0\n",
      "        LayerNorm-19             [-1, 257, 384]             768\n",
      "           Linear-20            [-1, 257, 1152]         443,520\n",
      "          Dropout-21          [-1, 6, 257, 257]               0\n",
      "           Linear-22             [-1, 257, 384]         147,840\n",
      "MultiHeadAttention-23             [-1, 257, 384]               0\n",
      "          Dropout-24             [-1, 257, 384]               0\n",
      "      ResidualAdd-25             [-1, 257, 384]               0\n",
      "        LayerNorm-26             [-1, 257, 384]             768\n",
      "           Linear-27            [-1, 257, 1536]         591,360\n",
      "             GELU-28            [-1, 257, 1536]               0\n",
      "          Dropout-29            [-1, 257, 1536]               0\n",
      "           Linear-30             [-1, 257, 384]         590,208\n",
      "          Dropout-31             [-1, 257, 384]               0\n",
      "      ResidualAdd-32             [-1, 257, 384]               0\n",
      "        LayerNorm-33             [-1, 257, 384]             768\n",
      "           Linear-34            [-1, 257, 1152]         443,520\n",
      "          Dropout-35          [-1, 6, 257, 257]               0\n",
      "           Linear-36             [-1, 257, 384]         147,840\n",
      "MultiHeadAttention-37             [-1, 257, 384]               0\n",
      "          Dropout-38             [-1, 257, 384]               0\n",
      "      ResidualAdd-39             [-1, 257, 384]               0\n",
      "        LayerNorm-40             [-1, 257, 384]             768\n",
      "           Linear-41            [-1, 257, 1536]         591,360\n",
      "             GELU-42            [-1, 257, 1536]               0\n",
      "          Dropout-43            [-1, 257, 1536]               0\n",
      "           Linear-44             [-1, 257, 384]         590,208\n",
      "          Dropout-45             [-1, 257, 384]               0\n",
      "      ResidualAdd-46             [-1, 257, 384]               0\n",
      "        LayerNorm-47             [-1, 257, 384]             768\n",
      "           Linear-48            [-1, 257, 1152]         443,520\n",
      "          Dropout-49          [-1, 6, 257, 257]               0\n",
      "           Linear-50             [-1, 257, 384]         147,840\n",
      "MultiHeadAttention-51             [-1, 257, 384]               0\n",
      "          Dropout-52             [-1, 257, 384]               0\n",
      "      ResidualAdd-53             [-1, 257, 384]               0\n",
      "        LayerNorm-54             [-1, 257, 384]             768\n",
      "           Linear-55            [-1, 257, 1536]         591,360\n",
      "             GELU-56            [-1, 257, 1536]               0\n",
      "          Dropout-57            [-1, 257, 1536]               0\n",
      "           Linear-58             [-1, 257, 384]         590,208\n",
      "          Dropout-59             [-1, 257, 384]               0\n",
      "      ResidualAdd-60             [-1, 257, 384]               0\n",
      "        LayerNorm-61             [-1, 257, 384]             768\n",
      "           Linear-62            [-1, 257, 1152]         443,520\n",
      "          Dropout-63          [-1, 6, 257, 257]               0\n",
      "           Linear-64             [-1, 257, 384]         147,840\n",
      "MultiHeadAttention-65             [-1, 257, 384]               0\n",
      "          Dropout-66             [-1, 257, 384]               0\n",
      "      ResidualAdd-67             [-1, 257, 384]               0\n",
      "        LayerNorm-68             [-1, 257, 384]             768\n",
      "           Linear-69            [-1, 257, 1536]         591,360\n",
      "             GELU-70            [-1, 257, 1536]               0\n",
      "          Dropout-71            [-1, 257, 1536]               0\n",
      "           Linear-72             [-1, 257, 384]         590,208\n",
      "          Dropout-73             [-1, 257, 384]               0\n",
      "      ResidualAdd-74             [-1, 257, 384]               0\n",
      "        LayerNorm-75             [-1, 257, 384]             768\n",
      "           Linear-76            [-1, 257, 1152]         443,520\n",
      "          Dropout-77          [-1, 6, 257, 257]               0\n",
      "           Linear-78             [-1, 257, 384]         147,840\n",
      "MultiHeadAttention-79             [-1, 257, 384]               0\n",
      "          Dropout-80             [-1, 257, 384]               0\n",
      "      ResidualAdd-81             [-1, 257, 384]               0\n",
      "        LayerNorm-82             [-1, 257, 384]             768\n",
      "           Linear-83            [-1, 257, 1536]         591,360\n",
      "             GELU-84            [-1, 257, 1536]               0\n",
      "          Dropout-85            [-1, 257, 1536]               0\n",
      "           Linear-86             [-1, 257, 384]         590,208\n",
      "          Dropout-87             [-1, 257, 384]               0\n",
      "      ResidualAdd-88             [-1, 257, 384]               0\n",
      "           Reduce-89                  [-1, 384]               0\n",
      "        LayerNorm-90                  [-1, 384]             768\n",
      "           Linear-91                 [-1, 1000]         385,000\n",
      "================================================================\n",
      "Total params: 14,277,096\n",
      "Trainable params: 14,277,096\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 28.14\n",
      "Forward/backward pass size (MB): 138.21\n",
      "Params size (MB): 54.46\n",
      "Estimated Total Size (MB): 220.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(ViT(), (3, 1568, 1568), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2fbdd2-b30e-4942-aa5a-46f2703e38cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_retina",
   "language": "python",
   "name": "pytorch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
