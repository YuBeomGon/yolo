{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ab6313-698a-46cc-9ab4-770b35f86211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# import random\n",
    "# import shutil\n",
    "# import time\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.parallel\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# import torch.distributed as dist\n",
    "import torch.optim\n",
    "# import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "# import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "# import torchvision.models as models\n",
    "from resnet import *\n",
    "\n",
    "from main import *\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56508d67-5301-4e1e-9454-1fe41b5f3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args(args=[])\n",
    "# args = parser.parse_args()\n",
    "import easydict \n",
    "args = easydict.EasyDict({ \"batch-size\": 256, \n",
    "                          \"epochs\": 100, \n",
    "                          \"data\": 0, \n",
    "                          'arch':'resnet18',\n",
    "                          'lr':0.1,\n",
    "                         'momentum':0.9,\n",
    "                         'weight_decay':1e-4,\n",
    "                         'start_epoch':0,\n",
    "                         'gpu':2,\n",
    "                         'saved_dir':'../trained_model/sent2vec/model_best.pt'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea598abf-d5fe-4f70-8cbf-78ad6038e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "cuda:2\n",
      "Current cuda device  2\n"
     ]
    }
   ],
   "source": [
    "ngpus_per_node = torch.cuda.device_count()\n",
    "print(ngpus_per_node)\n",
    "# device = torch.device('cpu')\n",
    "# device = torch.device('cuda')\n",
    "GPU_NUM = args.gpu # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "print(device)\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "#4번 디바이스만 이용하려면 \"4\"를 입력\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print ('Current cuda device ', torch.cuda.current_device()) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b5c549-7611-40a8-a5cd-54451c41e793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "imagenet_embeding = np.load('../data/imagenet_embeding.npy')\n",
    "imagenet_embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5e59a2-80d4-49f7-8998-7b78846b333f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet_embeding = torch.tensor(imagenet_embeding)\n",
    "imagenet_embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e9ebbf-4193-412c-b416-9faccf6aac25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet18'\n"
     ]
    }
   ],
   "source": [
    "print(\"=> using pre-trained model '{}'\".format('resnet18'))\n",
    "# model = models.__dict__['resnet18'](pretrained=True)\n",
    "# model = models.resnet18(pretrained=False)\n",
    "model = resnet18(pretrained=False)\n",
    "# model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                            momentum=args.momentum,\n",
    "                            weight_decay=args.weight_decay)\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf479d7-0b16-4399-8f59-fd72bec730d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0207, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc8e115d-c36b-4e00-8f16-9473415528af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), '../trained_model/init.pt')\n",
    "model.load_state_dict(torch.load('../trained_model/init.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a75cef50-6d29-411a-a565-898a4468242d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0301, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8910807d-3862-4446-a811-530857664cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 768])\n",
      "tensor([[-0.0301,  0.0198, -0.0104,  ..., -0.0012, -0.0136,  0.0011],\n",
      "        [-0.0023, -0.0009, -0.0329,  ..., -0.0281, -0.0132,  0.0052],\n",
      "        [ 0.0036, -0.0155, -0.0112,  ..., -0.0055,  0.0028,  0.0204],\n",
      "        ...,\n",
      "        [-0.0349, -0.0049, -0.0264,  ..., -0.0168, -0.0148,  0.0045],\n",
      "        [ 0.0063, -0.0293, -0.0185,  ..., -0.0337, -0.0108,  0.0270],\n",
      "        [-0.0275, -0.0346,  0.0034,  ..., -0.0311,  0.0181,  0.0030]])\n",
      "tensor([[ 0.5611,  0.2227,  0.2527,  ..., -0.1777,  0.2208,  0.6253],\n",
      "        [ 0.0625, -0.3026, -0.2862,  ..., -0.0785, -0.1146, -0.1183],\n",
      "        [ 0.1750,  0.1851, -0.3477,  ..., -0.3395, -0.0346, -0.3034],\n",
      "        ...,\n",
      "        [-0.3333,  0.0872, -0.0674,  ...,  0.2487, -0.1052,  0.0210],\n",
      "        [-0.0106,  0.0222,  0.2349,  ..., -0.4712, -0.5066,  0.3787],\n",
      "        [ 0.6112,  0.1495, -0.0799,  ..., -0.2983, -0.5161,  0.1615]])\n"
     ]
    }
   ],
   "source": [
    "model_dict = model.state_dict() \n",
    "for k in model_dict :\n",
    "    if 'fc.weight' in k :\n",
    "        print(model_dict[k].shape)\n",
    "        print(model_dict[k])\n",
    "        model_dict[k] = imagenet_embeding\n",
    "#     if 'fc.bias' in k :\n",
    "#         print(model_dict[k])        \n",
    "#     print(model_dict[k].shape)\n",
    "model.load_state_dict(model_dict)\n",
    "model_dict = model.state_dict() \n",
    "for k in model_dict :\n",
    "    if 'fc.weight' in k :\n",
    "#         print(model_dict[k].shape)\n",
    "        print(model_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66335ead-2157-4014-8671-62dbab63a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7785cdb-e3f2-4350-8702-f70a6ee3c877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5611)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77598382-abf2-459f-9457-f8bdd1cf8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters() :\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5493c8ea-4540-4067-8954-9e7a73cb10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6cd832a-5720-4925-9d50-36b16480132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "data_dir = '../ILSVRC/Data/CLS-LOC/'\n",
    "traindir = os.path.join(data_dir, 'train')\n",
    "valdir = os.path.join(data_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffb79f3f-e8a8-4e0d-9a55-f7dbf5c08236",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3d9c94c-b533-4147-96f0-57afbf42624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4460bdaa-261a-4858-a94c-c70aebd6fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "train_sampler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beece8cc-7ac9-4e30-8cca-0ca4adbd58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=256, shuffle=(train_sampler is None),\n",
    "    num_workers=8, pin_memory=True, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53171b34-4e2f-4b1b-ae3f-d8d2db58b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32, shuffle=False,\n",
    "    num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afed6352-ebb9-4fcf-ade1-12354b4e7755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(384, 768, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25092250-abbb-482c-bb3a-3c279e593859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon/anaconda3/envs/torch_retina/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][   0/5005]\tTime 60.628 (60.628)\tData 35.007 (35.007)\tLoss 9.3841e+00 (9.3841e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\n",
      "Epoch: [0][1001/5005]\tTime  0.383 ( 1.903)\tData  0.000 ( 1.345)\tLoss 6.7360e+00 (7.1124e+00)\tAcc@1   0.39 (  0.37)\tAcc@5   3.52 (  1.61)\n",
      "Epoch: [0][2002/5005]\tTime  0.965 ( 2.139)\tData  0.825 ( 1.482)\tLoss 6.3227e+00 (6.8376e+00)\tAcc@1   0.78 (  0.60)\tAcc@5   6.25 (  2.45)\n",
      "Epoch: [0][3003/5005]\tTime  0.381 ( 2.393)\tData  0.000 ( 1.618)\tLoss 6.0444e+00 (6.6577e+00)\tAcc@1   1.95 (  0.90)\tAcc@5   7.81 (  3.47)\n",
      "Epoch: [0][4004/5005]\tTime  0.373 ( 2.321)\tData  0.000 ( 1.590)\tLoss 6.0229e+00 (6.5164e+00)\tAcc@1   3.12 (  1.27)\tAcc@5   8.20 (  4.61)\n",
      " * Acc@1 4.140 Acc@5 12.852\n",
      "************train_loss 6.397014231305499 val_acc 4.139999866485596*************\n",
      "Epoch: [1][   0/5005]\tTime 34.130 (34.130)\tData 21.707 (21.707)\tLoss 5.9861e+00 (5.9861e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   8.98 (  8.98)\n",
      "Epoch: [1][1001/5005]\tTime 14.335 ( 2.501)\tData 14.187 ( 1.640)\tLoss 5.5873e+00 (5.7499e+00)\tAcc@1   4.30 (  4.38)\tAcc@5  15.23 ( 13.02)\n",
      "Epoch: [1][2002/5005]\tTime 10.479 ( 2.808)\tData 10.337 ( 1.806)\tLoss 5.4346e+00 (5.6686e+00)\tAcc@1   5.47 (  4.96)\tAcc@5  16.80 ( 14.33)\n",
      "Epoch: [1][3003/5005]\tTime  0.384 ( 2.649)\tData  0.000 ( 1.765)\tLoss 5.3058e+00 (5.5834e+00)\tAcc@1   8.59 (  5.65)\tAcc@5  20.70 ( 15.82)\n",
      "Epoch: [1][4004/5005]\tTime 11.992 ( 2.695)\tData 11.845 ( 1.780)\tLoss 4.9387e+00 (5.4931e+00)\tAcc@1  12.50 (  6.45)\tAcc@5  29.30 ( 17.39)\n",
      " * Acc@1 10.056 Acc@5 24.660\n",
      "************train_loss 5.400300355486341 val_acc 10.055999755859375*************\n",
      "Epoch: [2][   0/5005]\tTime 145.025 (145.025)\tData 68.345 (68.345)\tLoss 4.7102e+00 (4.7102e+00)\tAcc@1  14.06 ( 14.06)\tAcc@5  29.69 ( 29.69)\n",
      "Epoch: [2][1001/5005]\tTime  0.381 ( 2.981)\tData  0.000 ( 1.845)\tLoss 4.5873e+00 (4.8435e+00)\tAcc@1  14.06 ( 12.62)\tAcc@5  32.42 ( 29.02)\n",
      "Epoch: [2][2002/5005]\tTime  0.380 ( 2.509)\tData  0.000 ( 1.634)\tLoss 4.5655e+00 (4.7571e+00)\tAcc@1  14.84 ( 13.65)\tAcc@5  34.38 ( 30.65)\n",
      "Epoch: [2][3003/5005]\tTime  0.375 ( 2.691)\tData  0.000 ( 1.774)\tLoss 4.4047e+00 (4.6723e+00)\tAcc@1  15.23 ( 14.63)\tAcc@5  37.11 ( 32.21)\n",
      "Epoch: [2][4004/5005]\tTime  0.384 ( 2.694)\tData  0.000 ( 1.768)\tLoss 4.4297e+00 (4.5919e+00)\tAcc@1  17.19 ( 15.58)\tAcc@5  34.77 ( 33.70)\n",
      " * Acc@1 20.810 Acc@5 42.588\n",
      "************train_loss 4.516771338012192 val_acc 20.809999465942383*************\n",
      "Epoch: [3][   0/5005]\tTime 55.662 (55.662)\tData 34.627 (34.627)\tLoss 4.1167e+00 (4.1167e+00)\tAcc@1  21.48 ( 21.48)\tAcc@5  43.75 ( 43.75)\n",
      "Epoch: [3][1001/5005]\tTime  0.835 ( 1.977)\tData  0.701 ( 1.432)\tLoss 3.8012e+00 (4.0745e+00)\tAcc@1  24.22 ( 21.93)\tAcc@5  48.05 ( 43.29)\n",
      "Epoch: [3][2002/5005]\tTime  0.375 ( 2.484)\tData  0.000 ( 1.685)\tLoss 4.2908e+00 (4.0294e+00)\tAcc@1  19.14 ( 22.57)\tAcc@5  37.11 ( 44.12)\n",
      "Epoch: [3][3003/5005]\tTime  0.369 ( 2.543)\tData  0.000 ( 1.687)\tLoss 3.7414e+00 (3.9801e+00)\tAcc@1  24.22 ( 23.27)\tAcc@5  48.05 ( 45.01)\n",
      "Epoch: [3][4004/5005]\tTime  0.373 ( 2.507)\tData  0.000 ( 1.687)\tLoss 3.5776e+00 (3.9360e+00)\tAcc@1  29.69 ( 23.92)\tAcc@5  53.52 ( 45.82)\n",
      " * Acc@1 28.402 Acc@5 52.334\n",
      "************train_loss 3.893571285720353 val_acc 28.40199851989746*************\n",
      "Epoch: [4][   0/5005]\tTime 55.729 (55.729)\tData 30.458 (30.458)\tLoss 3.4295e+00 (3.4295e+00)\tAcc@1  32.03 ( 32.03)\tAcc@5  54.69 ( 54.69)\n",
      "Epoch: [4][1001/5005]\tTime  0.376 ( 2.716)\tData  0.000 ( 1.701)\tLoss 3.5457e+00 (3.6119e+00)\tAcc@1  30.47 ( 28.46)\tAcc@5  50.78 ( 51.52)\n",
      "Epoch: [4][2002/5005]\tTime  0.377 ( 2.408)\tData  0.000 ( 1.619)\tLoss 3.6739e+00 (3.5932e+00)\tAcc@1  25.00 ( 28.73)\tAcc@5  47.66 ( 51.93)\n",
      "Epoch: [4][3003/5005]\tTime  0.387 ( 2.215)\tData  0.000 ( 1.551)\tLoss 3.3585e+00 (3.5703e+00)\tAcc@1  28.91 ( 29.08)\tAcc@5  53.12 ( 52.37)\n",
      "Epoch: [4][4004/5005]\tTime  0.396 ( 2.390)\tData  0.001 ( 1.605)\tLoss 3.5689e+00 (3.5489e+00)\tAcc@1  22.66 ( 29.44)\tAcc@5  51.95 ( 52.76)\n",
      " * Acc@1 30.612 Acc@5 55.228\n",
      "************train_loss 3.5248884554985875 val_acc 30.61199951171875*************\n",
      "Epoch: [5][   0/5005]\tTime 57.440 (57.440)\tData 27.789 (27.789)\tLoss 3.2698e+00 (3.2698e+00)\tAcc@1  29.69 ( 29.69)\tAcc@5  57.42 ( 57.42)\n",
      "Epoch: [5][1001/5005]\tTime  0.191 ( 2.423)\tData  0.001 ( 1.598)\tLoss 3.3987e+00 (3.3599e+00)\tAcc@1  32.03 ( 32.42)\tAcc@5  54.69 ( 56.12)\n",
      "Epoch: [5][2002/5005]\tTime  5.910 ( 2.266)\tData  5.759 ( 1.532)\tLoss 3.4761e+00 (3.3476e+00)\tAcc@1  30.47 ( 32.60)\tAcc@5  54.69 ( 56.32)\n",
      "Epoch: [5][3003/5005]\tTime  8.188 ( 2.455)\tData  8.042 ( 1.647)\tLoss 3.3977e+00 (3.3342e+00)\tAcc@1  28.52 ( 32.76)\tAcc@5  56.25 ( 56.59)\n",
      "Epoch: [5][4004/5005]\tTime  0.380 ( 2.427)\tData  0.000 ( 1.670)\tLoss 3.0329e+00 (3.3186e+00)\tAcc@1  38.67 ( 32.99)\tAcc@5  63.67 ( 56.87)\n",
      " * Acc@1 34.538 Acc@5 59.286\n",
      "************train_loss 3.305264945463701 val_acc 34.53799819946289*************\n",
      "Epoch: [6][   0/5005]\tTime 45.729 (45.729)\tData 18.297 (18.297)\tLoss 3.0118e+00 (3.0118e+00)\tAcc@1  35.94 ( 35.94)\tAcc@5  63.67 ( 63.67)\n",
      "Epoch: [6][1001/5005]\tTime  4.775 ( 2.176)\tData  4.636 ( 1.534)\tLoss 3.1987e+00 (3.1957e+00)\tAcc@1  37.11 ( 34.97)\tAcc@5  58.98 ( 58.94)\n",
      "Epoch: [6][2002/5005]\tTime 39.866 ( 2.458)\tData 24.043 ( 1.711)\tLoss 2.8128e+00 (3.1845e+00)\tAcc@1  38.67 ( 35.17)\tAcc@5  67.97 ( 59.19)\n",
      "Epoch: [6][3003/5005]\tTime  0.399 ( 2.305)\tData  0.180 ( 1.621)\tLoss 3.0538e+00 (3.1732e+00)\tAcc@1  35.94 ( 35.37)\tAcc@5  65.23 ( 59.38)\n",
      "Epoch: [6][4004/5005]\tTime  0.386 ( 2.223)\tData  0.000 ( 1.572)\tLoss 3.3292e+00 (3.1662e+00)\tAcc@1  31.25 ( 35.49)\tAcc@5  60.55 ( 59.52)\n",
      " * Acc@1 37.846 Acc@5 62.904\n",
      "************train_loss 3.157549675980529 val_acc 37.84600067138672*************\n",
      "Epoch: [7][   0/5005]\tTime 55.070 (55.070)\tData 32.217 (32.217)\tLoss 2.9733e+00 (2.9733e+00)\tAcc@1  36.72 ( 36.72)\tAcc@5  64.06 ( 64.06)\n",
      "Epoch: [7][1001/5005]\tTime  0.410 ( 2.547)\tData  0.000 ( 1.837)\tLoss 3.0061e+00 (3.0690e+00)\tAcc@1  35.94 ( 37.02)\tAcc@5  63.28 ( 61.09)\n",
      "Epoch: [7][2002/5005]\tTime  4.094 ( 2.386)\tData  3.951 ( 1.709)\tLoss 2.9557e+00 (3.0646e+00)\tAcc@1  37.89 ( 37.14)\tAcc@5  60.55 ( 61.18)\n",
      "Epoch: [7][3003/5005]\tTime  0.389 ( 2.218)\tData  0.000 ( 1.613)\tLoss 3.1854e+00 (3.0616e+00)\tAcc@1  36.72 ( 37.13)\tAcc@5  61.33 ( 61.27)\n",
      "Epoch: [7][4004/5005]\tTime  0.803 ( 2.330)\tData  0.668 ( 1.680)\tLoss 3.2170e+00 (3.0577e+00)\tAcc@1  34.77 ( 37.18)\tAcc@5  60.16 ( 61.32)\n",
      " * Acc@1 38.184 Acc@5 63.494\n",
      "************train_loss 3.0515074686570602 val_acc 38.183998107910156*************\n",
      "Epoch: [8][   0/5005]\tTime 81.128 (81.128)\tData 50.981 (50.981)\tLoss 2.9238e+00 (2.9238e+00)\tAcc@1  38.67 ( 38.67)\tAcc@5  62.50 ( 62.50)\n",
      "Epoch: [8][1001/5005]\tTime  0.370 ( 2.628)\tData  0.000 ( 1.745)\tLoss 3.2095e+00 (2.9716e+00)\tAcc@1  34.38 ( 38.48)\tAcc@5  61.33 ( 62.83)\n",
      "Epoch: [8][2002/5005]\tTime  0.373 ( 2.457)\tData  0.000 ( 1.660)\tLoss 2.7861e+00 (2.9769e+00)\tAcc@1  42.97 ( 38.42)\tAcc@5  62.89 ( 62.71)\n",
      "Epoch: [8][3003/5005]\tTime  1.207 ( 2.578)\tData  1.074 ( 1.743)\tLoss 2.7511e+00 (2.9793e+00)\tAcc@1  43.36 ( 38.42)\tAcc@5  67.19 ( 62.69)\n",
      "Epoch: [8][4004/5005]\tTime  0.377 ( 2.577)\tData  0.000 ( 1.740)\tLoss 2.6558e+00 (2.9774e+00)\tAcc@1  43.75 ( 38.44)\tAcc@5  67.58 ( 62.73)\n",
      " * Acc@1 40.860 Acc@5 66.432\n",
      "************train_loss 2.9751313533935395 val_acc 40.86000061035156*************\n",
      "Epoch: [9][   0/5005]\tTime 58.966 (58.966)\tData 33.908 (33.908)\tLoss 2.9045e+00 (2.9045e+00)\tAcc@1  41.02 ( 41.02)\tAcc@5  66.41 ( 66.41)\n",
      "Epoch: [9][1001/5005]\tTime  7.496 ( 2.340)\tData  7.341 ( 1.628)\tLoss 2.9595e+00 (2.9187e+00)\tAcc@1  40.23 ( 39.50)\tAcc@5  65.62 ( 63.69)\n",
      "Epoch: [9][2002/5005]\tTime  1.391 ( 2.485)\tData  1.261 ( 1.672)\tLoss 2.6949e+00 (2.9192e+00)\tAcc@1  42.97 ( 39.45)\tAcc@5  65.62 ( 63.66)\n",
      "Epoch: [9][3003/5005]\tTime  0.386 ( 2.479)\tData  0.000 ( 1.677)\tLoss 2.6819e+00 (2.9218e+00)\tAcc@1  45.31 ( 39.38)\tAcc@5  68.75 ( 63.64)\n",
      "Epoch: [9][4004/5005]\tTime  0.376 ( 2.340)\tData  0.000 ( 1.606)\tLoss 3.0437e+00 (2.9199e+00)\tAcc@1  40.23 ( 39.43)\tAcc@5  63.67 ( 63.68)\n",
      " * Acc@1 41.054 Acc@5 66.472\n",
      "************train_loss 2.917955018900015 val_acc 41.05400085449219*************\n",
      "Epoch: [10][   0/5005]\tTime 41.395 (41.395)\tData 26.026 (26.026)\tLoss 3.1556e+00 (3.1556e+00)\tAcc@1  38.28 ( 38.28)\tAcc@5  60.16 ( 60.16)\n",
      "Epoch: [10][1001/5005]\tTime  1.437 ( 2.480)\tData  1.297 ( 1.694)\tLoss 2.8041e+00 (2.8627e+00)\tAcc@1  41.80 ( 40.38)\tAcc@5  65.23 ( 64.62)\n",
      "Epoch: [10][2002/5005]\tTime  0.375 ( 2.530)\tData  0.000 ( 1.666)\tLoss 2.8349e+00 (2.8730e+00)\tAcc@1  39.45 ( 40.29)\tAcc@5  65.23 ( 64.49)\n",
      "Epoch: [10][3003/5005]\tTime  0.378 ( 2.279)\tData  0.000 ( 1.552)\tLoss 2.8872e+00 (2.8746e+00)\tAcc@1  39.06 ( 40.25)\tAcc@5  62.11 ( 64.45)\n",
      "Epoch: [10][4004/5005]\tTime  1.796 ( 2.297)\tData  1.663 ( 1.597)\tLoss 2.8118e+00 (2.8761e+00)\tAcc@1  36.72 ( 40.21)\tAcc@5  62.11 ( 64.45)\n",
      " * Acc@1 41.052 Acc@5 66.966\n",
      "************train_loss 2.876183681840544 val_acc 41.051998138427734*************\n",
      "Epoch: [11][   0/5005]\tTime 71.230 (71.230)\tData 37.451 (37.451)\tLoss 2.6045e+00 (2.6045e+00)\tAcc@1  44.53 ( 44.53)\tAcc@5  71.09 ( 71.09)\n",
      "Epoch: [11][1001/5005]\tTime  0.395 ( 2.466)\tData  0.000 ( 1.665)\tLoss 2.8495e+00 (2.8284e+00)\tAcc@1  39.45 ( 41.16)\tAcc@5  63.28 ( 65.18)\n",
      "Epoch: [11][2002/5005]\tTime  0.377 ( 2.325)\tData  0.000 ( 1.602)\tLoss 2.9060e+00 (2.8348e+00)\tAcc@1  36.33 ( 40.97)\tAcc@5  60.16 ( 65.09)\n",
      "Epoch: [11][3003/5005]\tTime  0.374 ( 2.452)\tData  0.000 ( 1.656)\tLoss 2.9058e+00 (2.8391e+00)\tAcc@1  39.06 ( 40.88)\tAcc@5  61.72 ( 65.01)\n",
      "Epoch: [11][4004/5005]\tTime  0.737 ( 2.547)\tData  0.000 ( 1.716)\tLoss 3.1523e+00 (2.8435e+00)\tAcc@1  35.94 ( 40.83)\tAcc@5  59.38 ( 64.97)\n",
      " * Acc@1 42.050 Acc@5 67.724\n",
      "************train_loss 2.844721562331254 val_acc 42.04999923706055*************\n",
      "Epoch: [12][   0/5005]\tTime 71.280 (71.280)\tData 26.261 (26.261)\tLoss 2.8750e+00 (2.8750e+00)\tAcc@1  41.41 ( 41.41)\tAcc@5  62.50 ( 62.50)\n",
      "Epoch: [12][1001/5005]\tTime  0.371 ( 2.763)\tData  0.000 ( 1.813)\tLoss 2.6445e+00 (2.8044e+00)\tAcc@1  44.92 ( 41.45)\tAcc@5  66.80 ( 65.61)\n",
      "Epoch: [12][2002/5005]\tTime  0.381 ( 2.609)\tData  0.000 ( 1.738)\tLoss 3.0242e+00 (2.8092e+00)\tAcc@1  37.11 ( 41.36)\tAcc@5  61.72 ( 65.47)\n",
      "Epoch: [12][3003/5005]\tTime  4.076 ( 2.570)\tData  3.930 ( 1.695)\tLoss 2.6143e+00 (2.8090e+00)\tAcc@1  48.44 ( 41.34)\tAcc@5  68.36 ( 65.49)\n",
      "Epoch: [12][4004/5005]\tTime  0.370 ( 2.397)\tData  0.000 ( 1.602)\tLoss 2.7107e+00 (2.8117e+00)\tAcc@1  44.14 ( 41.34)\tAcc@5  66.80 ( 65.44)\n",
      " * Acc@1 42.708 Acc@5 67.970\n",
      "************train_loss 2.812973780874963 val_acc 42.70800018310547*************\n",
      "Epoch: [13][   0/5005]\tTime 95.034 (95.034)\tData 46.258 (46.258)\tLoss 2.6697e+00 (2.6697e+00)\tAcc@1  40.23 ( 40.23)\tAcc@5  69.14 ( 69.14)\n",
      "Epoch: [13][1001/5005]\tTime  0.375 ( 3.150)\tData  0.000 ( 2.213)\tLoss 2.9923e+00 (2.7723e+00)\tAcc@1  38.67 ( 41.90)\tAcc@5  62.50 ( 66.21)\n",
      "Epoch: [13][2002/5005]\tTime  0.375 ( 3.573)\tData  0.000 ( 2.660)\tLoss 2.7864e+00 (2.7805e+00)\tAcc@1  39.84 ( 41.81)\tAcc@5  63.67 ( 66.06)\n",
      "Epoch: [13][3003/5005]\tTime  0.371 ( 4.225)\tData  0.000 ( 3.267)\tLoss 2.6911e+00 (2.7868e+00)\tAcc@1  42.19 ( 41.68)\tAcc@5  69.14 ( 65.97)\n",
      "Epoch: [13][4004/5005]\tTime  0.141 ( 3.997)\tData  0.000 ( 3.130)\tLoss 2.5911e+00 (2.7900e+00)\tAcc@1  42.19 ( 41.65)\tAcc@5  68.75 ( 65.91)\n",
      " * Acc@1 40.572 Acc@5 66.182\n",
      "************train_loss 2.790302388889568 val_acc 40.571998596191406*************\n",
      "Epoch: [14][   0/5005]\tTime 36.842 (36.842)\tData 27.120 (27.120)\tLoss 2.8222e+00 (2.8222e+00)\tAcc@1  38.67 ( 38.67)\tAcc@5  66.02 ( 66.02)\n",
      "Epoch: [14][1001/5005]\tTime  1.345 ( 2.156)\tData  1.210 ( 1.636)\tLoss 2.6091e+00 (2.7515e+00)\tAcc@1  41.41 ( 42.22)\tAcc@5  68.36 ( 66.53)\n",
      "Epoch: [14][2002/5005]\tTime  0.665 ( 2.946)\tData  0.532 ( 2.192)\tLoss 2.6365e+00 (2.7603e+00)\tAcc@1  45.31 ( 42.09)\tAcc@5  66.02 ( 66.42)\n",
      "Epoch: [14][3003/5005]\tTime  0.378 ( 3.199)\tData  0.000 ( 2.415)\tLoss 2.7294e+00 (2.7670e+00)\tAcc@1  42.97 ( 41.99)\tAcc@5  66.02 ( 66.31)\n",
      "Epoch: [14][4004/5005]\tTime 10.510 ( 3.019)\tData 10.362 ( 2.289)\tLoss 2.6809e+00 (2.7719e+00)\tAcc@1  41.41 ( 41.95)\tAcc@5  70.70 ( 66.22)\n",
      " * Acc@1 43.670 Acc@5 69.146\n",
      "************train_loss 2.7727257539461423 val_acc 43.66999816894531*************\n",
      "Epoch: [15][   0/5005]\tTime 37.667 (37.667)\tData 22.002 (22.002)\tLoss 2.7014e+00 (2.7014e+00)\tAcc@1  41.41 ( 41.41)\tAcc@5  67.97 ( 67.97)\n",
      "Epoch: [15][1001/5005]\tTime  0.378 ( 2.692)\tData  0.000 ( 1.986)\tLoss 2.9260e+00 (2.7335e+00)\tAcc@1  36.33 ( 42.50)\tAcc@5  62.50 ( 66.74)\n",
      "Epoch: [15][2002/5005]\tTime  0.370 ( 2.529)\tData  0.000 ( 1.891)\tLoss 2.6219e+00 (2.7469e+00)\tAcc@1  46.48 ( 42.32)\tAcc@5  66.02 ( 66.61)\n",
      "Epoch: [15][3003/5005]\tTime  5.214 ( 2.970)\tData  5.070 ( 2.242)\tLoss 2.6817e+00 (2.7516e+00)\tAcc@1  46.09 ( 42.32)\tAcc@5  67.19 ( 66.54)\n",
      "Epoch: [15][4004/5005]\tTime  0.376 ( 2.824)\tData  0.000 ( 2.122)\tLoss 2.6048e+00 (2.7551e+00)\tAcc@1  42.58 ( 42.28)\tAcc@5  68.75 ( 66.46)\n",
      " * Acc@1 44.738 Acc@5 70.184\n",
      "************train_loss 2.756365468714025 val_acc 44.737998962402344*************\n",
      "Epoch: [16][   0/5005]\tTime 55.522 (55.522)\tData 37.502 (37.502)\tLoss 2.6000e+00 (2.6000e+00)\tAcc@1  44.14 ( 44.14)\tAcc@5  69.14 ( 69.14)\n",
      "Epoch: [16][1001/5005]\tTime  0.375 ( 3.399)\tData  0.000 ( 2.514)\tLoss 2.4446e+00 (2.7294e+00)\tAcc@1  46.09 ( 42.67)\tAcc@5  74.22 ( 66.95)\n",
      "Epoch: [16][2002/5005]\tTime  0.379 ( 3.137)\tData  0.000 ( 2.334)\tLoss 2.8507e+00 (2.7317e+00)\tAcc@1  39.45 ( 42.59)\tAcc@5  62.89 ( 66.83)\n",
      "Epoch: [16][3003/5005]\tTime  0.390 ( 3.123)\tData  0.000 ( 2.306)\tLoss 2.9469e+00 (2.7376e+00)\tAcc@1  40.23 ( 42.49)\tAcc@5  63.67 ( 66.71)\n",
      "Epoch: [16][4004/5005]\tTime  0.372 ( 3.178)\tData  0.000 ( 2.352)\tLoss 2.8198e+00 (2.7393e+00)\tAcc@1  39.06 ( 42.48)\tAcc@5  64.84 ( 66.71)\n",
      " * Acc@1 45.710 Acc@5 70.816\n",
      "************train_loss 2.7413866527549753 val_acc 45.709999084472656*************\n",
      "Epoch: [17][   0/5005]\tTime 26.449 (26.449)\tData 19.851 (19.851)\tLoss 2.5276e+00 (2.5276e+00)\tAcc@1  44.92 ( 44.92)\tAcc@5  72.27 ( 72.27)\n",
      "Epoch: [17][1001/5005]\tTime 49.497 ( 2.134)\tData 26.953 ( 1.601)\tLoss 2.4281e+00 (2.7187e+00)\tAcc@1  45.70 ( 42.96)\tAcc@5  72.27 ( 66.95)\n",
      "Epoch: [17][2002/5005]\tTime  0.676 ( 2.927)\tData  0.542 ( 2.143)\tLoss 2.6813e+00 (2.7231e+00)\tAcc@1  42.19 ( 42.88)\tAcc@5  66.41 ( 66.95)\n",
      "Epoch: [17][3003/5005]\tTime  0.383 ( 2.682)\tData  0.000 ( 1.972)\tLoss 2.9219e+00 (2.7264e+00)\tAcc@1  33.98 ( 42.79)\tAcc@5  64.45 ( 66.91)\n",
      "Epoch: [17][4004/5005]\tTime  0.380 ( 2.667)\tData  0.000 ( 1.947)\tLoss 2.7501e+00 (2.7278e+00)\tAcc@1  44.14 ( 42.75)\tAcc@5  70.31 ( 66.89)\n",
      " * Acc@1 44.934 Acc@5 70.586\n",
      "************train_loss 2.7279072163226483 val_acc 44.933998107910156*************\n",
      "Epoch: [18][   0/5005]\tTime 139.153 (139.153)\tData 84.325 (84.325)\tLoss 2.9903e+00 (2.9903e+00)\tAcc@1  35.16 ( 35.16)\tAcc@5  61.72 ( 61.72)\n",
      "Epoch: [18][1001/5005]\tTime  0.382 ( 2.615)\tData  0.000 ( 1.964)\tLoss 2.5187e+00 (2.7003e+00)\tAcc@1  46.48 ( 43.20)\tAcc@5  69.92 ( 67.33)\n",
      "Epoch: [18][2002/5005]\tTime  0.389 ( 2.650)\tData  0.000 ( 1.943)\tLoss 2.2024e+00 (2.7083e+00)\tAcc@1  48.83 ( 43.02)\tAcc@5  75.00 ( 67.25)\n",
      "Epoch: [18][3003/5005]\tTime 15.150 ( 2.818)\tData  2.299 ( 2.093)\tLoss 2.6714e+00 (2.7134e+00)\tAcc@1  50.00 ( 42.95)\tAcc@5  67.19 ( 67.15)\n",
      "Epoch: [18][4004/5005]\tTime  0.375 ( 2.886)\tData  0.000 ( 2.115)\tLoss 3.0091e+00 (2.7175e+00)\tAcc@1  35.94 ( 42.89)\tAcc@5  63.28 ( 67.10)\n",
      " * Acc@1 44.520 Acc@5 70.502\n",
      "************train_loss 2.7194011817326196 val_acc 44.52000045776367*************\n",
      "Epoch: [19][   0/5005]\tTime 108.213 (108.213)\tData 54.098 (54.098)\tLoss 2.1852e+00 (2.1852e+00)\tAcc@1  52.34 ( 52.34)\tAcc@5  76.17 ( 76.17)\n",
      "Epoch: [19][1001/5005]\tTime  1.189 ( 3.833)\tData  1.054 ( 2.723)\tLoss 2.5224e+00 (2.6845e+00)\tAcc@1  42.58 ( 43.48)\tAcc@5  72.27 ( 67.65)\n",
      "Epoch: [19][2002/5005]\tTime  0.387 ( 3.350)\tData  0.000 ( 2.417)\tLoss 2.5175e+00 (2.6957e+00)\tAcc@1  48.44 ( 43.29)\tAcc@5  69.53 ( 67.44)\n",
      "Epoch: [19][3003/5005]\tTime  0.375 ( 3.039)\tData  0.000 ( 2.221)\tLoss 2.7124e+00 (2.7030e+00)\tAcc@1  42.58 ( 43.18)\tAcc@5  67.58 ( 67.32)\n",
      "Epoch: [19][4004/5005]\tTime 17.028 ( 2.875)\tData 16.815 ( 2.096)\tLoss 2.3748e+00 (2.7035e+00)\tAcc@1  46.48 ( 43.19)\tAcc@5  72.66 ( 67.31)\n",
      " * Acc@1 45.242 Acc@5 70.958\n",
      "************train_loss 2.7062413746303133 val_acc 45.242000579833984*************\n",
      "Epoch: [20][   0/5005]\tTime 32.103 (32.103)\tData 22.888 (22.888)\tLoss 2.5665e+00 (2.5665e+00)\tAcc@1  46.09 ( 46.09)\tAcc@5  69.53 ( 69.53)\n",
      "Epoch: [20][1001/5005]\tTime  2.269 ( 2.875)\tData  2.124 ( 1.966)\tLoss 2.7057e+00 (2.6760e+00)\tAcc@1  41.80 ( 43.56)\tAcc@5  69.53 ( 67.73)\n",
      "Epoch: [20][2002/5005]\tTime  0.376 ( 3.043)\tData  0.000 ( 2.081)\tLoss 2.9037e+00 (2.6860e+00)\tAcc@1  39.06 ( 43.47)\tAcc@5  63.67 ( 67.54)\n",
      "Epoch: [20][3003/5005]\tTime  0.373 ( 3.137)\tData  0.000 ( 2.127)\tLoss 2.4181e+00 (2.6925e+00)\tAcc@1  46.48 ( 43.33)\tAcc@5  74.22 ( 67.45)\n",
      "Epoch: [20][4004/5005]\tTime  0.384 ( 2.799)\tData  0.000 ( 1.932)\tLoss 2.7992e+00 (2.6979e+00)\tAcc@1  41.41 ( 43.25)\tAcc@5  65.23 ( 67.38)\n",
      " * Acc@1 45.020 Acc@5 70.542\n",
      "************train_loss 2.701211883209564 val_acc 45.02000045776367*************\n",
      "Epoch: [21][   0/5005]\tTime 63.973 (63.973)\tData 43.629 (43.629)\tLoss 2.9647e+00 (2.9647e+00)\tAcc@1  42.19 ( 42.19)\tAcc@5  64.84 ( 64.84)\n",
      "Epoch: [21][1001/5005]\tTime  0.374 ( 3.255)\tData  0.000 ( 2.344)\tLoss 2.3780e+00 (2.6690e+00)\tAcc@1  48.44 ( 43.70)\tAcc@5  71.09 ( 67.89)\n",
      "Epoch: [21][2002/5005]\tTime  0.377 ( 2.814)\tData  0.000 ( 2.037)\tLoss 2.5437e+00 (2.6775e+00)\tAcc@1  45.31 ( 43.59)\tAcc@5  69.53 ( 67.79)\n",
      "Epoch: [21][3003/5005]\tTime 58.559 ( 2.537)\tData 24.328 ( 1.847)\tLoss 2.8019e+00 (2.6844e+00)\tAcc@1  45.70 ( 43.52)\tAcc@5  63.67 ( 67.69)\n",
      "Epoch: [21][4004/5005]\tTime  1.700 ( 2.703)\tData  1.570 ( 1.968)\tLoss 2.6830e+00 (2.6892e+00)\tAcc@1  44.53 ( 43.47)\tAcc@5  68.36 ( 67.59)\n",
      " * Acc@1 44.082 Acc@5 69.576\n",
      "************train_loss 2.6937084444276578 val_acc 44.082000732421875*************\n",
      "Epoch: [22][   0/5005]\tTime 71.197 (71.197)\tData 44.362 (44.362)\tLoss 2.7412e+00 (2.7412e+00)\tAcc@1  42.58 ( 42.58)\tAcc@5  65.23 ( 65.23)\n",
      "Epoch: [22][1001/5005]\tTime  0.382 ( 2.786)\tData  0.000 ( 2.048)\tLoss 2.8386e+00 (2.6605e+00)\tAcc@1  40.62 ( 43.92)\tAcc@5  65.62 ( 68.04)\n",
      "Epoch: [22][2002/5005]\tTime  1.888 ( 2.822)\tData  1.755 ( 2.045)\tLoss 2.7460e+00 (2.6705e+00)\tAcc@1  42.97 ( 43.68)\tAcc@5  66.02 ( 67.89)\n",
      "Epoch: [22][3003/5005]\tTime  0.378 ( 2.685)\tData  0.000 ( 1.938)\tLoss 2.6467e+00 (2.6755e+00)\tAcc@1  48.05 ( 43.63)\tAcc@5  66.02 ( 67.84)\n",
      "Epoch: [22][4004/5005]\tTime 20.973 ( 2.447)\tData  6.433 ( 1.750)\tLoss 2.8287e+00 (2.6783e+00)\tAcc@1  38.28 ( 43.59)\tAcc@5  65.23 ( 67.80)\n",
      " * Acc@1 44.924 Acc@5 70.570\n",
      "************train_loss 2.6824406863926176 val_acc 44.92399978637695*************\n",
      "Epoch: [23][   0/5005]\tTime 47.426 (47.426)\tData 32.986 (32.986)\tLoss 2.5244e+00 (2.5244e+00)\tAcc@1  46.48 ( 46.48)\tAcc@5  73.44 ( 73.44)\n",
      "Epoch: [23][1001/5005]\tTime  0.375 ( 3.258)\tData  0.000 ( 2.244)\tLoss 2.6398e+00 (2.6543e+00)\tAcc@1  43.75 ( 44.01)\tAcc@5  69.14 ( 68.19)\n",
      "Epoch: [23][2002/5005]\tTime  8.691 ( 2.727)\tData  8.551 ( 1.916)\tLoss 2.8374e+00 (2.6628e+00)\tAcc@1  40.62 ( 43.79)\tAcc@5  64.45 ( 68.01)\n",
      "Epoch: [23][3003/5005]\tTime  0.389 ( 2.530)\tData  0.000 ( 1.783)\tLoss 2.8152e+00 (2.6718e+00)\tAcc@1  44.14 ( 43.65)\tAcc@5  64.45 ( 67.87)\n",
      "Epoch: [23][4004/5005]\tTime  0.377 ( 2.595)\tData  0.000 ( 1.839)\tLoss 2.7419e+00 (2.6766e+00)\tAcc@1  41.02 ( 43.59)\tAcc@5  67.19 ( 67.78)\n",
      " * Acc@1 46.580 Acc@5 72.110\n",
      "************train_loss 2.6797345457258044 val_acc 46.57999801635742*************\n",
      "Epoch: [24][   0/5005]\tTime 35.429 (35.429)\tData 21.208 (21.208)\tLoss 2.6473e+00 (2.6473e+00)\tAcc@1  45.31 ( 45.31)\tAcc@5  71.09 ( 71.09)\n",
      "Epoch: [24][1001/5005]\tTime  0.378 ( 2.237)\tData  0.000 ( 1.706)\tLoss 2.7429e+00 (2.6570e+00)\tAcc@1  41.80 ( 43.96)\tAcc@5  69.53 ( 68.11)\n",
      "Epoch: [24][2002/5005]\tTime  1.057 ( 2.568)\tData  0.913 ( 1.937)\tLoss 2.9169e+00 (2.6610e+00)\tAcc@1  40.62 ( 43.84)\tAcc@5  64.45 ( 68.00)\n",
      "Epoch: [24][3003/5005]\tTime  0.376 ( 2.622)\tData  0.000 ( 1.935)\tLoss 2.4834e+00 (2.6668e+00)\tAcc@1  48.05 ( 43.81)\tAcc@5  70.31 ( 67.91)\n",
      "Epoch: [24][4004/5005]\tTime  0.375 ( 2.479)\tData  0.000 ( 1.831)\tLoss 3.0438e+00 (2.6723e+00)\tAcc@1  41.80 ( 43.73)\tAcc@5  65.62 ( 67.82)\n",
      " * Acc@1 44.246 Acc@5 69.734\n",
      "************train_loss 2.675050367484917 val_acc 44.24599838256836*************\n",
      "Epoch: [25][   0/5005]\tTime 91.350 (91.350)\tData 53.151 (53.151)\tLoss 2.5364e+00 (2.5364e+00)\tAcc@1  46.48 ( 46.48)\tAcc@5  70.70 ( 70.70)\n",
      "Epoch: [25][1001/5005]\tTime  0.373 ( 3.558)\tData  0.000 ( 2.422)\tLoss 2.8885e+00 (2.6413e+00)\tAcc@1  39.06 ( 44.18)\tAcc@5  62.50 ( 68.39)\n",
      "Epoch: [25][2002/5005]\tTime  0.385 ( 2.765)\tData  0.000 ( 1.934)\tLoss 2.7808e+00 (2.6494e+00)\tAcc@1  42.97 ( 44.08)\tAcc@5  67.19 ( 68.25)\n",
      "Epoch: [25][3003/5005]\tTime  4.120 ( 2.600)\tData  3.973 ( 1.831)\tLoss 2.6575e+00 (2.6579e+00)\tAcc@1  44.53 ( 43.90)\tAcc@5  68.75 ( 68.11)\n",
      "Epoch: [25][4004/5005]\tTime  0.377 ( 2.567)\tData  0.000 ( 1.811)\tLoss 2.9773e+00 (2.6598e+00)\tAcc@1  42.19 ( 43.88)\tAcc@5  64.45 ( 68.08)\n",
      " * Acc@1 44.578 Acc@5 70.398\n",
      "************train_loss 2.6631408399397083 val_acc 44.577999114990234*************\n",
      "Epoch: [26][   0/5005]\tTime 97.913 (97.913)\tData 46.949 (46.949)\tLoss 2.5974e+00 (2.5974e+00)\tAcc@1  44.14 ( 44.14)\tAcc@5  72.66 ( 72.66)\n",
      "Epoch: [26][1001/5005]\tTime  0.388 ( 1.836)\tData  0.000 ( 1.364)\tLoss 2.4566e+00 (2.6356e+00)\tAcc@1  49.22 ( 44.44)\tAcc@5  71.48 ( 68.42)\n",
      "Epoch: [26][2002/5005]\tTime  0.370 ( 2.369)\tData  0.000 ( 1.714)\tLoss 2.5333e+00 (2.6479e+00)\tAcc@1  43.75 ( 44.13)\tAcc@5  71.88 ( 68.25)\n",
      "Epoch: [26][3003/5005]\tTime  0.378 ( 2.493)\tData  0.000 ( 1.815)\tLoss 2.4353e+00 (2.6550e+00)\tAcc@1  49.22 ( 43.99)\tAcc@5  71.88 ( 68.15)\n",
      "Epoch: [26][4004/5005]\tTime  1.971 ( 2.486)\tData  1.838 ( 1.816)\tLoss 2.6568e+00 (2.6590e+00)\tAcc@1  42.19 ( 43.91)\tAcc@5  68.36 ( 68.11)\n",
      " * Acc@1 46.092 Acc@5 71.460\n",
      "************train_loss 2.659268886011678 val_acc 46.09199905395508*************\n",
      "Epoch: [27][   0/5005]\tTime 47.919 (47.919)\tData 32.285 (32.285)\tLoss 3.0037e+00 (3.0037e+00)\tAcc@1  40.62 ( 40.62)\tAcc@5  62.50 ( 62.50)\n",
      "Epoch: [27][1001/5005]\tTime  0.381 ( 3.257)\tData  0.000 ( 2.226)\tLoss 2.6571e+00 (2.6345e+00)\tAcc@1  42.97 ( 44.20)\tAcc@5  69.53 ( 68.54)\n",
      "Epoch: [27][2002/5005]\tTime  0.380 ( 2.975)\tData  0.000 ( 2.051)\tLoss 2.4905e+00 (2.6429e+00)\tAcc@1  46.88 ( 44.11)\tAcc@5  67.97 ( 68.33)\n",
      "Epoch: [27][3003/5005]\tTime 10.525 ( 2.773)\tData 10.380 ( 1.933)\tLoss 2.6597e+00 (2.6475e+00)\tAcc@1  42.97 ( 44.08)\tAcc@5  67.19 ( 68.26)\n",
      "Epoch: [27][4004/5005]\tTime  0.384 ( 2.655)\tData  0.000 ( 1.852)\tLoss 2.8817e+00 (2.6525e+00)\tAcc@1  41.02 ( 44.03)\tAcc@5  64.06 ( 68.21)\n",
      " * Acc@1 45.998 Acc@5 71.314\n",
      "************train_loss 2.655840679791781 val_acc 45.99799728393555*************\n",
      "Epoch: [28][   0/5005]\tTime 58.191 (58.191)\tData 38.594 (38.594)\tLoss 2.5739e+00 (2.5739e+00)\tAcc@1  48.83 ( 48.83)\tAcc@5  68.75 ( 68.75)\n",
      "Epoch: [28][1001/5005]\tTime  0.382 ( 1.928)\tData  0.000 ( 1.448)\tLoss 2.6238e+00 (2.6242e+00)\tAcc@1  45.70 ( 44.39)\tAcc@5  67.97 ( 68.60)\n",
      "Epoch: [28][2002/5005]\tTime  0.368 ( 2.242)\tData  0.000 ( 1.639)\tLoss 2.5049e+00 (2.6352e+00)\tAcc@1  48.44 ( 44.24)\tAcc@5  69.53 ( 68.39)\n",
      "Epoch: [28][3003/5005]\tTime  0.374 ( 2.441)\tData  0.000 ( 1.799)\tLoss 2.7159e+00 (2.6411e+00)\tAcc@1  44.92 ( 44.16)\tAcc@5  67.19 ( 68.32)\n",
      "Epoch: [28][4004/5005]\tTime  5.839 ( 2.499)\tData  5.687 ( 1.830)\tLoss 2.7770e+00 (2.6482e+00)\tAcc@1  46.88 ( 44.07)\tAcc@5  62.89 ( 68.22)\n",
      " * Acc@1 47.576 Acc@5 72.686\n",
      "************train_loss 2.652359434941432 val_acc 47.57600021362305*************\n",
      "Epoch: [29][   0/5005]\tTime 18.103 (18.103)\tData 12.129 (12.129)\tLoss 2.6534e+00 (2.6534e+00)\tAcc@1  39.84 ( 39.84)\tAcc@5  64.84 ( 64.84)\n",
      "Epoch: [29][1001/5005]\tTime 18.607 ( 2.355)\tData 18.428 ( 1.691)\tLoss 2.4553e+00 (2.6283e+00)\tAcc@1  44.53 ( 44.39)\tAcc@5  71.88 ( 68.62)\n",
      "Epoch: [29][2002/5005]\tTime  1.329 ( 2.603)\tData  1.194 ( 1.877)\tLoss 2.6050e+00 (2.6380e+00)\tAcc@1  49.22 ( 44.26)\tAcc@5  70.70 ( 68.45)\n",
      "Epoch: [29][3003/5005]\tTime  0.383 ( 2.644)\tData  0.000 ( 1.896)\tLoss 2.7056e+00 (2.6430e+00)\tAcc@1  46.48 ( 44.16)\tAcc@5  67.58 ( 68.36)\n",
      "Epoch: [29][4004/5005]\tTime  0.377 ( 2.580)\tData  0.000 ( 1.842)\tLoss 2.6321e+00 (2.6477e+00)\tAcc@1  45.70 ( 44.09)\tAcc@5  64.45 ( 68.28)\n",
      " * Acc@1 46.496 Acc@5 71.726\n",
      "************train_loss 2.6498106430579615 val_acc 46.49599838256836*************\n",
      "Epoch: [30][   0/5005]\tTime 57.648 (57.648)\tData 37.359 (37.359)\tLoss 2.5408e+00 (2.5408e+00)\tAcc@1  47.66 ( 47.66)\tAcc@5  69.92 ( 69.92)\n",
      "Epoch: [30][1001/5005]\tTime  0.374 ( 2.274)\tData  0.000 ( 1.665)\tLoss 1.9063e+00 (2.1656e+00)\tAcc@1  58.59 ( 52.96)\tAcc@5  78.91 ( 75.47)\n",
      "Epoch: [30][2002/5005]\tTime  2.993 ( 2.166)\tData  2.852 ( 1.572)\tLoss 1.8062e+00 (2.1006e+00)\tAcc@1  60.55 ( 54.14)\tAcc@5  77.73 ( 76.44)\n",
      "Epoch: [30][3003/5005]\tTime  0.377 ( 2.335)\tData  0.000 ( 1.677)\tLoss 1.8515e+00 (2.0603e+00)\tAcc@1  61.33 ( 54.84)\tAcc@5  80.47 ( 77.02)\n",
      "Epoch: [30][4004/5005]\tTime  0.375 ( 2.494)\tData  0.000 ( 1.773)\tLoss 2.0396e+00 (2.0350e+00)\tAcc@1  54.69 ( 55.28)\tAcc@5  76.56 ( 77.38)\n",
      " * Acc@1 62.040 Acc@5 83.848\n",
      "************train_loss 2.0158839406071607 val_acc 62.03999710083008*************\n",
      "Epoch: [31][   0/5005]\tTime 57.195 (57.195)\tData 31.926 (31.926)\tLoss 2.0723e+00 (2.0723e+00)\tAcc@1  55.08 ( 55.08)\tAcc@5  76.56 ( 76.56)\n",
      "Epoch: [31][1001/5005]\tTime  1.930 ( 1.821)\tData  1.797 ( 1.313)\tLoss 1.7411e+00 (1.8810e+00)\tAcc@1  60.16 ( 58.12)\tAcc@5  81.64 ( 79.59)\n",
      "Epoch: [31][2002/5005]\tTime  1.319 ( 2.284)\tData  1.181 ( 1.641)\tLoss 1.9729e+00 (1.8794e+00)\tAcc@1  55.47 ( 58.14)\tAcc@5  78.52 ( 79.59)\n",
      "Epoch: [31][3003/5005]\tTime  0.376 ( 2.364)\tData  0.000 ( 1.697)\tLoss 1.7222e+00 (1.8749e+00)\tAcc@1  58.59 ( 58.21)\tAcc@5  83.59 ( 79.66)\n",
      "Epoch: [31][4004/5005]\tTime  0.379 ( 2.435)\tData  0.000 ( 1.745)\tLoss 2.0775e+00 (1.8728e+00)\tAcc@1  55.08 ( 58.24)\tAcc@5  77.34 ( 79.70)\n",
      " * Acc@1 63.162 Acc@5 84.732\n",
      "************train_loss 1.8673076035140397 val_acc 63.1619987487793*************\n",
      "Epoch: [32][   0/5005]\tTime 151.806 (151.806)\tData 68.588 (68.588)\tLoss 1.5941e+00 (1.5941e+00)\tAcc@1  63.28 ( 63.28)\tAcc@5  83.20 ( 83.20)\n",
      "Epoch: [32][1001/5005]\tTime  0.372 ( 3.077)\tData  0.000 ( 2.055)\tLoss 1.7177e+00 (1.8205e+00)\tAcc@1  57.81 ( 59.23)\tAcc@5  82.42 ( 80.42)\n",
      "Epoch: [32][2002/5005]\tTime  0.144 ( 2.390)\tData  0.000 ( 1.613)\tLoss 1.7615e+00 (1.8176e+00)\tAcc@1  58.59 ( 59.26)\tAcc@5  79.30 ( 80.47)\n",
      "Epoch: [32][3003/5005]\tTime  2.143 ( 2.416)\tData  2.001 ( 1.614)\tLoss 1.9786e+00 (1.8147e+00)\tAcc@1  53.91 ( 59.32)\tAcc@5  76.56 ( 80.52)\n",
      "Epoch: [32][4004/5005]\tTime  0.374 ( 2.389)\tData  0.000 ( 1.629)\tLoss 2.0234e+00 (1.8111e+00)\tAcc@1  56.25 ( 59.38)\tAcc@5  74.22 ( 80.58)\n",
      " * Acc@1 63.300 Acc@5 84.918\n",
      "************train_loss 1.809890929587952 val_acc 63.29999923706055*************\n",
      "Epoch: [33][   0/5005]\tTime 39.172 (39.172)\tData 25.445 (25.445)\tLoss 1.6750e+00 (1.6750e+00)\tAcc@1  63.28 ( 63.28)\tAcc@5  82.81 ( 82.81)\n",
      "Epoch: [33][1001/5005]\tTime  0.377 ( 1.639)\tData  0.000 ( 1.166)\tLoss 1.6194e+00 (1.7731e+00)\tAcc@1  63.28 ( 60.13)\tAcc@5  82.03 ( 81.03)\n",
      "Epoch: [33][2002/5005]\tTime  0.373 ( 1.864)\tData  0.000 ( 1.330)\tLoss 1.5443e+00 (1.7694e+00)\tAcc@1  62.50 ( 60.19)\tAcc@5  80.86 ( 81.13)\n",
      "Epoch: [33][3003/5005]\tTime  0.387 ( 2.071)\tData  0.000 ( 1.454)\tLoss 1.7133e+00 (1.7703e+00)\tAcc@1  60.16 ( 60.17)\tAcc@5  83.20 ( 81.10)\n",
      "Epoch: [33][4004/5005]\tTime  0.374 ( 2.269)\tData  0.163 ( 1.583)\tLoss 1.6019e+00 (1.7698e+00)\tAcc@1  61.33 ( 60.17)\tAcc@5  83.98 ( 81.14)\n",
      " * Acc@1 63.732 Acc@5 85.220\n",
      "************train_loss 1.7696421225468715 val_acc 63.731998443603516*************\n",
      "Epoch: [34][   0/5005]\tTime 94.575 (94.575)\tData 56.513 (56.513)\tLoss 1.7946e+00 (1.7946e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  80.47 ( 80.47)\n",
      "Epoch: [34][1001/5005]\tTime  0.378 ( 3.409)\tData  0.000 ( 2.170)\tLoss 1.5371e+00 (1.7341e+00)\tAcc@1  64.45 ( 60.79)\tAcc@5  82.81 ( 81.68)\n",
      "Epoch: [34][2002/5005]\tTime  0.673 ( 2.883)\tData  0.538 ( 1.935)\tLoss 1.7239e+00 (1.7359e+00)\tAcc@1  61.72 ( 60.80)\tAcc@5  82.81 ( 81.64)\n",
      "Epoch: [34][3003/5005]\tTime  0.374 ( 2.572)\tData  0.000 ( 1.787)\tLoss 1.7266e+00 (1.7394e+00)\tAcc@1  58.20 ( 60.75)\tAcc@5  83.59 ( 81.58)\n",
      "Epoch: [34][4004/5005]\tTime  0.372 ( 2.429)\tData  0.000 ( 1.704)\tLoss 1.6768e+00 (1.7403e+00)\tAcc@1  59.38 ( 60.74)\tAcc@5  82.42 ( 81.56)\n",
      " * Acc@1 63.942 Acc@5 85.392\n",
      "************train_loss 1.7416431733301947 val_acc 63.94199752807617*************\n",
      "Epoch: [35][   0/5005]\tTime 160.191 (160.191)\tData 83.574 (83.574)\tLoss 1.4658e+00 (1.4658e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  85.94 ( 85.94)\n",
      "Epoch: [35][1001/5005]\tTime  0.370 ( 2.475)\tData  0.000 ( 1.770)\tLoss 1.7969e+00 (1.7089e+00)\tAcc@1  62.11 ( 61.35)\tAcc@5  80.86 ( 82.00)\n",
      "Epoch: [35][2002/5005]\tTime  0.628 ( 2.291)\tData  0.493 ( 1.636)\tLoss 1.6314e+00 (1.7125e+00)\tAcc@1  59.77 ( 61.24)\tAcc@5  85.94 ( 81.93)\n",
      "Epoch: [35][3003/5005]\tTime  0.377 ( 2.231)\tData  0.000 ( 1.602)\tLoss 1.9595e+00 (1.7179e+00)\tAcc@1  56.64 ( 61.15)\tAcc@5  79.30 ( 81.87)\n",
      "Epoch: [35][4004/5005]\tTime  0.378 ( 2.248)\tData  0.000 ( 1.610)\tLoss 1.6233e+00 (1.7206e+00)\tAcc@1  60.16 ( 61.09)\tAcc@5  83.98 ( 81.84)\n",
      " * Acc@1 63.964 Acc@5 85.510\n",
      "************train_loss 1.7230659078527522 val_acc 63.96399688720703*************\n",
      "Epoch: [36][   0/5005]\tTime 29.415 (29.415)\tData 17.348 (17.348)\tLoss 1.9272e+00 (1.9272e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  82.03 ( 82.03)\n",
      "Epoch: [36][1001/5005]\tTime  0.377 ( 2.259)\tData  0.000 ( 1.672)\tLoss 1.7088e+00 (1.6917e+00)\tAcc@1  61.33 ( 61.65)\tAcc@5  82.03 ( 82.22)\n",
      "Epoch: [36][2002/5005]\tTime  0.382 ( 2.682)\tData  0.000 ( 1.884)\tLoss 1.6998e+00 (1.6989e+00)\tAcc@1  60.94 ( 61.53)\tAcc@5  78.52 ( 82.12)\n",
      "Epoch: [36][3003/5005]\tTime  0.373 ( 2.626)\tData  0.000 ( 1.868)\tLoss 1.8030e+00 (1.7052e+00)\tAcc@1  58.59 ( 61.42)\tAcc@5  79.30 ( 82.04)\n",
      "Epoch: [36][4004/5005]\tTime  0.384 ( 2.484)\tData  0.000 ( 1.773)\tLoss 1.7871e+00 (1.7089e+00)\tAcc@1  58.98 ( 61.36)\tAcc@5  80.47 ( 82.00)\n",
      " * Acc@1 63.820 Acc@5 85.134\n",
      "************train_loss 1.7113509501372421 val_acc 63.81999969482422*************\n",
      "Epoch: [37][   0/5005]\tTime 67.830 (67.830)\tData 43.300 (43.300)\tLoss 1.6906e+00 (1.6906e+00)\tAcc@1  62.11 ( 62.11)\tAcc@5  81.25 ( 81.25)\n",
      "Epoch: [37][1001/5005]\tTime  1.531 ( 3.418)\tData  1.397 ( 2.317)\tLoss 1.5554e+00 (1.6783e+00)\tAcc@1  61.72 ( 61.79)\tAcc@5  82.81 ( 82.46)\n",
      "Epoch: [37][2002/5005]\tTime  3.517 ( 3.153)\tData  3.375 ( 2.154)\tLoss 1.8429e+00 (1.6876e+00)\tAcc@1  59.77 ( 61.64)\tAcc@5  81.64 ( 82.31)\n",
      "Epoch: [37][3003/5005]\tTime  0.381 ( 2.985)\tData  0.000 ( 2.028)\tLoss 1.9378e+00 (1.6941e+00)\tAcc@1  60.16 ( 61.55)\tAcc@5  76.56 ( 82.22)\n",
      "Epoch: [37][4004/5005]\tTime  0.382 ( 2.796)\tData  0.000 ( 1.905)\tLoss 1.5645e+00 (1.7003e+00)\tAcc@1  65.62 ( 61.46)\tAcc@5  83.20 ( 82.14)\n",
      " * Acc@1 64.404 Acc@5 85.764\n",
      "************train_loss 1.7033858326407936 val_acc 64.40399932861328*************\n",
      "Epoch: [38][   0/5005]\tTime 40.617 (40.617)\tData 24.826 (24.826)\tLoss 1.5460e+00 (1.5460e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  82.81 ( 82.81)\n",
      "Epoch: [38][1001/5005]\tTime  0.377 ( 2.060)\tData  0.000 ( 1.508)\tLoss 1.7195e+00 (1.6761e+00)\tAcc@1  62.11 ( 61.92)\tAcc@5  83.98 ( 82.49)\n",
      "Epoch: [38][2002/5005]\tTime  0.387 ( 2.642)\tData  0.000 ( 1.766)\tLoss 1.5450e+00 (1.6809e+00)\tAcc@1  65.23 ( 61.76)\tAcc@5  83.20 ( 82.43)\n",
      "Epoch: [38][3003/5005]\tTime  0.395 ( 2.768)\tData  0.000 ( 1.895)\tLoss 1.8386e+00 (1.6881e+00)\tAcc@1  58.59 ( 61.67)\tAcc@5  80.47 ( 82.32)\n",
      "Epoch: [38][4004/5005]\tTime  0.370 ( 2.839)\tData  0.000 ( 1.943)\tLoss 1.8654e+00 (1.6919e+00)\tAcc@1  60.16 ( 61.60)\tAcc@5  80.86 ( 82.28)\n",
      " * Acc@1 63.788 Acc@5 85.508\n",
      "************train_loss 1.6978628575861394 val_acc 63.78799819946289*************\n",
      "Epoch: [39][   0/5005]\tTime 37.319 (37.319)\tData 22.925 (22.925)\tLoss 1.4703e+00 (1.4703e+00)\tAcc@1  66.80 ( 66.80)\tAcc@5  84.77 ( 84.77)\n",
      "Epoch: [39][1001/5005]\tTime  0.382 ( 2.737)\tData  0.000 ( 1.790)\tLoss 1.8016e+00 (1.6710e+00)\tAcc@1  60.16 ( 62.04)\tAcc@5  80.47 ( 82.59)\n",
      "Epoch: [39][2002/5005]\tTime 10.170 ( 2.783)\tData 10.025 ( 1.903)\tLoss 1.7684e+00 (1.6806e+00)\tAcc@1  59.77 ( 61.81)\tAcc@5  83.59 ( 82.43)\n",
      "Epoch: [39][3003/5005]\tTime 14.484 ( 2.857)\tData 14.322 ( 1.944)\tLoss 1.7678e+00 (1.6872e+00)\tAcc@1  61.72 ( 61.67)\tAcc@5  81.25 ( 82.31)\n",
      "Epoch: [39][4004/5005]\tTime  0.380 ( 2.627)\tData  0.000 ( 1.821)\tLoss 1.5525e+00 (1.6902e+00)\tAcc@1  65.23 ( 61.60)\tAcc@5  84.77 ( 82.28)\n",
      " * Acc@1 63.932 Acc@5 85.320\n",
      "************train_loss 1.692713611466544 val_acc 63.93199920654297*************\n",
      "Epoch: [40][   0/5005]\tTime 58.061 (58.061)\tData 34.190 (34.190)\tLoss 1.6355e+00 (1.6355e+00)\tAcc@1  63.67 ( 63.67)\tAcc@5  85.55 ( 85.55)\n",
      "Epoch: [40][1001/5005]\tTime 15.113 ( 2.164)\tData 13.978 ( 1.591)\tLoss 1.6097e+00 (1.6643e+00)\tAcc@1  63.67 ( 62.19)\tAcc@5  84.77 ( 82.60)\n",
      "Epoch: [40][2002/5005]\tTime 41.357 ( 2.314)\tData 22.847 ( 1.665)\tLoss 1.6511e+00 (1.6753e+00)\tAcc@1  62.11 ( 61.92)\tAcc@5  83.59 ( 82.50)\n",
      "Epoch: [40][3003/5005]\tTime  0.379 ( 2.390)\tData  0.000 ( 1.709)\tLoss 1.8230e+00 (1.6794e+00)\tAcc@1  58.59 ( 61.84)\tAcc@5  77.34 ( 82.44)\n",
      "Epoch: [40][4004/5005]\tTime  0.378 ( 2.526)\tData  0.000 ( 1.797)\tLoss 1.7987e+00 (1.6853e+00)\tAcc@1  58.98 ( 61.74)\tAcc@5  81.25 ( 82.37)\n",
      " * Acc@1 63.730 Acc@5 85.160\n",
      "************train_loss 1.6920295829896803 val_acc 63.72999954223633*************\n",
      "Epoch: [41][   0/5005]\tTime 17.593 (17.593)\tData 14.443 (14.443)\tLoss 1.6393e+00 (1.6393e+00)\tAcc@1  62.11 ( 62.11)\tAcc@5  81.64 ( 81.64)\n",
      "Epoch: [41][1001/5005]\tTime  1.569 ( 1.664)\tData  1.432 ( 1.222)\tLoss 1.7781e+00 (1.6671e+00)\tAcc@1  57.81 ( 62.12)\tAcc@5  81.25 ( 82.65)\n",
      "Epoch: [41][2002/5005]\tTime 72.013 ( 1.992)\tData 33.279 ( 1.418)\tLoss 1.9543e+00 (1.6797e+00)\tAcc@1  57.42 ( 61.90)\tAcc@5  76.17 ( 82.45)\n",
      "Epoch: [41][3003/5005]\tTime  0.388 ( 2.200)\tData  0.000 ( 1.531)\tLoss 1.7465e+00 (1.6820e+00)\tAcc@1  62.11 ( 61.82)\tAcc@5  78.91 ( 82.43)\n",
      "Epoch: [41][4004/5005]\tTime  0.374 ( 2.413)\tData  0.000 ( 1.641)\tLoss 1.6084e+00 (1.6875e+00)\tAcc@1  60.55 ( 61.69)\tAcc@5  84.38 ( 82.35)\n",
      " * Acc@1 63.998 Acc@5 85.350\n",
      "************train_loss 1.6924467589114454 val_acc 63.99799728393555*************\n",
      "Epoch: [42][   0/5005]\tTime 76.257 (76.257)\tData 47.995 (47.995)\tLoss 1.5039e+00 (1.5039e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  87.50 ( 87.50)\n",
      "Epoch: [42][1001/5005]\tTime  8.317 ( 2.710)\tData  8.176 ( 1.935)\tLoss 1.5162e+00 (1.6612e+00)\tAcc@1  67.19 ( 62.09)\tAcc@5  83.59 ( 82.75)\n",
      "Epoch: [42][2002/5005]\tTime  0.381 ( 2.447)\tData  0.000 ( 1.767)\tLoss 1.6532e+00 (1.6710e+00)\tAcc@1  61.72 ( 61.97)\tAcc@5  83.98 ( 82.58)\n",
      "Epoch: [42][3003/5005]\tTime  0.378 ( 2.446)\tData  0.000 ( 1.735)\tLoss 1.9926e+00 (1.6817e+00)\tAcc@1  58.98 ( 61.79)\tAcc@5  74.61 ( 82.45)\n",
      "Epoch: [42][4004/5005]\tTime  0.387 ( 2.344)\tData  0.000 ( 1.683)\tLoss 1.8620e+00 (1.6860e+00)\tAcc@1  57.42 ( 61.72)\tAcc@5  80.08 ( 82.37)\n",
      " * Acc@1 63.958 Acc@5 85.498\n",
      "************train_loss 1.6907686176119032 val_acc 63.95800018310547*************\n",
      "Epoch: [43][   0/5005]\tTime 64.713 (64.713)\tData 39.821 (39.821)\tLoss 1.4128e+00 (1.4128e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  85.55 ( 85.55)\n",
      "Epoch: [43][1001/5005]\tTime  0.382 ( 2.362)\tData  0.000 ( 1.643)\tLoss 1.6055e+00 (1.6600e+00)\tAcc@1  63.28 ( 62.20)\tAcc@5  82.03 ( 82.74)\n",
      "Epoch: [43][2002/5005]\tTime  0.388 ( 2.318)\tData  0.001 ( 1.610)\tLoss 1.7778e+00 (1.6716e+00)\tAcc@1  62.11 ( 61.97)\tAcc@5  80.08 ( 82.58)\n",
      "Epoch: [43][3003/5005]\tTime  0.547 ( 2.176)\tData  0.413 ( 1.525)\tLoss 1.4194e+00 (1.6772e+00)\tAcc@1  68.36 ( 61.85)\tAcc@5  86.72 ( 82.50)\n",
      "Epoch: [43][4004/5005]\tTime  0.374 ( 2.378)\tData  0.000 ( 1.665)\tLoss 1.5704e+00 (1.6829e+00)\tAcc@1  62.50 ( 61.74)\tAcc@5  81.64 ( 82.44)\n",
      " * Acc@1 63.696 Acc@5 85.298\n",
      "************train_loss 1.6885396260957974 val_acc 63.69599914550781*************\n",
      "Epoch: [44][   0/5005]\tTime 113.431 (113.431)\tData 54.103 (54.103)\tLoss 1.5928e+00 (1.5928e+00)\tAcc@1  62.11 ( 62.11)\tAcc@5  85.16 ( 85.16)\n",
      "Epoch: [44][1001/5005]\tTime  0.375 ( 2.859)\tData  0.000 ( 1.873)\tLoss 1.8740e+00 (1.6618e+00)\tAcc@1  56.64 ( 62.12)\tAcc@5  82.81 ( 82.72)\n",
      "Epoch: [44][2002/5005]\tTime  0.388 ( 2.629)\tData  0.000 ( 1.759)\tLoss 1.5348e+00 (1.6702e+00)\tAcc@1  64.45 ( 61.93)\tAcc@5  83.59 ( 82.60)\n",
      "Epoch: [44][3003/5005]\tTime  2.154 ( 2.568)\tData  2.006 ( 1.751)\tLoss 1.7060e+00 (1.6765e+00)\tAcc@1  61.72 ( 61.80)\tAcc@5  83.98 ( 82.53)\n",
      "Epoch: [44][4004/5005]\tTime 10.117 ( 2.393)\tData  9.971 ( 1.659)\tLoss 1.7377e+00 (1.6840e+00)\tAcc@1  60.55 ( 61.68)\tAcc@5  80.86 ( 82.41)\n",
      " * Acc@1 63.242 Acc@5 85.080\n",
      "************train_loss 1.6897578047705697 val_acc 63.24199676513672*************\n",
      "Epoch: [45][   0/5005]\tTime 94.221 (94.221)\tData 63.459 (63.459)\tLoss 1.7737e+00 (1.7737e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  81.64 ( 81.64)\n",
      "Epoch: [45][1001/5005]\tTime  0.383 ( 2.829)\tData  0.000 ( 1.891)\tLoss 1.7539e+00 (1.6605e+00)\tAcc@1  59.38 ( 62.13)\tAcc@5  82.42 ( 82.80)\n",
      "Epoch: [45][2002/5005]\tTime  0.374 ( 2.724)\tData  0.000 ( 1.834)\tLoss 1.4154e+00 (1.6695e+00)\tAcc@1  64.06 ( 61.96)\tAcc@5  85.55 ( 82.66)\n",
      "Epoch: [45][3003/5005]\tTime  0.381 ( 2.402)\tData  0.000 ( 1.640)\tLoss 1.7614e+00 (1.6783e+00)\tAcc@1  59.38 ( 61.79)\tAcc@5  82.03 ( 82.55)\n",
      "Epoch: [45][4004/5005]\tTime  0.379 ( 2.353)\tData  0.000 ( 1.600)\tLoss 1.6332e+00 (1.6832e+00)\tAcc@1  57.03 ( 61.72)\tAcc@5  83.98 ( 82.48)\n",
      " * Acc@1 63.450 Acc@5 85.218\n",
      "************train_loss 1.6878902385761212 val_acc 63.44999694824219*************\n",
      "Epoch: [46][   0/5005]\tTime 91.296 (91.296)\tData 52.550 (52.550)\tLoss 1.6534e+00 (1.6534e+00)\tAcc@1  61.72 ( 61.72)\tAcc@5  82.42 ( 82.42)\n",
      "Epoch: [46][1001/5005]\tTime  0.731 ( 2.618)\tData  0.584 ( 1.790)\tLoss 1.8879e+00 (1.6563e+00)\tAcc@1  57.81 ( 62.27)\tAcc@5  79.30 ( 82.78)\n",
      "Epoch: [46][2002/5005]\tTime  3.124 ( 2.618)\tData  2.984 ( 1.868)\tLoss 1.6248e+00 (1.6662e+00)\tAcc@1  62.89 ( 62.14)\tAcc@5  83.20 ( 82.67)\n",
      "Epoch: [46][3003/5005]\tTime  0.431 ( 2.790)\tData  0.293 ( 1.926)\tLoss 1.6302e+00 (1.6750e+00)\tAcc@1  55.86 ( 61.95)\tAcc@5  84.38 ( 82.53)\n",
      "Epoch: [46][4004/5005]\tTime  8.182 ( 2.646)\tData  8.037 ( 1.862)\tLoss 1.8438e+00 (1.6810e+00)\tAcc@1  61.33 ( 61.79)\tAcc@5  78.12 ( 82.48)\n",
      " * Acc@1 63.990 Acc@5 85.532\n",
      "************train_loss 1.6866366990439066 val_acc 63.98999786376953*************\n",
      "Epoch: [47][   0/5005]\tTime 36.248 (36.248)\tData 24.179 (24.179)\tLoss 1.6541e+00 (1.6541e+00)\tAcc@1  64.45 ( 64.45)\tAcc@5  82.03 ( 82.03)\n",
      "Epoch: [47][1001/5005]\tTime  0.368 ( 2.641)\tData  0.000 ( 1.958)\tLoss 1.6481e+00 (1.6592e+00)\tAcc@1  62.11 ( 62.22)\tAcc@5  80.86 ( 82.78)\n",
      "Epoch: [47][2002/5005]\tTime 15.551 ( 2.803)\tData  4.747 ( 1.981)\tLoss 1.9582e+00 (1.6717e+00)\tAcc@1  57.81 ( 61.92)\tAcc@5  77.73 ( 82.61)\n",
      "Epoch: [47][3003/5005]\tTime  0.376 ( 2.778)\tData  0.000 ( 1.929)\tLoss 1.7983e+00 (1.6774e+00)\tAcc@1  63.67 ( 61.82)\tAcc@5  80.86 ( 82.53)\n",
      "Epoch: [47][4004/5005]\tTime  0.378 ( 2.576)\tData  0.000 ( 1.806)\tLoss 1.8279e+00 (1.6842e+00)\tAcc@1  55.86 ( 61.68)\tAcc@5  84.38 ( 82.41)\n",
      " * Acc@1 63.442 Acc@5 85.094\n",
      "************train_loss 1.688199867116107 val_acc 63.44199752807617*************\n",
      "Epoch: [48][   0/5005]\tTime 61.818 (61.818)\tData 28.129 (28.129)\tLoss 1.5888e+00 (1.5888e+00)\tAcc@1  61.72 ( 61.72)\tAcc@5  83.20 ( 83.20)\n",
      "Epoch: [48][1001/5005]\tTime  0.375 ( 2.262)\tData  0.000 ( 1.574)\tLoss 1.6407e+00 (1.6549e+00)\tAcc@1  60.55 ( 62.24)\tAcc@5  82.81 ( 82.76)\n",
      "Epoch: [48][2002/5005]\tTime  0.383 ( 2.399)\tData  0.000 ( 1.649)\tLoss 1.9863e+00 (1.6664e+00)\tAcc@1  60.55 ( 62.06)\tAcc@5  76.17 ( 82.63)\n",
      "Epoch: [48][3003/5005]\tTime  0.234 ( 2.485)\tData  0.088 ( 1.698)\tLoss 1.5724e+00 (1.6695e+00)\tAcc@1  58.20 ( 62.01)\tAcc@5  85.55 ( 82.61)\n",
      "Epoch: [48][4004/5005]\tTime  0.384 ( 2.654)\tData  0.000 ( 1.773)\tLoss 1.7049e+00 (1.6769e+00)\tAcc@1  58.20 ( 61.85)\tAcc@5  81.25 ( 82.51)\n",
      " * Acc@1 63.382 Acc@5 85.072\n",
      "************train_loss 1.6818573032583033 val_acc 63.38199996948242*************\n",
      "Epoch: [49][   0/5005]\tTime 55.993 (55.993)\tData 28.542 (28.542)\tLoss 1.5793e+00 (1.5793e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  83.20 ( 83.20)\n",
      "Epoch: [49][1001/5005]\tTime  0.377 ( 2.185)\tData  0.000 ( 1.505)\tLoss 1.7576e+00 (1.6513e+00)\tAcc@1  59.77 ( 62.37)\tAcc@5  81.25 ( 82.84)\n",
      "Epoch: [49][2002/5005]\tTime  0.384 ( 2.372)\tData  0.000 ( 1.617)\tLoss 1.8571e+00 (1.6654e+00)\tAcc@1  59.77 ( 62.11)\tAcc@5  80.86 ( 82.67)\n",
      "Epoch: [49][3003/5005]\tTime  0.378 ( 2.699)\tData  0.000 ( 1.784)\tLoss 1.5178e+00 (1.6743e+00)\tAcc@1  64.45 ( 61.91)\tAcc@5  83.20 ( 82.53)\n",
      "Epoch: [49][4004/5005]\tTime  0.382 ( 2.567)\tData  0.000 ( 1.730)\tLoss 1.6646e+00 (1.6788e+00)\tAcc@1  59.38 ( 61.85)\tAcc@5  84.38 ( 82.47)\n",
      " * Acc@1 63.556 Acc@5 85.188\n",
      "************train_loss 1.6829305855782477 val_acc 63.555999755859375*************\n",
      "Epoch: [50][   0/5005]\tTime 51.024 (51.024)\tData 34.255 (34.255)\tLoss 1.6674e+00 (1.6674e+00)\tAcc@1  62.50 ( 62.50)\tAcc@5  80.86 ( 80.86)\n",
      "Epoch: [50][1001/5005]\tTime 42.565 ( 2.368)\tData 28.206 ( 1.642)\tLoss 1.7843e+00 (1.6565e+00)\tAcc@1  59.77 ( 62.20)\tAcc@5  80.08 ( 82.72)\n",
      "Epoch: [50][2002/5005]\tTime  0.383 ( 2.348)\tData  0.000 ( 1.680)\tLoss 1.5236e+00 (1.6635e+00)\tAcc@1  64.45 ( 62.02)\tAcc@5  86.33 ( 82.70)\n",
      "Epoch: [50][3003/5005]\tTime  0.373 ( 2.172)\tData  0.000 ( 1.555)\tLoss 1.7082e+00 (1.6695e+00)\tAcc@1  61.72 ( 61.96)\tAcc@5  81.64 ( 82.61)\n",
      "Epoch: [50][4004/5005]\tTime  7.859 ( 2.424)\tData  7.715 ( 1.682)\tLoss 1.5341e+00 (1.6752e+00)\tAcc@1  64.84 ( 61.88)\tAcc@5  84.77 ( 82.54)\n",
      " * Acc@1 63.572 Acc@5 85.070\n",
      "************train_loss 1.6782491287627777 val_acc 63.571998596191406*************\n",
      "Epoch: [51][   0/5005]\tTime 87.193 (87.193)\tData 32.173 (32.173)\tLoss 1.6373e+00 (1.6373e+00)\tAcc@1  60.55 ( 60.55)\tAcc@5  82.81 ( 82.81)\n",
      "Epoch: [51][1001/5005]\tTime  0.392 ( 2.307)\tData  0.000 ( 1.723)\tLoss 1.5906e+00 (1.6470e+00)\tAcc@1  59.38 ( 62.52)\tAcc@5  83.20 ( 82.98)\n",
      "Epoch: [51][2002/5005]\tTime  0.581 ( 1.967)\tData  0.447 ( 1.447)\tLoss 1.5752e+00 (1.6575e+00)\tAcc@1  63.28 ( 62.28)\tAcc@5  81.64 ( 82.76)\n",
      "Epoch: [51][3003/5005]\tTime  0.369 ( 2.191)\tData  0.000 ( 1.593)\tLoss 1.7042e+00 (1.6657e+00)\tAcc@1  59.38 ( 62.08)\tAcc@5  82.42 ( 82.65)\n",
      "Epoch: [51][4004/5005]\tTime  7.357 ( 2.297)\tData  7.213 ( 1.632)\tLoss 1.8899e+00 (1.6738e+00)\tAcc@1  57.42 ( 61.93)\tAcc@5  79.69 ( 82.56)\n",
      " * Acc@1 63.652 Acc@5 85.152\n",
      "************train_loss 1.6783125719704948 val_acc 63.65199661254883*************\n",
      "Epoch: [52][   0/5005]\tTime 136.423 (136.423)\tData 70.837 (70.837)\tLoss 1.5625e+00 (1.5625e+00)\tAcc@1  62.50 ( 62.50)\tAcc@5  85.16 ( 85.16)\n",
      "Epoch: [52][1001/5005]\tTime  1.222 ( 2.598)\tData  1.089 ( 1.883)\tLoss 1.4939e+00 (1.6419e+00)\tAcc@1  67.97 ( 62.60)\tAcc@5  83.59 ( 83.05)\n",
      "Epoch: [52][2002/5005]\tTime  0.387 ( 2.668)\tData  0.000 ( 1.906)\tLoss 1.6952e+00 (1.6583e+00)\tAcc@1  60.94 ( 62.22)\tAcc@5  83.20 ( 82.80)\n",
      "Epoch: [52][3003/5005]\tTime  0.380 ( 2.435)\tData  0.000 ( 1.750)\tLoss 1.6803e+00 (1.6634e+00)\tAcc@1  59.77 ( 62.12)\tAcc@5  83.98 ( 82.74)\n",
      "Epoch: [52][4004/5005]\tTime  2.679 ( 2.294)\tData  2.539 ( 1.669)\tLoss 1.6158e+00 (1.6679e+00)\tAcc@1  66.02 ( 62.04)\tAcc@5  82.03 ( 82.65)\n",
      " * Acc@1 63.560 Acc@5 85.202\n",
      "************train_loss 1.6738000975741254 val_acc 63.55999755859375*************\n",
      "Epoch: [53][   0/5005]\tTime 124.590 (124.590)\tData 64.940 (64.940)\tLoss 1.8180e+00 (1.8180e+00)\tAcc@1  58.98 ( 58.98)\tAcc@5  80.08 ( 80.08)\n",
      "Epoch: [53][1001/5005]\tTime  0.383 ( 2.829)\tData  0.000 ( 1.920)\tLoss 1.6151e+00 (1.6506e+00)\tAcc@1  65.62 ( 62.34)\tAcc@5  83.20 ( 82.86)\n",
      "Epoch: [53][2002/5005]\tTime  6.670 ( 2.480)\tData  6.530 ( 1.693)\tLoss 1.6030e+00 (1.6571e+00)\tAcc@1  60.55 ( 62.24)\tAcc@5  83.59 ( 82.78)\n",
      "Epoch: [53][3003/5005]\tTime  0.377 ( 2.111)\tData  0.000 ( 1.452)\tLoss 1.4934e+00 (1.6650e+00)\tAcc@1  63.67 ( 62.06)\tAcc@5  84.38 ( 82.68)\n",
      "Epoch: [53][4004/5005]\tTime 10.790 ( 2.196)\tData 10.640 ( 1.540)\tLoss 1.5397e+00 (1.6698e+00)\tAcc@1  64.84 ( 61.97)\tAcc@5  84.38 ( 82.61)\n",
      " * Acc@1 63.662 Acc@5 85.444\n",
      "************train_loss 1.6745758855735862 val_acc 63.6619987487793*************\n",
      "Epoch: [54][   0/5005]\tTime 68.506 (68.506)\tData 43.182 (43.182)\tLoss 1.4883e+00 (1.4883e+00)\tAcc@1  63.28 ( 63.28)\tAcc@5  85.94 ( 85.94)\n",
      "Epoch: [54][1001/5005]\tTime  0.375 ( 2.665)\tData  0.000 ( 1.914)\tLoss 1.9734e+00 (1.6342e+00)\tAcc@1  57.42 ( 62.64)\tAcc@5  79.30 ( 83.11)\n",
      "Epoch: [54][2002/5005]\tTime  0.378 ( 2.478)\tData  0.000 ( 1.746)\tLoss 1.5654e+00 (1.6469e+00)\tAcc@1  67.19 ( 62.43)\tAcc@5  84.38 ( 82.95)\n",
      "Epoch: [54][3003/5005]\tTime  0.375 ( 2.629)\tData  0.000 ( 1.814)\tLoss 1.8538e+00 (1.6560e+00)\tAcc@1  57.42 ( 62.25)\tAcc@5  79.69 ( 82.84)\n",
      "Epoch: [54][4004/5005]\tTime  8.110 ( 2.431)\tData  7.962 ( 1.685)\tLoss 1.7547e+00 (1.6652e+00)\tAcc@1  61.72 ( 62.06)\tAcc@5  81.25 ( 82.68)\n",
      " * Acc@1 63.818 Acc@5 85.390\n",
      "************train_loss 1.66904698680569 val_acc 63.817996978759766*************\n",
      "Epoch: [55][   0/5005]\tTime 76.035 (76.035)\tData 44.721 (44.721)\tLoss 1.6889e+00 (1.6889e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  80.08 ( 80.08)\n",
      "Epoch: [55][1001/5005]\tTime  0.384 ( 2.790)\tData  0.000 ( 1.847)\tLoss 1.6622e+00 (1.6366e+00)\tAcc@1  61.33 ( 62.51)\tAcc@5  83.20 ( 83.02)\n",
      "Epoch: [55][2002/5005]\tTime  0.372 ( 2.873)\tData  0.000 ( 1.910)\tLoss 1.5670e+00 (1.6482e+00)\tAcc@1  64.84 ( 62.38)\tAcc@5  82.81 ( 82.86)\n",
      "Epoch: [55][3003/5005]\tTime  9.908 ( 2.627)\tData  9.758 ( 1.795)\tLoss 1.7144e+00 (1.6554e+00)\tAcc@1  58.98 ( 62.25)\tAcc@5  81.64 ( 82.77)\n",
      "Epoch: [55][4004/5005]\tTime  0.386 ( 2.437)\tData  0.000 ( 1.703)\tLoss 1.9142e+00 (1.6609e+00)\tAcc@1  59.77 ( 62.16)\tAcc@5  78.91 ( 82.70)\n",
      " * Acc@1 63.224 Acc@5 84.950\n",
      "************train_loss 1.6656615140316609 val_acc 63.2239990234375*************\n",
      "Epoch: [56][   0/5005]\tTime 47.064 (47.064)\tData 24.109 (24.109)\tLoss 1.4098e+00 (1.4098e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  87.11 ( 87.11)\n",
      "Epoch: [56][1001/5005]\tTime  0.400 ( 2.671)\tData  0.000 ( 1.782)\tLoss 1.7525e+00 (1.6338e+00)\tAcc@1  58.20 ( 62.70)\tAcc@5  81.25 ( 83.10)\n",
      "Epoch: [56][2002/5005]\tTime  0.380 ( 2.532)\tData  0.000 ( 1.717)\tLoss 1.7428e+00 (1.6460e+00)\tAcc@1  59.77 ( 62.50)\tAcc@5  81.64 ( 82.94)\n",
      "Epoch: [56][3003/5005]\tTime  0.367 ( 2.412)\tData  0.000 ( 1.660)\tLoss 1.4719e+00 (1.6530e+00)\tAcc@1  65.62 ( 62.35)\tAcc@5  84.38 ( 82.85)\n",
      "Epoch: [56][4004/5005]\tTime  5.041 ( 2.593)\tData  4.896 ( 1.740)\tLoss 1.8775e+00 (1.6600e+00)\tAcc@1  55.86 ( 62.21)\tAcc@5  79.30 ( 82.74)\n",
      " * Acc@1 63.206 Acc@5 85.078\n",
      "************train_loss 1.6638589272608648 val_acc 63.205997467041016*************\n",
      "Epoch: [57][   0/5005]\tTime 45.227 (45.227)\tData 27.169 (27.169)\tLoss 1.7837e+00 (1.7837e+00)\tAcc@1  59.77 ( 59.77)\tAcc@5  78.91 ( 78.91)\n",
      "Epoch: [57][1001/5005]\tTime  4.429 ( 2.181)\tData  4.280 ( 1.549)\tLoss 1.7093e+00 (1.6369e+00)\tAcc@1  62.50 ( 62.64)\tAcc@5  80.86 ( 83.02)\n",
      "Epoch: [57][2002/5005]\tTime  0.380 ( 2.059)\tData  0.000 ( 1.488)\tLoss 1.6551e+00 (1.6424e+00)\tAcc@1  64.06 ( 62.56)\tAcc@5  83.59 ( 82.98)\n",
      "Epoch: [57][3003/5005]\tTime  0.379 ( 2.347)\tData  0.000 ( 1.642)\tLoss 1.6653e+00 (1.6526e+00)\tAcc@1  67.19 ( 62.34)\tAcc@5  83.20 ( 82.83)\n"
     ]
    }
   ],
   "source": [
    "best_acc1 = 0\n",
    "acc1 = 0\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, args.lr)\n",
    "\n",
    "    # train for one epoch\n",
    "    epoch_loss = train(train_loader, model, criterion, optimizer, epoch, args, device)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion, args, device)  \n",
    "    \n",
    "    train_loss.append(epoch_loss)\n",
    "    val_acc.append(acc1)\n",
    "    print('************train_loss {} val_acc {}*************'.format(epoch_loss, acc1))\n",
    "    \n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "#     if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
    "#             and args.rank % ngpus_per_node == 0):\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': args.arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best, args, filename='../trained_model/sent2vec/checkpoint.pt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6de6a0-a209-4b66-b52d-322f333aacae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48e851-7663-40f7-b4d8-417cc3498927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e3c79-e618-4360-b0db-9efc176fbd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_retina",
   "language": "python",
   "name": "torch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
