{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ab6313-698a-46cc-9ab4-770b35f86211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# import random\n",
    "# import shutil\n",
    "# import time\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.parallel\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# import torch.distributed as dist\n",
    "\n",
    "import torch.optim\n",
    "# import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "# import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "# import torchvision.models as models\n",
    "from resnet import *\n",
    "\n",
    "from main import *\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56508d67-5301-4e1e-9454-1fe41b5f3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args(args=[])\n",
    "# args = parser.parse_args()\n",
    "import easydict \n",
    "args = easydict.EasyDict({ \"batch-size\": 256, \n",
    "                          \"epochs\": 100,\n",
    "                          \"data\": 0, \n",
    "                          'arch':'resnet18',\n",
    "                          'lr':0.1,\n",
    "                         'momentum':0.9,\n",
    "                         'weight_decay':1e-4,\n",
    "                         'start_epoch':0,\n",
    "                         'gpu':3,\n",
    "                         'saved_dir':'../trained_model/model_best.pt'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea598abf-d5fe-4f70-8cbf-78ad6038e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "cuda:3\n",
      "Current cuda device  3\n"
     ]
    }
   ],
   "source": [
    "ngpus_per_node = torch.cuda.device_count()\n",
    "print(ngpus_per_node)\n",
    "# device = torch.device('cpu')\n",
    "# device = torch.device('cuda')\n",
    "GPU_NUM = args.gpu # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "print(device)\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "# 4번 디바이스만 이용하려면 \"4\"를 입력\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print ('Current cuda device ', torch.cuda.current_device()) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b5c549-7611-40a8-a5cd-54451c41e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# imagenet_embeding = np.load('../data/imagenet_embeding.npy')\n",
    "# imagenet_embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5e59a2-80d4-49f7-8998-7b78846b333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet_embeding = torch.tensor(imagenet_embeding)\n",
    "# imagenet_embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e9ebbf-4193-412c-b416-9faccf6aac25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet18'\n"
     ]
    }
   ],
   "source": [
    "print(\"=> using pre-trained model '{}'\".format('resnet18'))\n",
    "# model = models.__dict__['resnet18'](pretrained=True)\n",
    "# model = models.resnet18(pretrained=False)\n",
    "model = resnet18(pretrained=False)\n",
    "# model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                            momentum=args.momentum,\n",
    "                            weight_decay=args.weight_decay)\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c3f88ac-915e-417c-8385-741b3991daf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0302, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30884528-5e32-4449-882d-18de2058aa2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), '../trained_model/init.pt')\n",
    "model.load_state_dict(torch.load('../trained_model/init.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c911ff-1595-4229-b0a3-3f5bee138747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0301, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8910807d-3862-4446-a811-530857664cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict = model.state_dict() \n",
    "# for k in model_dict :\n",
    "#     if 'fc.weight' in k :\n",
    "#         print(model_dict[k].shape)\n",
    "#         print(model_dict[k])\n",
    "#         model_dict[k] = imagenet_embeding\n",
    "# #     if 'fc.bias' in k :\n",
    "# #         print(model_dict[k])        \n",
    "# #     print(model_dict[k].shape)\n",
    "# model.load_state_dict(model_dict)\n",
    "# model_dict = model.state_dict() \n",
    "# for k in model_dict :\n",
    "#     if 'fc.weight' in k :\n",
    "# #         print(model_dict[k].shape)\n",
    "#         print(model_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66335ead-2157-4014-8671-62dbab63a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fc.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77598382-abf2-459f-9457-f8bdd1cf8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters() :\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5493c8ea-4540-4067-8954-9e7a73cb10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6cd832a-5720-4925-9d50-36b16480132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "data_dir = '../ILSVRC/Data/CLS-LOC/'\n",
    "traindir = os.path.join(data_dir, 'train')\n",
    "valdir = os.path.join(data_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb79f3f-e8a8-4e0d-9a55-f7dbf5c08236",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3d9c94c-b533-4147-96f0-57afbf42624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4460bdaa-261a-4858-a94c-c70aebd6fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "train_sampler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beece8cc-7ac9-4e30-8cca-0ca4adbd58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=256, shuffle=(train_sampler is None),\n",
    "    num_workers=8, pin_memory=True, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53171b34-4e2f-4b1b-ae3f-d8d2db58b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32, shuffle=False,\n",
    "    num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afed6352-ebb9-4fcf-ade1-12354b4e7755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(384, 768, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25092250-abbb-482c-bb3a-3c279e593859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon/anaconda3/envs/torch_retina/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][   0/5005]\tTime 60.624 (60.624)\tData 34.235 (34.235)\tLoss 7.0416e+00 (7.0416e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\n",
      "Epoch: [0][1001/5005]\tTime  0.379 ( 1.922)\tData  0.000 ( 1.361)\tLoss 5.6062e+00 (6.1895e+00)\tAcc@1   3.52 (  1.99)\tAcc@5  14.84 (  6.97)\n",
      "Epoch: [0][2002/5005]\tTime  0.365 ( 2.160)\tData  0.000 ( 1.491)\tLoss 4.9209e+00 (5.7257e+00)\tAcc@1   7.81 (  4.45)\tAcc@5  22.27 ( 13.16)\n",
      "Epoch: [0][3003/5005]\tTime  0.385 ( 2.411)\tData  0.000 ( 1.617)\tLoss 4.5214e+00 (5.3751e+00)\tAcc@1  14.84 (  7.14)\tAcc@5  34.38 ( 18.74)\n",
      "Epoch: [0][4004/5005]\tTime  0.379 ( 2.330)\tData  0.000 ( 1.593)\tLoss 4.0172e+00 (5.0981e+00)\tAcc@1  20.70 (  9.68)\tAcc@5  42.97 ( 23.49)\n",
      " * Acc@1 22.670 Acc@5 46.460\n",
      "************train_loss 4.8716156125902295 val_acc 22.670000076293945*************\n",
      "Epoch: [1][   0/5005]\tTime 34.107 (34.107)\tData 23.164 (23.164)\tLoss 3.9349e+00 (3.9349e+00)\tAcc@1  21.88 ( 21.88)\tAcc@5  44.14 ( 44.14)\n",
      "Epoch: [1][1001/5005]\tTime  0.387 ( 2.504)\tData  0.000 ( 1.641)\tLoss 3.5058e+00 (3.6951e+00)\tAcc@1  29.69 ( 25.02)\tAcc@5  53.91 ( 48.58)\n",
      "Epoch: [1][2002/5005]\tTime  0.379 ( 2.809)\tData  0.000 ( 1.811)\tLoss 3.5222e+00 (3.6090e+00)\tAcc@1  28.91 ( 26.34)\tAcc@5  54.30 ( 50.23)\n",
      "Epoch: [1][3003/5005]\tTime  0.385 ( 2.649)\tData  0.000 ( 1.761)\tLoss 3.2414e+00 (3.5310e+00)\tAcc@1  32.03 ( 27.55)\tAcc@5  57.03 ( 51.69)\n",
      "Epoch: [1][4004/5005]\tTime 14.370 ( 2.691)\tData 14.223 ( 1.769)\tLoss 3.1878e+00 (3.4635e+00)\tAcc@1  32.42 ( 28.63)\tAcc@5  57.42 ( 52.96)\n",
      " * Acc@1 35.784 Acc@5 62.198\n",
      "************train_loss 3.4039449173015552 val_acc 35.784000396728516*************\n",
      "Epoch: [2][   0/5005]\tTime 147.213 (147.213)\tData 70.165 (70.165)\tLoss 3.1937e+00 (3.1937e+00)\tAcc@1  33.20 ( 33.20)\tAcc@5  56.64 ( 56.64)\n",
      "Epoch: [2][1001/5005]\tTime  0.374 ( 2.983)\tData  0.000 ( 1.881)\tLoss 3.0972e+00 (3.0552e+00)\tAcc@1  35.16 ( 35.21)\tAcc@5  60.16 ( 60.48)\n",
      "Epoch: [2][2002/5005]\tTime  1.316 ( 2.517)\tData  1.182 ( 1.664)\tLoss 3.0134e+00 (3.0285e+00)\tAcc@1  33.98 ( 35.65)\tAcc@5  59.38 ( 60.94)\n",
      "Epoch: [2][3003/5005]\tTime  0.383 ( 2.692)\tData  0.000 ( 1.791)\tLoss 2.9018e+00 (2.9994e+00)\tAcc@1  37.50 ( 36.19)\tAcc@5  60.55 ( 61.46)\n",
      "Epoch: [2][4004/5005]\tTime  0.381 ( 2.694)\tData  0.000 ( 1.776)\tLoss 2.6281e+00 (2.9750e+00)\tAcc@1  41.41 ( 36.61)\tAcc@5  66.02 ( 61.88)\n",
      " * Acc@1 41.140 Acc@5 67.604\n",
      "************train_loss 2.953343516891891 val_acc 41.13999938964844*************\n",
      "Epoch: [3][   0/5005]\tTime 55.351 (55.351)\tData 34.630 (34.630)\tLoss 2.7876e+00 (2.7876e+00)\tAcc@1  41.02 ( 41.02)\tAcc@5  64.84 ( 64.84)\n",
      "Epoch: [3][1001/5005]\tTime  2.845 ( 1.985)\tData  2.702 ( 1.414)\tLoss 2.7738e+00 (2.7928e+00)\tAcc@1  40.62 ( 39.71)\tAcc@5  67.19 ( 64.95)\n",
      "Epoch: [3][2002/5005]\tTime  0.486 ( 2.492)\tData  0.340 ( 1.677)\tLoss 3.0660e+00 (2.7832e+00)\tAcc@1  35.16 ( 39.89)\tAcc@5  61.33 ( 65.19)\n",
      "Epoch: [3][3003/5005]\tTime  0.372 ( 2.546)\tData  0.000 ( 1.681)\tLoss 2.7000e+00 (2.7737e+00)\tAcc@1  40.23 ( 40.09)\tAcc@5  66.02 ( 65.31)\n",
      "Epoch: [3][4004/5005]\tTime  0.285 ( 2.508)\tData  0.000 ( 1.686)\tLoss 2.5235e+00 (2.7620e+00)\tAcc@1  41.41 ( 40.32)\tAcc@5  72.27 ( 65.51)\n",
      " * Acc@1 43.814 Acc@5 70.624\n",
      "************train_loss 2.7517317675210378 val_acc 43.81399917602539*************\n",
      "Epoch: [4][   0/5005]\tTime 54.392 (54.392)\tData 28.399 (28.399)\tLoss 2.6621e+00 (2.6621e+00)\tAcc@1  42.97 ( 42.97)\tAcc@5  67.97 ( 67.97)\n",
      "Epoch: [4][1001/5005]\tTime  0.380 ( 2.731)\tData  0.000 ( 1.765)\tLoss 2.5201e+00 (2.6362e+00)\tAcc@1  41.41 ( 42.46)\tAcc@5  69.92 ( 67.63)\n",
      "Epoch: [4][2002/5005]\tTime  3.652 ( 2.429)\tData  3.510 ( 1.630)\tLoss 2.8723e+00 (2.6427e+00)\tAcc@1  36.72 ( 42.45)\tAcc@5  64.06 ( 67.55)\n",
      "Epoch: [4][3003/5005]\tTime  0.385 ( 2.250)\tData  0.001 ( 1.551)\tLoss 2.7657e+00 (2.6446e+00)\tAcc@1  41.02 ( 42.40)\tAcc@5  64.84 ( 67.52)\n",
      "Epoch: [4][4004/5005]\tTime  0.382 ( 2.403)\tData  0.000 ( 1.621)\tLoss 2.5421e+00 (2.6394e+00)\tAcc@1  42.19 ( 42.51)\tAcc@5  67.19 ( 67.62)\n",
      " * Acc@1 45.332 Acc@5 71.936\n",
      "************train_loss 2.634245185251836 val_acc 45.332000732421875*************\n",
      "Epoch: [5][   0/5005]\tTime 56.971 (56.971)\tData 27.569 (27.569)\tLoss 2.5920e+00 (2.5920e+00)\tAcc@1  44.92 ( 44.92)\tAcc@5  67.58 ( 67.58)\n",
      "Epoch: [5][1001/5005]\tTime  0.389 ( 2.424)\tData  0.000 ( 1.616)\tLoss 2.6221e+00 (2.5520e+00)\tAcc@1  44.92 ( 44.06)\tAcc@5  69.53 ( 69.13)\n",
      "Epoch: [5][2002/5005]\tTime  0.386 ( 2.266)\tData  0.000 ( 1.550)\tLoss 2.7154e+00 (2.5564e+00)\tAcc@1  42.19 ( 43.97)\tAcc@5  66.41 ( 69.01)\n",
      "Epoch: [5][3003/5005]\tTime  0.379 ( 2.459)\tData  0.000 ( 1.675)\tLoss 2.4600e+00 (2.5609e+00)\tAcc@1  47.66 ( 43.92)\tAcc@5  67.58 ( 68.93)\n",
      "Epoch: [5][4004/5005]\tTime  0.386 ( 2.432)\tData  0.000 ( 1.674)\tLoss 2.7917e+00 (2.5594e+00)\tAcc@1  38.67 ( 43.95)\tAcc@5  64.06 ( 68.95)\n",
      " * Acc@1 44.478 Acc@5 70.998\n",
      "************train_loss 2.5587230202677724 val_acc 44.47800064086914*************\n",
      "Epoch: [6][   0/5005]\tTime 54.688 (54.688)\tData 27.114 (27.114)\tLoss 2.4790e+00 (2.4790e+00)\tAcc@1  44.14 ( 44.14)\tAcc@5  70.31 ( 70.31)\n",
      "Epoch: [6][1001/5005]\tTime  0.393 ( 2.244)\tData  0.000 ( 1.607)\tLoss 2.5101e+00 (2.5017e+00)\tAcc@1  42.58 ( 44.99)\tAcc@5  69.14 ( 69.90)\n",
      "Epoch: [6][2002/5005]\tTime  0.376 ( 2.488)\tData  0.000 ( 1.726)\tLoss 2.4142e+00 (2.5096e+00)\tAcc@1  44.53 ( 44.84)\tAcc@5  70.31 ( 69.81)\n",
      "Epoch: [6][3003/5005]\tTime  0.377 ( 2.336)\tData  0.000 ( 1.637)\tLoss 2.3527e+00 (2.5119e+00)\tAcc@1  46.88 ( 44.83)\tAcc@5  71.88 ( 69.74)\n",
      "Epoch: [6][4004/5005]\tTime  2.102 ( 2.256)\tData  1.954 ( 1.586)\tLoss 2.3423e+00 (2.5134e+00)\tAcc@1  49.22 ( 44.82)\tAcc@5  73.05 ( 69.72)\n",
      " * Acc@1 47.834 Acc@5 73.826\n",
      "************train_loss 2.5109933370595927 val_acc 47.83399963378906*************\n",
      "Epoch: [7][   0/5005]\tTime 55.943 (55.943)\tData 32.903 (32.903)\tLoss 2.4501e+00 (2.4501e+00)\tAcc@1  46.09 ( 46.09)\tAcc@5  71.88 ( 71.88)\n",
      "Epoch: [7][1001/5005]\tTime  0.378 ( 2.549)\tData  0.000 ( 1.798)\tLoss 2.5015e+00 (2.4616e+00)\tAcc@1  46.09 ( 45.75)\tAcc@5  69.14 ( 70.61)\n",
      "Epoch: [7][2002/5005]\tTime  0.236 ( 2.398)\tData  0.000 ( 1.686)\tLoss 2.5906e+00 (2.4691e+00)\tAcc@1  44.53 ( 45.63)\tAcc@5  68.75 ( 70.50)\n",
      "Epoch: [7][3003/5005]\tTime  0.378 ( 2.238)\tData  0.000 ( 1.617)\tLoss 2.3381e+00 (2.4739e+00)\tAcc@1  47.27 ( 45.56)\tAcc@5  70.70 ( 70.42)\n",
      "Epoch: [7][4004/5005]\tTime  0.372 ( 2.369)\tData  0.000 ( 1.677)\tLoss 2.2648e+00 (2.4748e+00)\tAcc@1  49.22 ( 45.56)\tAcc@5  73.83 ( 70.38)\n",
      " * Acc@1 48.056 Acc@5 74.148\n",
      "************train_loss 2.4759147235563583 val_acc 48.055999755859375*************\n",
      "Epoch: [8][   0/5005]\tTime 80.811 (80.811)\tData 50.665 (50.665)\tLoss 2.2584e+00 (2.2584e+00)\tAcc@1  49.22 ( 49.22)\tAcc@5  72.27 ( 72.27)\n",
      "Epoch: [8][1001/5005]\tTime  0.375 ( 2.627)\tData  0.000 ( 1.747)\tLoss 2.2284e+00 (2.4271e+00)\tAcc@1  51.17 ( 46.33)\tAcc@5  75.00 ( 71.20)\n",
      "Epoch: [8][2002/5005]\tTime  0.378 ( 2.456)\tData  0.000 ( 1.669)\tLoss 2.5624e+00 (2.4395e+00)\tAcc@1  44.53 ( 46.18)\tAcc@5  66.80 ( 71.02)\n",
      "Epoch: [8][3003/5005]\tTime  5.621 ( 2.573)\tData  5.478 ( 1.718)\tLoss 2.5020e+00 (2.4414e+00)\tAcc@1  46.09 ( 46.16)\tAcc@5  70.31 ( 70.95)\n",
      "Epoch: [8][4004/5005]\tTime  9.830 ( 2.572)\tData  9.673 ( 1.714)\tLoss 2.4863e+00 (2.4447e+00)\tAcc@1  44.14 ( 46.13)\tAcc@5  67.58 ( 70.91)\n",
      " * Acc@1 48.658 Acc@5 74.698\n",
      "************train_loss 2.4466535155947033 val_acc 48.657997131347656*************\n",
      "Epoch: [9][   0/5005]\tTime 58.582 (58.582)\tData 32.459 (32.459)\tLoss 2.2908e+00 (2.2908e+00)\tAcc@1  47.66 ( 47.66)\tAcc@5  73.05 ( 73.05)\n",
      "Epoch: [9][1001/5005]\tTime  1.779 ( 2.341)\tData  1.617 ( 1.580)\tLoss 2.4364e+00 (2.4057e+00)\tAcc@1  48.83 ( 46.84)\tAcc@5  70.70 ( 71.45)\n",
      "Epoch: [9][2002/5005]\tTime  0.367 ( 2.494)\tData  0.000 ( 1.641)\tLoss 2.7291e+00 (2.4114e+00)\tAcc@1  41.80 ( 46.64)\tAcc@5  65.23 ( 71.37)\n",
      "Epoch: [9][3003/5005]\tTime  0.380 ( 2.481)\tData  0.000 ( 1.653)\tLoss 2.4047e+00 (2.4161e+00)\tAcc@1  48.83 ( 46.56)\tAcc@5  73.83 ( 71.32)\n",
      "Epoch: [9][4004/5005]\tTime  0.382 ( 2.337)\tData  0.000 ( 1.583)\tLoss 2.3013e+00 (2.4178e+00)\tAcc@1  51.17 ( 46.53)\tAcc@5  71.48 ( 71.32)\n",
      " * Acc@1 48.932 Acc@5 74.820\n",
      "************train_loss 2.419851233099367 val_acc 48.93199920654297*************\n",
      "Epoch: [10][   0/5005]\tTime 41.402 (41.402)\tData 27.011 (27.011)\tLoss 2.2759e+00 (2.2759e+00)\tAcc@1  47.27 ( 47.27)\tAcc@5  71.48 ( 71.48)\n",
      "Epoch: [10][1001/5005]\tTime  0.375 ( 2.548)\tData  0.000 ( 1.744)\tLoss 2.4122e+00 (2.3786e+00)\tAcc@1  46.09 ( 47.28)\tAcc@5  72.66 ( 71.92)\n",
      "Epoch: [10][2002/5005]\tTime  0.807 ( 2.544)\tData  0.673 ( 1.683)\tLoss 2.5454e+00 (2.3884e+00)\tAcc@1  45.31 ( 47.15)\tAcc@5  69.92 ( 71.74)\n",
      "Epoch: [10][3003/5005]\tTime  0.382 ( 2.289)\tData  0.000 ( 1.559)\tLoss 2.3178e+00 (2.3974e+00)\tAcc@1  50.00 ( 47.04)\tAcc@5  73.44 ( 71.62)\n",
      "Epoch: [10][4004/5005]\tTime  0.378 ( 2.319)\tData  0.000 ( 1.594)\tLoss 2.5692e+00 (2.4019e+00)\tAcc@1  42.58 ( 46.95)\tAcc@5  67.97 ( 71.57)\n",
      " * Acc@1 47.812 Acc@5 74.226\n",
      "************train_loss 2.4038947409564084 val_acc 47.8120002746582*************\n",
      "Epoch: [11][   0/5005]\tTime 71.233 (71.233)\tData 37.451 (37.451)\tLoss 2.1901e+00 (2.1901e+00)\tAcc@1  49.61 ( 49.61)\tAcc@5  72.27 ( 72.27)\n",
      "Epoch: [11][1001/5005]\tTime  4.453 ( 2.454)\tData  4.303 ( 1.623)\tLoss 2.3037e+00 (2.3738e+00)\tAcc@1  48.44 ( 47.30)\tAcc@5  73.05 ( 72.04)\n",
      "Epoch: [11][2002/5005]\tTime  0.383 ( 2.309)\tData  0.000 ( 1.577)\tLoss 2.2738e+00 (2.3788e+00)\tAcc@1  51.17 ( 47.23)\tAcc@5  74.22 ( 71.98)\n",
      "Epoch: [11][3003/5005]\tTime  0.367 ( 2.431)\tData  0.000 ( 1.640)\tLoss 2.3020e+00 (2.3823e+00)\tAcc@1  48.83 ( 47.13)\tAcc@5  74.61 ( 71.91)\n",
      "Epoch: [11][4004/5005]\tTime  0.378 ( 2.532)\tData  0.000 ( 1.711)\tLoss 2.1508e+00 (2.3857e+00)\tAcc@1  53.52 ( 47.13)\tAcc@5  76.56 ( 71.84)\n",
      " * Acc@1 48.776 Acc@5 74.928\n",
      "************train_loss 2.388416127177266 val_acc 48.775997161865234*************\n",
      "Epoch: [12][   0/5005]\tTime 82.840 (82.840)\tData 37.821 (37.821)\tLoss 2.2592e+00 (2.2592e+00)\tAcc@1  47.27 ( 47.27)\tAcc@5  71.88 ( 71.88)\n",
      "Epoch: [12][1001/5005]\tTime  0.580 ( 2.773)\tData  0.439 ( 1.801)\tLoss 2.3880e+00 (2.3547e+00)\tAcc@1  50.39 ( 47.64)\tAcc@5  70.31 ( 72.34)\n",
      "Epoch: [12][2002/5005]\tTime  8.895 ( 2.620)\tData  8.750 ( 1.714)\tLoss 2.5621e+00 (2.3631e+00)\tAcc@1  42.19 ( 47.51)\tAcc@5  70.70 ( 72.18)\n",
      "Epoch: [12][3003/5005]\tTime  0.379 ( 2.579)\tData  0.000 ( 1.677)\tLoss 2.3984e+00 (2.3687e+00)\tAcc@1  49.22 ( 47.46)\tAcc@5  72.27 ( 72.11)\n",
      "Epoch: [12][4004/5005]\tTime  0.385 ( 2.412)\tData  0.001 ( 1.607)\tLoss 2.2502e+00 (2.3730e+00)\tAcc@1  50.39 ( 47.40)\tAcc@5  75.00 ( 72.06)\n",
      " * Acc@1 48.804 Acc@5 74.730\n",
      "************train_loss 2.3766463234469843 val_acc 48.80399703979492*************\n",
      "Epoch: [13][   0/5005]\tTime 107.791 (107.791)\tData 58.962 (58.962)\tLoss 2.4221e+00 (2.4221e+00)\tAcc@1  47.66 ( 47.66)\tAcc@5  71.09 ( 71.09)\n",
      "Epoch: [13][1001/5005]\tTime  0.387 ( 3.161)\tData  0.000 ( 2.254)\tLoss 2.4816e+00 (2.3404e+00)\tAcc@1  43.75 ( 47.92)\tAcc@5  68.36 ( 72.51)\n",
      "Epoch: [13][2002/5005]\tTime  0.342 ( 3.584)\tData  0.000 ( 2.754)\tLoss 2.2488e+00 (2.3498e+00)\tAcc@1  50.39 ( 47.80)\tAcc@5  73.83 ( 72.42)\n",
      "Epoch: [13][3003/5005]\tTime  0.374 ( 4.239)\tData  0.000 ( 3.325)\tLoss 2.4370e+00 (2.3556e+00)\tAcc@1  49.22 ( 47.69)\tAcc@5  72.27 ( 72.34)\n",
      "Epoch: [13][4004/5005]\tTime  0.379 ( 4.007)\tData  0.000 ( 3.159)\tLoss 2.5014e+00 (2.3597e+00)\tAcc@1  44.92 ( 47.60)\tAcc@5  68.75 ( 72.28)\n",
      " * Acc@1 49.212 Acc@5 75.242\n",
      "************train_loss 2.3616477336083257 val_acc 49.211997985839844*************\n",
      "Epoch: [14][   0/5005]\tTime 30.037 (30.037)\tData 21.306 (21.306)\tLoss 2.3317e+00 (2.3317e+00)\tAcc@1  49.22 ( 49.22)\tAcc@5  74.22 ( 74.22)\n",
      "Epoch: [14][1001/5005]\tTime  6.017 ( 2.198)\tData  5.871 ( 1.650)\tLoss 2.4511e+00 (2.3416e+00)\tAcc@1  45.70 ( 47.95)\tAcc@5  69.14 ( 72.54)\n",
      "Epoch: [14][2002/5005]\tTime  0.396 ( 2.974)\tData  0.000 ( 2.245)\tLoss 2.2206e+00 (2.3414e+00)\tAcc@1  51.95 ( 48.01)\tAcc@5  72.27 ( 72.59)\n",
      "Epoch: [14][3003/5005]\tTime 24.247 ( 3.212)\tData 23.079 ( 2.437)\tLoss 2.7097e+00 (2.3464e+00)\tAcc@1  43.75 ( 47.92)\tAcc@5  64.84 ( 72.49)\n",
      "Epoch: [14][4004/5005]\tTime  0.380 ( 3.028)\tData  0.000 ( 2.298)\tLoss 2.2940e+00 (2.3514e+00)\tAcc@1  48.05 ( 47.82)\tAcc@5  72.66 ( 72.41)\n",
      " * Acc@1 48.764 Acc@5 74.966\n",
      "************train_loss 2.354461709316913 val_acc 48.763999938964844*************\n",
      "Epoch: [15][   0/5005]\tTime 44.532 (44.532)\tData 28.607 (28.607)\tLoss 2.4004e+00 (2.4004e+00)\tAcc@1  46.88 ( 46.88)\tAcc@5  74.61 ( 74.61)\n",
      "Epoch: [15][1001/5005]\tTime  1.727 ( 2.680)\tData  1.593 ( 1.981)\tLoss 2.2932e+00 (2.3184e+00)\tAcc@1  47.66 ( 48.30)\tAcc@5  72.27 ( 72.91)\n",
      "Epoch: [15][2002/5005]\tTime  0.372 ( 2.501)\tData  0.178 ( 1.875)\tLoss 2.2409e+00 (2.3334e+00)\tAcc@1  50.00 ( 48.11)\tAcc@5  74.22 ( 72.69)\n",
      "Epoch: [15][3003/5005]\tTime  7.386 ( 2.945)\tData  7.242 ( 2.244)\tLoss 2.4450e+00 (2.3397e+00)\tAcc@1  45.31 ( 48.02)\tAcc@5  69.92 ( 72.60)\n",
      "Epoch: [15][4004/5005]\tTime 14.531 ( 2.815)\tData 11.480 ( 2.140)\tLoss 2.2958e+00 (2.3439e+00)\tAcc@1  46.48 ( 47.93)\tAcc@5  75.00 ( 72.54)\n",
      " * Acc@1 49.230 Acc@5 75.144\n",
      "************train_loss 2.3444204808472398 val_acc 49.22999954223633*************\n",
      "Epoch: [16][   0/5005]\tTime 56.748 (56.748)\tData 40.536 (40.536)\tLoss 2.2827e+00 (2.2827e+00)\tAcc@1  53.52 ( 53.52)\tAcc@5  74.61 ( 74.61)\n",
      "Epoch: [16][1001/5005]\tTime  0.381 ( 3.483)\tData  0.000 ( 2.622)\tLoss 2.3640e+00 (2.3238e+00)\tAcc@1  46.88 ( 48.34)\tAcc@5  71.48 ( 72.83)\n",
      "Epoch: [16][2002/5005]\tTime 13.276 ( 3.144)\tData  8.934 ( 2.337)\tLoss 2.1395e+00 (2.3275e+00)\tAcc@1  50.00 ( 48.21)\tAcc@5  73.05 ( 72.77)\n",
      "Epoch: [16][3003/5005]\tTime  0.382 ( 3.122)\tData  0.000 ( 2.302)\tLoss 2.5616e+00 (2.3299e+00)\tAcc@1  44.92 ( 48.18)\tAcc@5  68.36 ( 72.75)\n",
      "Epoch: [16][4004/5005]\tTime  0.377 ( 3.178)\tData  0.000 ( 2.332)\tLoss 2.2094e+00 (2.3349e+00)\tAcc@1  48.44 ( 48.12)\tAcc@5  76.56 ( 72.67)\n",
      " * Acc@1 50.468 Acc@5 76.408\n",
      "************train_loss 2.3379003108440934 val_acc 50.46799850463867*************\n",
      "Epoch: [17][   0/5005]\tTime 26.405 (26.405)\tData 18.210 (18.210)\tLoss 2.2716e+00 (2.2716e+00)\tAcc@1  46.88 ( 46.88)\tAcc@5  76.95 ( 76.95)\n",
      "Epoch: [17][1001/5005]\tTime 11.450 ( 2.075)\tData 11.307 ( 1.581)\tLoss 2.6290e+00 (2.3013e+00)\tAcc@1  45.70 ( 48.71)\tAcc@5  69.92 ( 73.24)\n",
      "Epoch: [17][2002/5005]\tTime  0.386 ( 2.917)\tData  0.000 ( 2.133)\tLoss 2.0818e+00 (2.3155e+00)\tAcc@1  53.91 ( 48.52)\tAcc@5  76.56 ( 72.98)\n",
      "Epoch: [17][3003/5005]\tTime  0.381 ( 2.669)\tData  0.000 ( 1.955)\tLoss 2.0519e+00 (2.3199e+00)\tAcc@1  54.69 ( 48.43)\tAcc@5  77.73 ( 72.91)\n",
      "Epoch: [17][4004/5005]\tTime  3.337 ( 2.660)\tData  3.182 ( 1.942)\tLoss 2.2300e+00 (2.3266e+00)\tAcc@1  48.05 ( 48.30)\tAcc@5  73.05 ( 72.81)\n",
      " * Acc@1 50.886 Acc@5 76.426\n",
      "************train_loss 2.3301708250731736 val_acc 50.8859977722168*************\n",
      "Epoch: [18][   0/5005]\tTime 123.358 (123.358)\tData 68.473 (68.473)\tLoss 2.3278e+00 (2.3278e+00)\tAcc@1  50.39 ( 50.39)\tAcc@5  73.05 ( 73.05)\n",
      "Epoch: [18][1001/5005]\tTime  1.079 ( 2.592)\tData  0.911 ( 1.959)\tLoss 2.4092e+00 (2.2990e+00)\tAcc@1  49.61 ( 48.71)\tAcc@5  71.09 ( 73.20)\n",
      "Epoch: [18][2002/5005]\tTime  0.390 ( 2.633)\tData  0.000 ( 1.914)\tLoss 2.3785e+00 (2.3111e+00)\tAcc@1  45.31 ( 48.49)\tAcc@5  73.05 ( 73.01)\n",
      "Epoch: [18][3003/5005]\tTime  0.376 ( 2.802)\tData  0.000 ( 2.045)\tLoss 2.3312e+00 (2.3146e+00)\tAcc@1  49.61 ( 48.43)\tAcc@5  73.83 ( 72.96)\n",
      "Epoch: [18][4004/5005]\tTime  0.377 ( 2.878)\tData  0.067 ( 2.083)\tLoss 2.5098e+00 (2.3230e+00)\tAcc@1  47.27 ( 48.33)\tAcc@5  72.27 ( 72.85)\n",
      " * Acc@1 51.862 Acc@5 77.502\n",
      "************train_loss 2.3247569565291886 val_acc 51.86199951171875*************\n",
      "Epoch: [19][   0/5005]\tTime 95.831 (95.831)\tData 41.716 (41.716)\tLoss 2.1473e+00 (2.1473e+00)\tAcc@1  54.30 ( 54.30)\tAcc@5  76.17 ( 76.17)\n",
      "Epoch: [19][1001/5005]\tTime  4.250 ( 3.783)\tData  4.106 ( 2.681)\tLoss 2.2785e+00 (2.2953e+00)\tAcc@1  50.00 ( 48.86)\tAcc@5  71.09 ( 73.26)\n",
      "Epoch: [19][2002/5005]\tTime  0.379 ( 3.341)\tData  0.000 ( 2.385)\tLoss 2.3705e+00 (2.3062e+00)\tAcc@1  46.09 ( 48.61)\tAcc@5  72.66 ( 73.10)\n",
      "Epoch: [19][3003/5005]\tTime  0.387 ( 3.035)\tData  0.000 ( 2.181)\tLoss 2.4076e+00 (2.3097e+00)\tAcc@1  44.53 ( 48.59)\tAcc@5  72.27 ( 73.05)\n",
      "Epoch: [19][4004/5005]\tTime  0.380 ( 2.867)\tData  0.000 ( 2.052)\tLoss 2.2510e+00 (2.3133e+00)\tAcc@1  48.44 ( 48.50)\tAcc@5  75.39 ( 73.00)\n",
      " * Acc@1 51.128 Acc@5 76.874\n",
      "************train_loss 2.3179817194467063 val_acc 51.12799835205078*************\n",
      "Epoch: [20][   0/5005]\tTime 32.165 (32.165)\tData 23.568 (23.568)\tLoss 2.0741e+00 (2.0741e+00)\tAcc@1  52.34 ( 52.34)\tAcc@5  78.12 ( 78.12)\n",
      "Epoch: [20][1001/5005]\tTime  2.606 ( 2.857)\tData  2.461 ( 2.017)\tLoss 2.2397e+00 (2.2918e+00)\tAcc@1  48.05 ( 48.87)\tAcc@5  72.66 ( 73.39)\n",
      "Epoch: [20][2002/5005]\tTime  0.378 ( 3.032)\tData  0.000 ( 2.073)\tLoss 2.5937e+00 (2.3009e+00)\tAcc@1  40.23 ( 48.72)\tAcc@5  67.58 ( 73.21)\n",
      "Epoch: [20][3003/5005]\tTime  0.504 ( 3.124)\tData  0.367 ( 2.100)\tLoss 2.3931e+00 (2.3047e+00)\tAcc@1  46.09 ( 48.65)\tAcc@5  71.48 ( 73.18)\n",
      "Epoch: [20][4004/5005]\tTime  0.390 ( 2.791)\tData  0.000 ( 1.918)\tLoss 2.5486e+00 (2.3099e+00)\tAcc@1  45.31 ( 48.56)\tAcc@5  66.41 ( 73.08)\n",
      " * Acc@1 52.152 Acc@5 77.668\n",
      "************train_loss 2.3113301875469805 val_acc 52.152000427246094*************\n",
      "Epoch: [21][   0/5005]\tTime 50.383 (50.383)\tData 30.038 (30.038)\tLoss 2.2039e+00 (2.2039e+00)\tAcc@1  52.73 ( 52.73)\tAcc@5  75.00 ( 75.00)\n",
      "Epoch: [21][1001/5005]\tTime 48.159 ( 3.238)\tData 26.824 ( 2.193)\tLoss 2.2838e+00 (2.2831e+00)\tAcc@1  46.88 ( 49.03)\tAcc@5  72.66 ( 73.54)\n",
      "Epoch: [21][2002/5005]\tTime  0.380 ( 2.792)\tData  0.000 ( 1.972)\tLoss 2.2272e+00 (2.2927e+00)\tAcc@1  50.00 ( 48.86)\tAcc@5  75.00 ( 73.40)\n",
      "Epoch: [21][3003/5005]\tTime  3.572 ( 2.507)\tData  3.425 ( 1.792)\tLoss 2.3321e+00 (2.3004e+00)\tAcc@1  45.31 ( 48.77)\tAcc@5  73.44 ( 73.26)\n",
      "Epoch: [21][4004/5005]\tTime  4.010 ( 2.685)\tData  3.720 ( 1.920)\tLoss 2.2902e+00 (2.3064e+00)\tAcc@1  52.34 ( 48.66)\tAcc@5  73.44 ( 73.17)\n",
      " * Acc@1 51.612 Acc@5 77.142\n",
      "************train_loss 2.3083289136419762 val_acc 51.61199951171875*************\n",
      "Epoch: [22][   0/5005]\tTime 70.135 (70.135)\tData 40.418 (40.418)\tLoss 2.3483e+00 (2.3483e+00)\tAcc@1  50.78 ( 50.78)\tAcc@5  71.48 ( 71.48)\n",
      "Epoch: [22][1001/5005]\tTime  0.145 ( 2.824)\tData  0.001 ( 2.013)\tLoss 2.1715e+00 (2.2809e+00)\tAcc@1  48.05 ( 49.10)\tAcc@5  76.95 ( 73.62)\n",
      "Epoch: [22][2002/5005]\tTime  0.380 ( 2.857)\tData  0.000 ( 2.068)\tLoss 2.1148e+00 (2.2884e+00)\tAcc@1  51.17 ( 48.97)\tAcc@5  75.00 ( 73.48)\n",
      "Epoch: [22][3003/5005]\tTime  0.378 ( 2.697)\tData  0.000 ( 1.946)\tLoss 2.4745e+00 (2.2975e+00)\tAcc@1  50.00 ( 48.82)\tAcc@5  71.88 ( 73.30)\n",
      "Epoch: [22][4004/5005]\tTime  4.954 ( 2.450)\tData  4.810 ( 1.770)\tLoss 2.2091e+00 (2.3018e+00)\tAcc@1  48.83 ( 48.75)\tAcc@5  75.00 ( 73.23)\n",
      " * Acc@1 50.998 Acc@5 76.994\n",
      "************train_loss 2.304803952470526 val_acc 50.99799728393555*************\n",
      "Epoch: [23][   0/5005]\tTime 46.276 (46.276)\tData 30.802 (30.802)\tLoss 2.1475e+00 (2.1475e+00)\tAcc@1  53.91 ( 53.91)\tAcc@5  75.00 ( 75.00)\n",
      "Epoch: [23][1001/5005]\tTime  8.560 ( 3.271)\tData  8.415 ( 2.324)\tLoss 2.2824e+00 (2.2804e+00)\tAcc@1  50.78 ( 49.17)\tAcc@5  75.39 ( 73.63)\n",
      "Epoch: [23][2002/5005]\tTime  0.387 ( 2.731)\tData  0.000 ( 1.941)\tLoss 2.3924e+00 (2.2913e+00)\tAcc@1  44.92 ( 48.95)\tAcc@5  73.83 ( 73.40)\n",
      "Epoch: [23][3003/5005]\tTime  0.379 ( 2.558)\tData  0.000 ( 1.814)\tLoss 2.4023e+00 (2.2974e+00)\tAcc@1  46.88 ( 48.84)\tAcc@5  74.61 ( 73.29)\n",
      "Epoch: [23][4004/5005]\tTime  0.375 ( 2.622)\tData  0.000 ( 1.860)\tLoss 2.2999e+00 (2.2990e+00)\tAcc@1  46.09 ( 48.81)\tAcc@5  74.61 ( 73.27)\n",
      " * Acc@1 50.486 Acc@5 76.530\n",
      "************train_loss 2.302660849044373 val_acc 50.486000061035156*************\n",
      "Epoch: [24][   0/5005]\tTime 48.147 (48.147)\tData 33.980 (33.980)\tLoss 2.3047e+00 (2.3047e+00)\tAcc@1  46.48 ( 46.48)\tAcc@5  71.88 ( 71.88)\n",
      "Epoch: [24][1001/5005]\tTime  0.375 ( 2.265)\tData  0.000 ( 1.704)\tLoss 2.5232e+00 (2.2788e+00)\tAcc@1  39.84 ( 49.12)\tAcc@5  70.70 ( 73.57)\n",
      "Epoch: [24][2002/5005]\tTime  0.389 ( 2.619)\tData  0.000 ( 1.913)\tLoss 2.4537e+00 (2.2818e+00)\tAcc@1  46.48 ( 49.13)\tAcc@5  70.70 ( 73.58)\n",
      "Epoch: [24][3003/5005]\tTime 11.939 ( 2.638)\tData 11.794 ( 1.943)\tLoss 2.3424e+00 (2.2876e+00)\tAcc@1  50.78 ( 49.07)\tAcc@5  71.09 ( 73.47)\n",
      "Epoch: [24][4004/5005]\tTime  0.394 ( 2.500)\tData  0.000 ( 1.853)\tLoss 2.2905e+00 (2.2926e+00)\tAcc@1  47.66 ( 48.95)\tAcc@5  73.05 ( 73.41)\n",
      " * Acc@1 49.942 Acc@5 76.256\n",
      "************train_loss 2.295401140431186 val_acc 49.94199752807617*************\n",
      "Epoch: [25][   0/5005]\tTime 91.351 (91.351)\tData 52.273 (52.273)\tLoss 2.5290e+00 (2.5290e+00)\tAcc@1  46.09 ( 46.09)\tAcc@5  69.14 ( 69.14)\n",
      "Epoch: [25][1001/5005]\tTime  0.386 ( 3.544)\tData  0.000 ( 2.352)\tLoss 2.1804e+00 (2.2725e+00)\tAcc@1  49.22 ( 49.26)\tAcc@5  73.44 ( 73.82)\n",
      "Epoch: [25][2002/5005]\tTime  0.385 ( 2.752)\tData  0.000 ( 1.881)\tLoss 2.2901e+00 (2.2802e+00)\tAcc@1  48.05 ( 49.11)\tAcc@5  72.66 ( 73.63)\n",
      "Epoch: [25][3003/5005]\tTime  0.382 ( 2.591)\tData  0.042 ( 1.789)\tLoss 2.5464e+00 (2.2844e+00)\tAcc@1  46.09 ( 49.03)\tAcc@5  68.36 ( 73.54)\n",
      "Epoch: [25][4004/5005]\tTime 11.728 ( 2.560)\tData  7.757 ( 1.774)\tLoss 2.4334e+00 (2.2882e+00)\tAcc@1  44.53 ( 48.98)\tAcc@5  71.09 ( 73.48)\n",
      " * Acc@1 51.388 Acc@5 77.096\n",
      "************train_loss 2.291109002243865 val_acc 51.38800048828125*************\n",
      "Epoch: [26][   0/5005]\tTime 98.211 (98.211)\tData 47.209 (47.209)\tLoss 2.1547e+00 (2.1547e+00)\tAcc@1  48.05 ( 48.05)\tAcc@5  74.22 ( 74.22)\n",
      "Epoch: [26][1001/5005]\tTime  0.389 ( 1.837)\tData  0.000 ( 1.377)\tLoss 2.3219e+00 (2.2670e+00)\tAcc@1  48.44 ( 49.41)\tAcc@5  77.73 ( 73.82)\n",
      "Epoch: [26][2002/5005]\tTime  0.373 ( 2.391)\tData  0.000 ( 1.711)\tLoss 2.2521e+00 (2.2768e+00)\tAcc@1  50.78 ( 49.16)\tAcc@5  75.00 ( 73.66)\n",
      "Epoch: [26][3003/5005]\tTime 22.300 ( 2.508)\tData  6.872 ( 1.816)\tLoss 2.2728e+00 (2.2833e+00)\tAcc@1  50.39 ( 49.08)\tAcc@5  73.44 ( 73.57)\n",
      "Epoch: [26][4004/5005]\tTime  0.380 ( 2.492)\tData  0.000 ( 1.796)\tLoss 2.2349e+00 (2.2876e+00)\tAcc@1  50.78 ( 49.02)\tAcc@5  74.61 ( 73.51)\n",
      " * Acc@1 50.430 Acc@5 76.064\n",
      "************train_loss 2.290390357747302 val_acc 50.43000030517578*************\n",
      "Epoch: [27][   0/5005]\tTime 47.919 (47.919)\tData 32.529 (32.529)\tLoss 2.1686e+00 (2.1686e+00)\tAcc@1  50.78 ( 50.78)\tAcc@5  74.61 ( 74.61)\n",
      "Epoch: [27][1001/5005]\tTime  0.379 ( 3.257)\tData  0.000 ( 2.218)\tLoss 2.1217e+00 (2.2564e+00)\tAcc@1  51.56 ( 49.39)\tAcc@5  75.39 ( 73.94)\n",
      "Epoch: [27][2002/5005]\tTime  0.382 ( 2.975)\tData  0.000 ( 2.038)\tLoss 2.1733e+00 (2.2719e+00)\tAcc@1  53.52 ( 49.18)\tAcc@5  73.05 ( 73.68)\n",
      "Epoch: [27][3003/5005]\tTime  0.376 ( 2.770)\tData  0.000 ( 1.922)\tLoss 2.1148e+00 (2.2772e+00)\tAcc@1  53.52 ( 49.14)\tAcc@5  78.91 ( 73.61)\n",
      "Epoch: [27][4004/5005]\tTime  0.384 ( 2.668)\tData  0.000 ( 1.854)\tLoss 2.4343e+00 (2.2824e+00)\tAcc@1  44.92 ( 49.04)\tAcc@5  73.83 ( 73.54)\n",
      " * Acc@1 51.042 Acc@5 77.042\n",
      "************train_loss 2.2880254964371183 val_acc 51.04199981689453*************\n",
      "Epoch: [28][   0/5005]\tTime 58.193 (58.193)\tData 38.849 (38.849)\tLoss 2.3348e+00 (2.3348e+00)\tAcc@1  49.22 ( 49.22)\tAcc@5  71.48 ( 71.48)\n",
      "Epoch: [28][1001/5005]\tTime  0.382 ( 1.928)\tData  0.000 ( 1.450)\tLoss 2.1317e+00 (2.2551e+00)\tAcc@1  52.34 ( 49.61)\tAcc@5  74.22 ( 73.94)\n",
      "Epoch: [28][2002/5005]\tTime  0.379 ( 2.243)\tData  0.000 ( 1.654)\tLoss 2.4986e+00 (2.2696e+00)\tAcc@1  44.92 ( 49.29)\tAcc@5  71.88 ( 73.76)\n",
      "Epoch: [28][3003/5005]\tTime  0.385 ( 2.450)\tData  0.000 ( 1.786)\tLoss 2.2097e+00 (2.2771e+00)\tAcc@1  52.34 ( 49.19)\tAcc@5  75.00 ( 73.62)\n",
      "Epoch: [28][4004/5005]\tTime  0.376 ( 2.504)\tData  0.001 ( 1.819)\tLoss 2.2733e+00 (2.2804e+00)\tAcc@1  50.39 ( 49.15)\tAcc@5  72.66 ( 73.57)\n",
      " * Acc@1 50.478 Acc@5 76.506\n",
      "************train_loss 2.284757839359127 val_acc 50.477996826171875*************\n",
      "Epoch: [29][   0/5005]\tTime 23.008 (23.008)\tData 16.963 (16.963)\tLoss 2.1064e+00 (2.1064e+00)\tAcc@1  49.22 ( 49.22)\tAcc@5  76.95 ( 76.95)\n",
      "Epoch: [29][1001/5005]\tTime  0.375 ( 2.321)\tData  0.000 ( 1.643)\tLoss 2.0979e+00 (2.2591e+00)\tAcc@1  53.12 ( 49.48)\tAcc@5  75.78 ( 73.92)\n",
      "Epoch: [29][2002/5005]\tTime  0.380 ( 2.599)\tData  0.000 ( 1.853)\tLoss 2.1684e+00 (2.2712e+00)\tAcc@1  49.22 ( 49.26)\tAcc@5  74.22 ( 73.67)\n",
      "Epoch: [29][3003/5005]\tTime  0.374 ( 2.640)\tData  0.000 ( 1.856)\tLoss 2.2370e+00 (2.2738e+00)\tAcc@1  49.61 ( 49.24)\tAcc@5  75.78 ( 73.66)\n",
      "Epoch: [29][4004/5005]\tTime  0.377 ( 2.575)\tData  0.000 ( 1.820)\tLoss 2.4377e+00 (2.2774e+00)\tAcc@1  46.09 ( 49.18)\tAcc@5  69.92 ( 73.60)\n",
      " * Acc@1 52.256 Acc@5 77.660\n",
      "************train_loss 2.282086740268932 val_acc 52.25600051879883*************\n",
      "Epoch: [30][   0/5005]\tTime 45.345 (45.345)\tData 24.778 (24.778)\tLoss 2.0706e+00 (2.0706e+00)\tAcc@1  51.95 ( 51.95)\tAcc@5  75.39 ( 75.39)\n",
      "Epoch: [30][1001/5005]\tTime  6.245 ( 2.269)\tData  6.105 ( 1.657)\tLoss 1.5810e+00 (1.8798e+00)\tAcc@1  62.11 ( 57.38)\tAcc@5  82.81 ( 79.53)\n",
      "Epoch: [30][2002/5005]\tTime  0.382 ( 2.160)\tData  0.000 ( 1.556)\tLoss 1.7453e+00 (1.8287e+00)\tAcc@1  60.94 ( 58.40)\tAcc@5  81.64 ( 80.23)\n",
      "Epoch: [30][3003/5005]\tTime  0.375 ( 2.332)\tData  0.000 ( 1.667)\tLoss 1.7673e+00 (1.7983e+00)\tAcc@1  59.77 ( 59.01)\tAcc@5  81.25 ( 80.65)\n",
      "Epoch: [30][4004/5005]\tTime  0.387 ( 2.490)\tData  0.000 ( 1.783)\tLoss 1.6621e+00 (1.7764e+00)\tAcc@1  63.28 ( 59.42)\tAcc@5  79.69 ( 80.97)\n",
      " * Acc@1 65.240 Acc@5 86.692\n",
      "************train_loss 1.7591748449114057 val_acc 65.23999786376953*************\n",
      "Epoch: [31][   0/5005]\tTime 57.155 (57.155)\tData 31.871 (31.871)\tLoss 1.6939e+00 (1.6939e+00)\tAcc@1  62.89 ( 62.89)\tAcc@5  83.20 ( 83.20)\n",
      "Epoch: [31][1001/5005]\tTime  0.378 ( 1.823)\tData  0.000 ( 1.303)\tLoss 1.6258e+00 (1.6449e+00)\tAcc@1  61.33 ( 61.91)\tAcc@5  82.03 ( 82.73)\n",
      "Epoch: [31][2002/5005]\tTime  0.384 ( 2.282)\tData  0.000 ( 1.639)\tLoss 1.6393e+00 (1.6433e+00)\tAcc@1  58.98 ( 61.97)\tAcc@5  84.77 ( 82.76)\n",
      "Epoch: [31][3003/5005]\tTime  0.396 ( 2.359)\tData  0.000 ( 1.723)\tLoss 1.7250e+00 (1.6384e+00)\tAcc@1  58.98 ( 62.13)\tAcc@5  82.03 ( 82.87)\n",
      "Epoch: [31][4004/5005]\tTime  0.374 ( 2.430)\tData  0.000 ( 1.768)\tLoss 1.5596e+00 (1.6349e+00)\tAcc@1  63.67 ( 62.19)\tAcc@5  85.55 ( 82.93)\n",
      " * Acc@1 65.930 Acc@5 87.056\n",
      "************train_loss 1.6321390874855048 val_acc 65.93000030517578*************\n",
      "Epoch: [32][   0/5005]\tTime 153.466 (153.466)\tData 70.419 (70.419)\tLoss 1.7598e+00 (1.7598e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  82.42 ( 82.42)\n",
      "Epoch: [32][1001/5005]\tTime  0.377 ( 3.080)\tData  0.000 ( 2.063)\tLoss 1.5311e+00 (1.5861e+00)\tAcc@1  65.62 ( 63.13)\tAcc@5  83.98 ( 83.63)\n",
      "Epoch: [32][2002/5005]\tTime  0.384 ( 2.401)\tData  0.000 ( 1.636)\tLoss 1.5700e+00 (1.5845e+00)\tAcc@1  63.67 ( 63.24)\tAcc@5  82.42 ( 83.67)\n",
      "Epoch: [32][3003/5005]\tTime  1.125 ( 2.442)\tData  0.000 ( 1.660)\tLoss 1.4279e+00 (1.5837e+00)\tAcc@1  64.45 ( 63.27)\tAcc@5  85.94 ( 83.70)\n",
      "Epoch: [32][4004/5005]\tTime  0.375 ( 2.413)\tData  0.000 ( 1.665)\tLoss 1.5640e+00 (1.5830e+00)\tAcc@1  63.28 ( 63.24)\tAcc@5  82.03 ( 83.70)\n",
      " * Acc@1 66.864 Acc@5 87.582\n",
      "************train_loss 1.5816608443960443 val_acc 66.86399841308594*************\n",
      "Epoch: [33][   0/5005]\tTime 39.285 (39.285)\tData 25.597 (25.597)\tLoss 1.4131e+00 (1.4131e+00)\tAcc@1  68.36 ( 68.36)\tAcc@5  84.38 ( 84.38)\n",
      "Epoch: [33][1001/5005]\tTime  0.377 ( 1.661)\tData  0.000 ( 1.206)\tLoss 1.5791e+00 (1.5484e+00)\tAcc@1  64.84 ( 63.93)\tAcc@5  83.20 ( 84.13)\n",
      "Epoch: [33][2002/5005]\tTime  0.377 ( 1.875)\tData  0.000 ( 1.358)\tLoss 1.4133e+00 (1.5493e+00)\tAcc@1  61.72 ( 63.89)\tAcc@5  86.72 ( 84.10)\n",
      "Epoch: [33][3003/5005]\tTime  0.371 ( 2.076)\tData  0.000 ( 1.486)\tLoss 1.6316e+00 (1.5508e+00)\tAcc@1  58.59 ( 63.86)\tAcc@5  85.16 ( 84.09)\n",
      "Epoch: [33][4004/5005]\tTime  0.376 ( 2.280)\tData  0.000 ( 1.608)\tLoss 1.5028e+00 (1.5505e+00)\tAcc@1  64.84 ( 63.87)\tAcc@5  83.98 ( 84.11)\n",
      " * Acc@1 67.150 Acc@5 87.710\n",
      "************train_loss 1.5514604640650107 val_acc 67.1500015258789*************\n",
      "Epoch: [34][   0/5005]\tTime 94.568 (94.568)\tData 56.505 (56.505)\tLoss 1.5517e+00 (1.5517e+00)\tAcc@1  62.50 ( 62.50)\tAcc@5  83.59 ( 83.59)\n",
      "Epoch: [34][1001/5005]\tTime 21.099 ( 3.412)\tData 20.937 ( 2.199)\tLoss 1.5378e+00 (1.5176e+00)\tAcc@1  63.67 ( 64.51)\tAcc@5  87.11 ( 84.61)\n",
      "Epoch: [34][2002/5005]\tTime  0.382 ( 2.884)\tData  0.000 ( 1.931)\tLoss 1.6980e+00 (1.5203e+00)\tAcc@1  60.94 ( 64.46)\tAcc@5  83.98 ( 84.60)\n",
      "Epoch: [34][3003/5005]\tTime  0.371 ( 2.593)\tData  0.000 ( 1.789)\tLoss 1.5308e+00 (1.5218e+00)\tAcc@1  62.89 ( 64.45)\tAcc@5  83.59 ( 84.55)\n",
      "Epoch: [34][4004/5005]\tTime  0.379 ( 2.435)\tData  0.000 ( 1.698)\tLoss 1.4244e+00 (1.5246e+00)\tAcc@1  70.70 ( 64.42)\tAcc@5  87.89 ( 84.50)\n",
      " * Acc@1 67.482 Acc@5 87.970\n",
      "************train_loss 1.5257760398037783 val_acc 67.48199462890625*************\n",
      "Epoch: [35][   0/5005]\tTime 158.819 (158.819)\tData 82.189 (82.189)\tLoss 1.3705e+00 (1.3705e+00)\tAcc@1  66.80 ( 66.80)\tAcc@5  87.89 ( 87.89)\n",
      "Epoch: [35][1001/5005]\tTime  0.376 ( 2.468)\tData  0.000 ( 1.728)\tLoss 1.4560e+00 (1.4950e+00)\tAcc@1  64.45 ( 65.01)\tAcc@5  84.77 ( 84.91)\n",
      "Epoch: [35][2002/5005]\tTime  0.378 ( 2.276)\tData  0.000 ( 1.611)\tLoss 1.5263e+00 (1.5032e+00)\tAcc@1  67.58 ( 64.83)\tAcc@5  84.38 ( 84.79)\n",
      "Epoch: [35][3003/5005]\tTime  1.245 ( 2.219)\tData  1.108 ( 1.573)\tLoss 1.3430e+00 (1.5064e+00)\tAcc@1  71.09 ( 64.76)\tAcc@5  86.72 ( 84.74)\n",
      "Epoch: [35][4004/5005]\tTime  0.376 ( 2.234)\tData  0.000 ( 1.595)\tLoss 1.5196e+00 (1.5094e+00)\tAcc@1  67.19 ( 64.70)\tAcc@5  83.98 ( 84.69)\n",
      " * Acc@1 67.170 Acc@5 87.900\n",
      "************train_loss 1.5119928498129982 val_acc 67.16999816894531*************\n",
      "Epoch: [36][   0/5005]\tTime 37.573 (37.573)\tData 26.239 (26.239)\tLoss 1.5174e+00 (1.5174e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  85.94 ( 85.94)\n",
      "Epoch: [36][1001/5005]\tTime  8.109 ( 2.283)\tData  7.956 ( 1.659)\tLoss 1.5027e+00 (1.4834e+00)\tAcc@1  66.80 ( 65.15)\tAcc@5  83.98 ( 85.02)\n",
      "Epoch: [36][2002/5005]\tTime  9.943 ( 2.686)\tData  9.789 ( 1.885)\tLoss 1.7529e+00 (1.4897e+00)\tAcc@1  58.20 ( 65.07)\tAcc@5  83.59 ( 84.96)\n",
      "Epoch: [36][3003/5005]\tTime  2.790 ( 2.629)\tData  2.647 ( 1.860)\tLoss 1.5296e+00 (1.4946e+00)\tAcc@1  63.67 ( 64.97)\tAcc@5  83.20 ( 84.90)\n",
      "Epoch: [36][4004/5005]\tTime  0.381 ( 2.487)\tData  0.000 ( 1.766)\tLoss 1.7441e+00 (1.4978e+00)\tAcc@1  59.38 ( 64.90)\tAcc@5  82.81 ( 84.86)\n",
      " * Acc@1 67.462 Acc@5 88.088\n",
      "************train_loss 1.5013712297548185 val_acc 67.46199798583984*************\n",
      "Epoch: [37][   0/5005]\tTime 67.831 (67.831)\tData 45.675 (45.675)\tLoss 1.4529e+00 (1.4529e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  83.20 ( 83.20)\n",
      "Epoch: [37][1001/5005]\tTime  0.381 ( 3.419)\tData  0.000 ( 2.290)\tLoss 1.7461e+00 (1.4722e+00)\tAcc@1  61.72 ( 65.43)\tAcc@5  82.42 ( 85.29)\n",
      "Epoch: [37][2002/5005]\tTime  9.589 ( 3.156)\tData  9.445 ( 2.166)\tLoss 1.4596e+00 (1.4810e+00)\tAcc@1  64.45 ( 65.25)\tAcc@5  84.77 ( 85.16)\n",
      "Epoch: [37][3003/5005]\tTime  0.378 ( 2.985)\tData  0.000 ( 2.045)\tLoss 1.6258e+00 (1.4857e+00)\tAcc@1  62.89 ( 65.17)\tAcc@5  82.03 ( 85.09)\n",
      "Epoch: [37][4004/5005]\tTime  9.093 ( 2.795)\tData  8.944 ( 1.911)\tLoss 1.5344e+00 (1.4898e+00)\tAcc@1  65.23 ( 65.09)\tAcc@5  83.20 ( 85.04)\n",
      " * Acc@1 67.022 Acc@5 87.858\n",
      "************train_loss 1.4924005484723901 val_acc 67.0219955444336*************\n",
      "Epoch: [38][   0/5005]\tTime 48.189 (48.189)\tData 31.726 (31.726)\tLoss 1.4199e+00 (1.4199e+00)\tAcc@1  66.02 ( 66.02)\tAcc@5  87.89 ( 87.89)\n",
      "Epoch: [38][1001/5005]\tTime  0.378 ( 2.069)\tData  0.000 ( 1.502)\tLoss 1.5176e+00 (1.4685e+00)\tAcc@1  63.28 ( 65.50)\tAcc@5  84.77 ( 85.24)\n",
      "Epoch: [38][2002/5005]\tTime  3.031 ( 2.660)\tData  2.883 ( 1.821)\tLoss 1.2492e+00 (1.4734e+00)\tAcc@1  67.58 ( 65.37)\tAcc@5  91.02 ( 85.22)\n",
      "Epoch: [38][3003/5005]\tTime  5.975 ( 2.795)\tData  5.827 ( 1.927)\tLoss 1.2875e+00 (1.4802e+00)\tAcc@1  68.36 ( 65.25)\tAcc@5  89.84 ( 85.11)\n",
      "Epoch: [38][4004/5005]\tTime  5.949 ( 2.856)\tData  0.000 ( 1.941)\tLoss 1.4183e+00 (1.4858e+00)\tAcc@1  66.80 ( 65.14)\tAcc@5  85.16 ( 85.02)\n",
      " * Acc@1 67.288 Acc@5 87.792\n",
      "************train_loss 1.490524658432731 val_acc 67.28800201416016*************\n",
      "Epoch: [39][   0/5005]\tTime 37.303 (37.303)\tData 22.681 (22.681)\tLoss 1.7414e+00 (1.7414e+00)\tAcc@1  62.89 ( 62.89)\tAcc@5  81.64 ( 81.64)\n",
      "Epoch: [39][1001/5005]\tTime  0.368 ( 2.734)\tData  0.000 ( 1.791)\tLoss 1.6284e+00 (1.4638e+00)\tAcc@1  63.28 ( 65.70)\tAcc@5  80.08 ( 85.29)\n",
      "Epoch: [39][2002/5005]\tTime  4.043 ( 2.774)\tData  3.898 ( 1.888)\tLoss 1.2917e+00 (1.4689e+00)\tAcc@1  68.75 ( 65.55)\tAcc@5  86.72 ( 85.22)\n",
      "Epoch: [39][3003/5005]\tTime  0.372 ( 2.851)\tData  0.000 ( 1.915)\tLoss 1.2992e+00 (1.4771e+00)\tAcc@1  67.19 ( 65.33)\tAcc@5  89.84 ( 85.12)\n",
      "Epoch: [39][4004/5005]\tTime 19.997 ( 2.623)\tData 13.021 ( 1.791)\tLoss 1.4448e+00 (1.4807e+00)\tAcc@1  64.84 ( 65.24)\tAcc@5  86.72 ( 85.07)\n",
      " * Acc@1 67.512 Acc@5 87.898\n",
      "************train_loss 1.4847272390609498 val_acc 67.51200103759766*************\n",
      "Epoch: [40][   0/5005]\tTime 46.695 (46.695)\tData 21.536 (21.536)\tLoss 1.3124e+00 (1.3124e+00)\tAcc@1  68.36 ( 68.36)\tAcc@5  87.50 ( 87.50)\n",
      "Epoch: [40][1001/5005]\tTime  0.374 ( 2.136)\tData  0.000 ( 1.552)\tLoss 1.4461e+00 (1.4618e+00)\tAcc@1  65.23 ( 65.63)\tAcc@5  86.33 ( 85.33)\n",
      "Epoch: [40][2002/5005]\tTime  0.372 ( 2.286)\tData  0.111 ( 1.655)\tLoss 1.2739e+00 (1.4704e+00)\tAcc@1  70.31 ( 65.50)\tAcc@5  87.11 ( 85.22)\n",
      "Epoch: [40][3003/5005]\tTime  0.383 ( 2.380)\tData  0.000 ( 1.706)\tLoss 1.4488e+00 (1.4754e+00)\tAcc@1  64.84 ( 65.39)\tAcc@5  86.33 ( 85.16)\n",
      "Epoch: [40][4004/5005]\tTime  0.388 ( 2.509)\tData  0.000 ( 1.803)\tLoss 1.4604e+00 (1.4796e+00)\tAcc@1  66.80 ( 65.25)\tAcc@5  84.77 ( 85.10)\n",
      " * Acc@1 67.232 Acc@5 87.624\n",
      "************train_loss 1.4836119543660533 val_acc 67.23199462890625*************\n",
      "Epoch: [41][   0/5005]\tTime 17.738 (17.738)\tData 14.436 (14.436)\tLoss 1.2943e+00 (1.2943e+00)\tAcc@1  70.31 ( 70.31)\tAcc@5  87.50 ( 87.50)\n",
      "Epoch: [41][1001/5005]\tTime  0.376 ( 1.648)\tData  0.000 ( 1.216)\tLoss 1.3487e+00 (1.4512e+00)\tAcc@1  68.75 ( 65.78)\tAcc@5  86.33 ( 85.61)\n",
      "Epoch: [41][2002/5005]\tTime  0.375 ( 1.947)\tData  0.000 ( 1.404)\tLoss 1.4700e+00 (1.4641e+00)\tAcc@1  65.23 ( 65.53)\tAcc@5  84.38 ( 85.41)\n",
      "Epoch: [41][3003/5005]\tTime  0.370 ( 2.167)\tData  0.000 ( 1.521)\tLoss 1.6484e+00 (1.4698e+00)\tAcc@1  58.59 ( 65.44)\tAcc@5  82.42 ( 85.32)\n",
      "Epoch: [41][4004/5005]\tTime  0.372 ( 2.404)\tData  0.000 ( 1.648)\tLoss 1.5371e+00 (1.4760e+00)\tAcc@1  63.67 ( 65.30)\tAcc@5  83.59 ( 85.23)\n",
      " * Acc@1 67.244 Acc@5 87.976\n",
      "************train_loss 1.4816545486688375 val_acc 67.2439956665039*************\n",
      "Epoch: [42][   0/5005]\tTime 76.255 (76.255)\tData 47.995 (47.995)\tLoss 1.4583e+00 (1.4583e+00)\tAcc@1  65.23 ( 65.23)\tAcc@5  87.11 ( 87.11)\n",
      "Epoch: [42][1001/5005]\tTime  0.388 ( 2.714)\tData  0.000 ( 1.880)\tLoss 1.4308e+00 (1.4531e+00)\tAcc@1  63.67 ( 65.88)\tAcc@5  85.16 ( 85.51)\n",
      "Epoch: [42][2002/5005]\tTime 14.983 ( 2.464)\tData 10.993 ( 1.763)\tLoss 1.5002e+00 (1.4596e+00)\tAcc@1  67.19 ( 65.71)\tAcc@5  85.55 ( 85.43)\n",
      "Epoch: [42][3003/5005]\tTime  0.883 ( 2.461)\tData  0.752 ( 1.720)\tLoss 1.5412e+00 (1.4694e+00)\tAcc@1  61.72 ( 65.51)\tAcc@5  82.03 ( 85.29)\n",
      "Epoch: [42][4004/5005]\tTime  0.382 ( 2.354)\tData  0.000 ( 1.662)\tLoss 1.4896e+00 (1.4770e+00)\tAcc@1  63.67 ( 65.37)\tAcc@5  86.33 ( 85.16)\n",
      " * Acc@1 67.080 Acc@5 87.876\n",
      "************train_loss 1.4825125561846602 val_acc 67.08000183105469*************\n",
      "Epoch: [43][   0/5005]\tTime 64.714 (64.714)\tData 41.116 (41.116)\tLoss 1.3003e+00 (1.3003e+00)\tAcc@1  69.53 ( 69.53)\tAcc@5  88.67 ( 88.67)\n",
      "Epoch: [43][1001/5005]\tTime  0.372 ( 2.363)\tData  0.000 ( 1.620)\tLoss 1.5144e+00 (1.4625e+00)\tAcc@1  62.89 ( 65.66)\tAcc@5  85.55 ( 85.31)\n",
      "Epoch: [43][2002/5005]\tTime  0.374 ( 2.316)\tData  0.000 ( 1.591)\tLoss 1.3698e+00 (1.4664e+00)\tAcc@1  69.14 ( 65.57)\tAcc@5  87.11 ( 85.27)\n",
      "Epoch: [43][3003/5005]\tTime  0.379 ( 2.171)\tData  0.000 ( 1.500)\tLoss 1.5649e+00 (1.4725e+00)\tAcc@1  64.45 ( 65.43)\tAcc@5  85.55 ( 85.21)\n",
      "Epoch: [43][4004/5005]\tTime  0.378 ( 2.377)\tData  0.000 ( 1.645)\tLoss 1.5627e+00 (1.4775e+00)\tAcc@1  63.67 ( 65.32)\tAcc@5  83.20 ( 85.15)\n",
      " * Acc@1 66.966 Acc@5 87.690\n",
      "************train_loss 1.4800380189697464 val_acc 66.96599578857422*************\n",
      "Epoch: [44][   0/5005]\tTime 113.432 (113.432)\tData 54.103 (54.103)\tLoss 1.4753e+00 (1.4753e+00)\tAcc@1  67.19 ( 67.19)\tAcc@5  85.55 ( 85.55)\n",
      "Epoch: [44][1001/5005]\tTime  4.344 ( 2.864)\tData  4.197 ( 1.925)\tLoss 1.3846e+00 (1.4561e+00)\tAcc@1  65.23 ( 65.83)\tAcc@5  86.33 ( 85.42)\n",
      "Epoch: [44][2002/5005]\tTime  1.856 ( 2.635)\tData  1.723 ( 1.801)\tLoss 1.3993e+00 (1.4658e+00)\tAcc@1  66.80 ( 65.57)\tAcc@5  87.11 ( 85.33)\n",
      "Epoch: [44][3003/5005]\tTime  2.358 ( 2.576)\tData  2.214 ( 1.763)\tLoss 1.2728e+00 (1.4728e+00)\tAcc@1  69.53 ( 65.42)\tAcc@5  87.11 ( 85.24)\n",
      "Epoch: [44][4004/5005]\tTime 12.294 ( 2.401)\tData 12.139 ( 1.674)\tLoss 1.4941e+00 (1.4776e+00)\tAcc@1  63.67 ( 65.31)\tAcc@5  86.72 ( 85.16)\n",
      " * Acc@1 67.288 Acc@5 87.784\n",
      "************train_loss 1.4819052675029971 val_acc 67.28800201416016*************\n",
      "Epoch: [45][   0/5005]\tTime 92.388 (92.388)\tData 60.964 (60.964)\tLoss 1.3738e+00 (1.3738e+00)\tAcc@1  66.80 ( 66.80)\tAcc@5  85.55 ( 85.55)\n",
      "Epoch: [45][1001/5005]\tTime  0.373 ( 2.829)\tData  0.000 ( 1.842)\tLoss 1.5184e+00 (1.4445e+00)\tAcc@1  66.80 ( 66.02)\tAcc@5  83.98 ( 85.62)\n",
      "Epoch: [45][2002/5005]\tTime  0.376 ( 2.728)\tData  0.000 ( 1.777)\tLoss 1.6194e+00 (1.4580e+00)\tAcc@1  60.16 ( 65.74)\tAcc@5  82.42 ( 85.45)\n",
      "Epoch: [45][3003/5005]\tTime  0.374 ( 2.401)\tData  0.000 ( 1.590)\tLoss 1.5217e+00 (1.4665e+00)\tAcc@1  65.62 ( 65.50)\tAcc@5  83.59 ( 85.32)\n",
      "Epoch: [45][4004/5005]\tTime  0.373 ( 2.353)\tData  0.000 ( 1.564)\tLoss 1.5512e+00 (1.4738e+00)\tAcc@1  62.11 ( 65.32)\tAcc@5  86.72 ( 85.21)\n",
      " * Acc@1 66.594 Acc@5 87.344\n",
      "************train_loss 1.4801226535877148 val_acc 66.59400177001953*************\n",
      "Epoch: [46][   0/5005]\tTime 91.293 (91.293)\tData 52.911 (52.911)\tLoss 1.5380e+00 (1.5380e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  87.11 ( 87.11)\n",
      "Epoch: [46][1001/5005]\tTime  1.126 ( 2.618)\tData  0.993 ( 1.900)\tLoss 1.3813e+00 (1.4536e+00)\tAcc@1  63.67 ( 65.81)\tAcc@5  87.11 ( 85.55)\n",
      "Epoch: [46][2002/5005]\tTime  0.381 ( 2.610)\tData  0.171 ( 1.903)\tLoss 1.5084e+00 (1.4619e+00)\tAcc@1  67.19 ( 65.59)\tAcc@5  86.72 ( 85.41)\n",
      "Epoch: [46][3003/5005]\tTime  0.374 ( 2.790)\tData  0.000 ( 1.976)\tLoss 1.4680e+00 (1.4670e+00)\tAcc@1  66.80 ( 65.48)\tAcc@5  84.77 ( 85.35)\n",
      "Epoch: [46][4004/5005]\tTime  0.376 ( 2.646)\tData  0.000 ( 1.890)\tLoss 1.6164e+00 (1.4729e+00)\tAcc@1  62.50 ( 65.34)\tAcc@5  84.77 ( 85.26)\n",
      " * Acc@1 66.316 Acc@5 87.308\n",
      "************train_loss 1.478102286807545 val_acc 66.31600189208984*************\n",
      "Epoch: [47][   0/5005]\tTime 35.511 (35.511)\tData 23.184 (23.184)\tLoss 1.3608e+00 (1.3608e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  88.28 ( 88.28)\n",
      "Epoch: [47][1001/5005]\tTime  0.381 ( 2.639)\tData  0.000 ( 1.923)\tLoss 1.3888e+00 (1.4523e+00)\tAcc@1  69.14 ( 65.76)\tAcc@5  85.94 ( 85.51)\n",
      "Epoch: [47][2002/5005]\tTime  0.154 ( 2.793)\tData  0.001 ( 1.937)\tLoss 1.4665e+00 (1.4580e+00)\tAcc@1  68.75 ( 65.64)\tAcc@5  83.98 ( 85.43)\n",
      "Epoch: [47][3003/5005]\tTime  0.373 ( 2.776)\tData  0.000 ( 1.890)\tLoss 1.5565e+00 (1.4643e+00)\tAcc@1  64.45 ( 65.51)\tAcc@5  83.59 ( 85.37)\n",
      "Epoch: [47][4004/5005]\tTime  0.374 ( 2.576)\tData  0.000 ( 1.783)\tLoss 1.4463e+00 (1.4719e+00)\tAcc@1  68.75 ( 65.37)\tAcc@5  83.59 ( 85.26)\n",
      " * Acc@1 66.896 Acc@5 87.594\n",
      "************train_loss 1.476380440571925 val_acc 66.89599609375*************\n",
      "Epoch: [48][   0/5005]\tTime 61.568 (61.568)\tData 28.261 (28.261)\tLoss 1.3644e+00 (1.3644e+00)\tAcc@1  66.02 ( 66.02)\tAcc@5  85.55 ( 85.55)\n",
      "Epoch: [48][1001/5005]\tTime  0.380 ( 2.293)\tData  0.000 ( 1.588)\tLoss 1.4538e+00 (1.4475e+00)\tAcc@1  64.45 ( 66.03)\tAcc@5  85.94 ( 85.57)\n",
      "Epoch: [48][2002/5005]\tTime  0.379 ( 2.410)\tData  0.000 ( 1.696)\tLoss 1.5496e+00 (1.4583e+00)\tAcc@1  67.19 ( 65.76)\tAcc@5  81.25 ( 85.45)\n",
      "Epoch: [48][3003/5005]\tTime  0.374 ( 2.486)\tData  0.000 ( 1.727)\tLoss 1.5630e+00 (1.4665e+00)\tAcc@1  61.72 ( 65.58)\tAcc@5  82.42 ( 85.33)\n",
      "Epoch: [48][4004/5005]\tTime  0.375 ( 2.654)\tData  0.000 ( 1.806)\tLoss 1.4044e+00 (1.4721e+00)\tAcc@1  66.80 ( 65.46)\tAcc@5  86.72 ( 85.27)\n",
      " * Acc@1 66.786 Acc@5 87.692\n",
      "************train_loss 1.4781545030248986 val_acc 66.78599548339844*************\n",
      "Epoch: [49][   0/5005]\tTime 54.397 (54.397)\tData 24.234 (24.234)\tLoss 1.5667e+00 (1.5667e+00)\tAcc@1  62.89 ( 62.89)\tAcc@5  84.38 ( 84.38)\n",
      "Epoch: [49][1001/5005]\tTime  0.371 ( 2.198)\tData  0.000 ( 1.549)\tLoss 1.4353e+00 (1.4525e+00)\tAcc@1  68.36 ( 65.80)\tAcc@5  85.55 ( 85.48)\n",
      "Epoch: [49][2002/5005]\tTime  0.376 ( 2.373)\tData  0.000 ( 1.621)\tLoss 1.4265e+00 (1.4573e+00)\tAcc@1  69.92 ( 65.71)\tAcc@5  85.16 ( 85.42)\n",
      "Epoch: [49][3003/5005]\tTime  2.017 ( 2.706)\tData  1.883 ( 1.785)\tLoss 1.4504e+00 (1.4656e+00)\tAcc@1  68.36 ( 65.53)\tAcc@5  85.55 ( 85.34)\n",
      "Epoch: [49][4004/5005]\tTime  0.382 ( 2.581)\tData  0.000 ( 1.736)\tLoss 1.5079e+00 (1.4687e+00)\tAcc@1  64.84 ( 65.45)\tAcc@5  84.77 ( 85.29)\n",
      " * Acc@1 66.522 Acc@5 87.412\n",
      "************train_loss 1.4737195147381914 val_acc 66.5219955444336*************\n",
      "Epoch: [50][   0/5005]\tTime 51.048 (51.048)\tData 34.085 (34.085)\tLoss 1.4059e+00 (1.4059e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  87.50 ( 87.50)\n",
      "Epoch: [50][1001/5005]\tTime 11.453 ( 2.336)\tData 11.307 ( 1.582)\tLoss 1.3193e+00 (1.4422e+00)\tAcc@1  64.06 ( 66.09)\tAcc@5  89.06 ( 85.74)\n",
      "Epoch: [50][2002/5005]\tTime  9.353 ( 2.334)\tData  9.212 ( 1.634)\tLoss 1.6588e+00 (1.4546e+00)\tAcc@1  62.89 ( 65.80)\tAcc@5  85.55 ( 85.56)\n",
      "Epoch: [50][3003/5005]\tTime  0.378 ( 2.165)\tData  0.000 ( 1.520)\tLoss 1.2696e+00 (1.4610e+00)\tAcc@1  67.97 ( 65.62)\tAcc@5  88.28 ( 85.44)\n",
      "Epoch: [50][4004/5005]\tTime  6.253 ( 2.420)\tData  6.103 ( 1.669)\tLoss 1.4374e+00 (1.4657e+00)\tAcc@1  66.80 ( 65.49)\tAcc@5  85.94 ( 85.37)\n",
      " * Acc@1 66.840 Acc@5 87.558\n",
      "************train_loss 1.4708534730421556 val_acc 66.83999633789062*************\n",
      "Epoch: [51][   0/5005]\tTime 85.110 (85.110)\tData 32.392 (32.392)\tLoss 1.3395e+00 (1.3395e+00)\tAcc@1  64.45 ( 64.45)\tAcc@5  88.67 ( 88.67)\n",
      "Epoch: [51][1001/5005]\tTime  0.378 ( 2.306)\tData  0.000 ( 1.727)\tLoss 1.3821e+00 (1.4428e+00)\tAcc@1  66.41 ( 66.12)\tAcc@5  86.72 ( 85.70)\n",
      "Epoch: [51][2002/5005]\tTime  0.380 ( 1.964)\tData  0.000 ( 1.475)\tLoss 1.3843e+00 (1.4537e+00)\tAcc@1  71.09 ( 65.83)\tAcc@5  86.33 ( 85.52)\n",
      "Epoch: [51][3003/5005]\tTime  0.369 ( 2.191)\tData  0.188 ( 1.621)\tLoss 1.4515e+00 (1.4595e+00)\tAcc@1  64.45 ( 65.67)\tAcc@5  86.33 ( 85.43)\n",
      "Epoch: [51][4004/5005]\tTime  0.391 ( 2.303)\tData  0.000 ( 1.654)\tLoss 1.4341e+00 (1.4649e+00)\tAcc@1  66.02 ( 65.53)\tAcc@5  85.55 ( 85.38)\n",
      " * Acc@1 66.550 Acc@5 87.430\n",
      "************train_loss 1.469319171743555 val_acc 66.54999542236328*************\n",
      "Epoch: [52][   0/5005]\tTime 136.422 (136.422)\tData 70.737 (70.737)\tLoss 1.4119e+00 (1.4119e+00)\tAcc@1  63.67 ( 63.67)\tAcc@5  87.50 ( 87.50)\n",
      "Epoch: [52][1001/5005]\tTime  0.375 ( 2.584)\tData  0.000 ( 1.844)\tLoss 1.3917e+00 (1.4376e+00)\tAcc@1  69.14 ( 66.08)\tAcc@5  86.33 ( 85.70)\n",
      "Epoch: [52][2002/5005]\tTime  0.381 ( 2.653)\tData  0.000 ( 1.863)\tLoss 1.5559e+00 (1.4488e+00)\tAcc@1  66.80 ( 65.80)\tAcc@5  85.55 ( 85.55)\n",
      "Epoch: [52][3003/5005]\tTime  8.461 ( 2.425)\tData  8.320 ( 1.715)\tLoss 1.5312e+00 (1.4578e+00)\tAcc@1  62.89 ( 65.63)\tAcc@5  82.81 ( 85.43)\n",
      "Epoch: [52][4004/5005]\tTime 24.804 ( 2.281)\tData 19.438 ( 1.632)\tLoss 1.5518e+00 (1.4615e+00)\tAcc@1  64.45 ( 65.56)\tAcc@5  83.98 ( 85.40)\n",
      " * Acc@1 66.730 Acc@5 87.600\n",
      "************train_loss 1.4660155737435783 val_acc 66.72999572753906*************\n",
      "Epoch: [53][   0/5005]\tTime 124.589 (124.589)\tData 64.361 (64.361)\tLoss 1.4881e+00 (1.4881e+00)\tAcc@1  63.28 ( 63.28)\tAcc@5  85.55 ( 85.55)\n",
      "Epoch: [53][1001/5005]\tTime  0.374 ( 2.844)\tData  0.000 ( 1.911)\tLoss 1.6536e+00 (1.4430e+00)\tAcc@1  57.42 ( 65.93)\tAcc@5  83.59 ( 85.68)\n",
      "Epoch: [53][2002/5005]\tTime  1.285 ( 2.483)\tData  1.148 ( 1.693)\tLoss 1.4699e+00 (1.4523e+00)\tAcc@1  66.02 ( 65.75)\tAcc@5  84.38 ( 85.54)\n",
      "Epoch: [53][3003/5005]\tTime  0.377 ( 2.121)\tData  0.000 ( 1.464)\tLoss 1.5486e+00 (1.4589e+00)\tAcc@1  61.72 ( 65.65)\tAcc@5  84.38 ( 85.43)\n",
      "Epoch: [53][4004/5005]\tTime  0.378 ( 2.205)\tData  0.000 ( 1.555)\tLoss 1.5123e+00 (1.4642e+00)\tAcc@1  63.28 ( 65.54)\tAcc@5  84.38 ( 85.37)\n",
      " * Acc@1 66.952 Acc@5 87.474\n",
      "************train_loss 1.4684214232803938 val_acc 66.95199584960938*************\n",
      "Epoch: [54][   0/5005]\tTime 68.506 (68.506)\tData 43.192 (43.192)\tLoss 1.6507e+00 (1.6507e+00)\tAcc@1  63.28 ( 63.28)\tAcc@5  83.20 ( 83.20)\n",
      "Epoch: [54][1001/5005]\tTime  0.377 ( 2.676)\tData  0.000 ( 1.874)\tLoss 1.5838e+00 (1.4332e+00)\tAcc@1  62.89 ( 66.34)\tAcc@5  84.77 ( 85.80)\n",
      "Epoch: [54][2002/5005]\tTime  0.383 ( 2.478)\tData  0.000 ( 1.723)\tLoss 1.4705e+00 (1.4459e+00)\tAcc@1  67.58 ( 66.01)\tAcc@5  83.59 ( 85.62)\n",
      "Epoch: [54][3003/5005]\tTime  0.370 ( 2.640)\tData  0.000 ( 1.794)\tLoss 1.4729e+00 (1.4523e+00)\tAcc@1  66.41 ( 65.83)\tAcc@5  84.38 ( 85.52)\n",
      "Epoch: [54][4004/5005]\tTime  9.517 ( 2.428)\tData  9.365 ( 1.661)\tLoss 1.5050e+00 (1.4585e+00)\tAcc@1  66.80 ( 65.66)\tAcc@5  83.98 ( 85.44)\n",
      " * Acc@1 66.644 Acc@5 87.456\n",
      "************train_loss 1.4630344028834934 val_acc 66.64399719238281*************\n",
      "Epoch: [55][   0/5005]\tTime 76.036 (76.036)\tData 44.802 (44.802)\tLoss 1.3922e+00 (1.3922e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  86.33 ( 86.33)\n",
      "Epoch: [55][1001/5005]\tTime  2.173 ( 2.807)\tData  2.031 ( 1.799)\tLoss 1.3759e+00 (1.4312e+00)\tAcc@1  69.92 ( 66.30)\tAcc@5  85.16 ( 85.74)\n",
      "Epoch: [55][2002/5005]\tTime 86.252 ( 2.926)\tData 47.784 ( 1.872)\tLoss 1.4868e+00 (1.4409e+00)\tAcc@1  67.19 ( 66.06)\tAcc@5  85.55 ( 85.62)\n",
      "Epoch: [55][3003/5005]\tTime  0.364 ( 2.634)\tData  0.000 ( 1.746)\tLoss 1.6049e+00 (1.4493e+00)\tAcc@1  60.94 ( 65.87)\tAcc@5  83.59 ( 85.53)\n",
      "Epoch: [55][4004/5005]\tTime  0.378 ( 2.448)\tData  0.000 ( 1.651)\tLoss 1.5981e+00 (1.4555e+00)\tAcc@1  62.89 ( 65.73)\tAcc@5  85.16 ( 85.44)\n",
      " * Acc@1 66.936 Acc@5 87.388\n",
      "************train_loss 1.4610032380758584 val_acc 66.93599700927734*************\n",
      "Epoch: [56][   0/5005]\tTime 47.063 (47.063)\tData 24.224 (24.224)\tLoss 1.5584e+00 (1.5584e+00)\tAcc@1  62.89 ( 62.89)\tAcc@5  84.77 ( 84.77)\n",
      "Epoch: [56][1001/5005]\tTime  0.401 ( 2.628)\tData  0.000 ( 1.758)\tLoss 1.3225e+00 (1.4342e+00)\tAcc@1  66.02 ( 66.02)\tAcc@5  88.28 ( 85.79)\n",
      "Epoch: [56][2002/5005]\tTime  2.028 ( 2.524)\tData  1.896 ( 1.701)\tLoss 1.3689e+00 (1.4428e+00)\tAcc@1  66.41 ( 65.95)\tAcc@5  86.33 ( 85.63)\n",
      "Epoch: [56][3003/5005]\tTime  0.377 ( 2.403)\tData  0.000 ( 1.661)\tLoss 1.2319e+00 (1.4505e+00)\tAcc@1  67.97 ( 65.81)\tAcc@5  90.62 ( 85.54)\n",
      "Epoch: [56][4004/5005]\tTime  3.424 ( 2.592)\tData  3.284 ( 1.757)\tLoss 1.6133e+00 (1.4553e+00)\tAcc@1  65.23 ( 65.72)\tAcc@5  83.59 ( 85.48)\n",
      " * Acc@1 66.682 Acc@5 87.502\n",
      "************train_loss 1.4582585921416154 val_acc 66.68199920654297*************\n",
      "Epoch: [57][   0/5005]\tTime 45.226 (45.226)\tData 26.360 (26.360)\tLoss 1.1752e+00 (1.1752e+00)\tAcc@1  70.31 ( 70.31)\tAcc@5  89.45 ( 89.45)\n",
      "Epoch: [57][1001/5005]\tTime  0.375 ( 2.164)\tData  0.000 ( 1.530)\tLoss 1.4386e+00 (1.4274e+00)\tAcc@1  63.67 ( 66.21)\tAcc@5  86.72 ( 85.84)\n",
      "Epoch: [57][2002/5005]\tTime  0.376 ( 2.031)\tData  0.000 ( 1.461)\tLoss 1.3829e+00 (1.4355e+00)\tAcc@1  66.02 ( 66.08)\tAcc@5  85.94 ( 85.73)\n",
      "Epoch: [57][3003/5005]\tTime  0.371 ( 2.311)\tData  0.000 ( 1.631)\tLoss 1.6047e+00 (1.4445e+00)\tAcc@1  65.23 ( 65.88)\tAcc@5  85.16 ( 85.61)\n"
     ]
    }
   ],
   "source": [
    "best_acc1 = 0\n",
    "acc1 = 0\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, args.lr)\n",
    "\n",
    "    # train for one epoch\n",
    "    epoch_loss = train(train_loader, model, criterion, optimizer, epoch, args, device)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion, args, device)  \n",
    "    \n",
    "    train_loss.append(epoch_loss)\n",
    "    val_acc.append(acc1)\n",
    "    print('************train_loss {} val_acc {}*************'.format(epoch_loss, acc1))\n",
    "    \n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "#     if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
    "#             and args.rank % ngpus_per_node == 0):\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': args.arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best, args, filename='../trained_model/checkpoint.pt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6de6a0-a209-4b66-b52d-322f333aacae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48e851-7663-40f7-b4d8-417cc3498927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_retina",
   "language": "python",
   "name": "torch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
