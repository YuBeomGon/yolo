{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ab6313-698a-46cc-9ab4-770b35f86211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "# import torchvision.models as models\n",
    "from resnet import *\n",
    "\n",
    "from main import *\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56508d67-5301-4e1e-9454-1fe41b5f3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args(args=[])\n",
    "# args = parser.parse_args()\n",
    "import easydict \n",
    "args = easydict.EasyDict({ \"batch-size\": 256, \n",
    "                          \"epochs\": 50, \n",
    "                          \"data\": 0, \n",
    "                          'arch':'resnet18',\n",
    "                          'lr':0.1,\n",
    "                         'momentum':0.9,\n",
    "                         'weight_decay':1e-4,\n",
    "                         'start_epoch':0,\n",
    "                         'gpu':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea598abf-d5fe-4f70-8cbf-78ad6038e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "ngpus_per_node = torch.cuda.device_count()\n",
    "print(ngpus_per_node)\n",
    "# device = 'cpu'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b5c549-7611-40a8-a5cd-54451c41e793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "imagenet_embeding = np.load('../data/imagenet_embeding.npy')\n",
    "imagenet_embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5e59a2-80d4-49f7-8998-7b78846b333f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet_embeding = torch.tensor(imagenet_embeding)\n",
    "imagenet_embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e9ebbf-4193-412c-b416-9faccf6aac25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet18'\n"
     ]
    }
   ],
   "source": [
    "print(\"=> using pre-trained model '{}'\".format('resnet18'))\n",
    "# model = models.__dict__['resnet18'](pretrained=True)\n",
    "# model = models.resnet18(pretrained=False)\n",
    "model = resnet18(pretrained=False)\n",
    "# model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                            momentum=args.momentum,\n",
    "                            weight_decay=args.weight_decay)\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf479d7-0b16-4399-8f59-fd72bec730d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0136, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc8e115d-c36b-4e00-8f16-9473415528af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), '../trained_model/init.pt')\n",
    "model.load_state_dict(torch.load('../trained_model/init.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a75cef50-6d29-411a-a565-898a4468242d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0301, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8910807d-3862-4446-a811-530857664cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 768])\n",
      "tensor([[-0.0301,  0.0198, -0.0104,  ..., -0.0012, -0.0136,  0.0011],\n",
      "        [-0.0023, -0.0009, -0.0329,  ..., -0.0281, -0.0132,  0.0052],\n",
      "        [ 0.0036, -0.0155, -0.0112,  ..., -0.0055,  0.0028,  0.0204],\n",
      "        ...,\n",
      "        [-0.0349, -0.0049, -0.0264,  ..., -0.0168, -0.0148,  0.0045],\n",
      "        [ 0.0063, -0.0293, -0.0185,  ..., -0.0337, -0.0108,  0.0270],\n",
      "        [-0.0275, -0.0346,  0.0034,  ..., -0.0311,  0.0181,  0.0030]])\n",
      "tensor([[ 0.5611,  0.2227,  0.2527,  ..., -0.1777,  0.2208,  0.6253],\n",
      "        [ 0.0625, -0.3026, -0.2862,  ..., -0.0785, -0.1146, -0.1183],\n",
      "        [ 0.1750,  0.1851, -0.3477,  ..., -0.3395, -0.0346, -0.3034],\n",
      "        ...,\n",
      "        [-0.3333,  0.0872, -0.0674,  ...,  0.2487, -0.1052,  0.0210],\n",
      "        [-0.0106,  0.0222,  0.2349,  ..., -0.4712, -0.5066,  0.3787],\n",
      "        [ 0.6112,  0.1495, -0.0799,  ..., -0.2983, -0.5161,  0.1615]])\n"
     ]
    }
   ],
   "source": [
    "model_dict = model.state_dict() \n",
    "for k in model_dict :\n",
    "    if 'fc.weight' in k :\n",
    "        print(model_dict[k].shape)\n",
    "        print(model_dict[k])\n",
    "        model_dict[k] = imagenet_embeding\n",
    "#     if 'fc.bias' in k :\n",
    "#         print(model_dict[k])        \n",
    "#     print(model_dict[k].shape)\n",
    "model.load_state_dict(model_dict)\n",
    "model_dict = model.state_dict() \n",
    "for k in model_dict :\n",
    "    if 'fc.weight' in k :\n",
    "#         print(model_dict[k].shape)\n",
    "        print(model_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66335ead-2157-4014-8671-62dbab63a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77598382-abf2-459f-9457-f8bdd1cf8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters() :\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5493c8ea-4540-4067-8954-9e7a73cb10da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(384, 768, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6cd832a-5720-4925-9d50-36b16480132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "data_dir = '../ILSVRC/Data/CLS-LOC/'\n",
    "traindir = os.path.join(data_dir, 'train')\n",
    "valdir = os.path.join(data_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb79f3f-e8a8-4e0d-9a55-f7dbf5c08236",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3d9c94c-b533-4147-96f0-57afbf42624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4460bdaa-261a-4858-a94c-c70aebd6fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "train_sampler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beece8cc-7ac9-4e30-8cca-0ca4adbd58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=256, shuffle=(train_sampler is None),\n",
    "    num_workers=8, pin_memory=True, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53171b34-4e2f-4b1b-ae3f-d8d2db58b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32, shuffle=False,\n",
    "    num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afed6352-ebb9-4fcf-ade1-12354b4e7755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(384, 768, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25092250-abbb-482c-bb3a-3c279e593859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][   0/5005]\tTime  2.852 ( 2.852)\tData  2.473 ( 2.473)\tLoss 9.6591e+00 (9.6591e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\n",
      "Epoch: [0][1001/5005]\tTime  0.390 ( 0.387)\tData  0.000 ( 0.003)\tLoss 6.9821e+00 (7.0464e+00)\tAcc@1   0.39 (  0.35)\tAcc@5   1.56 (  1.55)\n",
      "Epoch: [0][2002/5005]\tTime  0.390 ( 0.388)\tData  0.000 ( 0.002)\tLoss 6.5474e+00 (6.8368e+00)\tAcc@1   0.00 (  0.57)\tAcc@5   3.52 (  2.36)\n",
      "Epoch: [0][3003/5005]\tTime  0.391 ( 0.388)\tData  0.000 ( 0.001)\tLoss 6.3105e+00 (6.6725e+00)\tAcc@1   2.73 (  0.86)\tAcc@5   7.42 (  3.40)\n",
      "Epoch: [0][4004/5005]\tTime  0.390 ( 0.389)\tData  0.000 ( 0.001)\tLoss 6.0042e+00 (6.5211e+00)\tAcc@1   3.12 (  1.27)\tAcc@5  10.16 (  4.68)\n",
      " * Acc@1 3.727 Acc@5 11.476\n",
      "************train_loss 6.388201045132541 val_acc 3.7274527549743652*************\n",
      "Epoch: [1][   0/5005]\tTime  2.266 ( 2.266)\tData  2.124 ( 2.124)\tLoss 5.9463e+00 (5.9463e+00)\tAcc@1   3.91 (  3.91)\tAcc@5  10.55 ( 10.55)\n",
      "Epoch: [1][1001/5005]\tTime  0.405 ( 0.392)\tData  0.000 ( 0.003)\tLoss 5.7594e+00 (5.6648e+00)\tAcc@1   4.30 (  5.07)\tAcc@5  14.06 ( 14.60)\n",
      "Epoch: [1][2002/5005]\tTime  0.376 ( 0.390)\tData  0.000 ( 0.001)\tLoss 5.5883e+00 (5.5743e+00)\tAcc@1   8.59 (  5.76)\tAcc@5  17.19 ( 16.06)\n",
      "Epoch: [1][3003/5005]\tTime  0.378 ( 0.390)\tData  0.000 ( 0.001)\tLoss 5.2312e+00 (5.4811e+00)\tAcc@1   9.38 (  6.55)\tAcc@5  23.83 ( 17.65)\n",
      "Epoch: [1][4004/5005]\tTime  0.389 ( 0.390)\tData  0.000 ( 0.001)\tLoss 4.9051e+00 (5.3804e+00)\tAcc@1  14.84 (  7.50)\tAcc@5  30.08 ( 19.42)\n",
      " * Acc@1 12.467 Acc@5 29.238\n",
      "************train_loss 5.280509019826914 val_acc 12.466791152954102*************\n",
      "Epoch: [2][   0/5005]\tTime  2.458 ( 2.458)\tData  2.313 ( 2.313)\tLoss 4.8310e+00 (4.8310e+00)\tAcc@1  12.50 ( 12.50)\tAcc@5  28.52 ( 28.52)\n",
      "Epoch: [2][1001/5005]\tTime  0.389 ( 0.392)\tData  0.000 ( 0.003)\tLoss 4.4780e+00 (4.6691e+00)\tAcc@1  16.80 ( 14.58)\tAcc@5  33.98 ( 32.19)\n",
      "Epoch: [2][2002/5005]\tTime  0.388 ( 0.391)\tData  0.000 ( 0.002)\tLoss 4.6987e+00 (4.5824e+00)\tAcc@1  16.02 ( 15.66)\tAcc@5  35.55 ( 33.86)\n",
      "Epoch: [2][3003/5005]\tTime  0.387 ( 0.390)\tData  0.000 ( 0.001)\tLoss 4.3456e+00 (4.5042e+00)\tAcc@1  18.36 ( 16.62)\tAcc@5  40.23 ( 35.29)\n",
      "Epoch: [2][4004/5005]\tTime  0.391 ( 0.390)\tData  0.000 ( 0.001)\tLoss 4.3190e+00 (4.4297e+00)\tAcc@1  18.36 ( 17.57)\tAcc@5  38.67 ( 36.68)\n",
      " * Acc@1 24.262 Acc@5 47.300\n",
      "************train_loss 4.363346622659491 val_acc 24.262401580810547*************\n",
      "Epoch: [3][   0/5005]\tTime  2.074 ( 2.074)\tData  1.916 ( 1.916)\tLoss 3.7201e+00 (3.7201e+00)\tAcc@1  25.78 ( 25.78)\tAcc@5  48.05 ( 48.05)\n",
      "Epoch: [3][1001/5005]\tTime  0.392 ( 0.392)\tData  0.000 ( 0.002)\tLoss 4.0346e+00 (3.9593e+00)\tAcc@1  22.27 ( 23.51)\tAcc@5  42.58 ( 45.30)\n",
      "Epoch: [3][2002/5005]\tTime  0.385 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.8928e+00 (3.9201e+00)\tAcc@1  23.44 ( 24.20)\tAcc@5  48.83 ( 46.09)\n",
      "Epoch: [3][3003/5005]\tTime  0.390 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.6000e+00 (3.8807e+00)\tAcc@1  26.56 ( 24.78)\tAcc@5  53.91 ( 46.80)\n",
      "Epoch: [3][4004/5005]\tTime  0.389 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.8493e+00 (3.8418e+00)\tAcc@1  24.61 ( 25.37)\tAcc@5  49.22 ( 47.52)\n",
      " * Acc@1 27.682 Acc@5 51.337\n",
      "************train_loss 3.806273166664116 val_acc 27.682228088378906*************\n",
      "Epoch: [4][   0/5005]\tTime  2.359 ( 2.359)\tData  2.207 ( 2.207)\tLoss 3.5635e+00 (3.5635e+00)\tAcc@1  32.42 ( 32.42)\tAcc@5  50.78 ( 50.78)\n",
      "Epoch: [4][1001/5005]\tTime  0.392 ( 0.392)\tData  0.000 ( 0.003)\tLoss 3.4403e+00 (3.5672e+00)\tAcc@1  31.64 ( 29.31)\tAcc@5  53.12 ( 52.45)\n",
      "Epoch: [4][2002/5005]\tTime  0.388 ( 0.391)\tData  0.000 ( 0.002)\tLoss 3.5774e+00 (3.5489e+00)\tAcc@1  30.08 ( 29.63)\tAcc@5  52.34 ( 52.76)\n",
      "Epoch: [4][3003/5005]\tTime  0.391 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.4451e+00 (3.5295e+00)\tAcc@1  30.86 ( 29.94)\tAcc@5  57.03 ( 53.11)\n",
      "Epoch: [4][4004/5005]\tTime  0.391 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.6272e+00 (3.5107e+00)\tAcc@1  30.86 ( 30.22)\tAcc@5  50.78 ( 53.45)\n",
      " * Acc@1 33.669 Acc@5 58.241\n",
      "************train_loss 3.4883547952006033 val_acc 33.66892623901367*************\n",
      "Epoch: [5][   0/5005]\tTime  2.147 ( 2.147)\tData  2.002 ( 2.002)\tLoss 3.1337e+00 (3.1337e+00)\tAcc@1  37.89 ( 37.89)\tAcc@5  58.20 ( 58.20)\n",
      "Epoch: [5][1001/5005]\tTime  0.393 ( 0.392)\tData  0.000 ( 0.002)\tLoss 3.1749e+00 (3.3339e+00)\tAcc@1  33.59 ( 32.83)\tAcc@5  59.77 ( 56.56)\n",
      "Epoch: [5][2002/5005]\tTime  0.385 ( 0.391)\tData  0.000 ( 0.001)\tLoss 3.6076e+00 (3.3260e+00)\tAcc@1  26.17 ( 32.96)\tAcc@5  50.00 ( 56.70)\n",
      "Epoch: [5][3003/5005]\tTime  0.388 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.3676e+00 (3.3155e+00)\tAcc@1  32.42 ( 33.15)\tAcc@5  56.64 ( 56.92)\n",
      "Epoch: [5][4004/5005]\tTime  0.386 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.1069e+00 (3.3043e+00)\tAcc@1  37.50 ( 33.35)\tAcc@5  60.16 ( 57.13)\n",
      " * Acc@1 32.730 Acc@5 57.406\n",
      "************train_loss 3.2932225483638065 val_acc 32.730072021484375*************\n",
      "Epoch: [6][   0/5005]\tTime  2.248 ( 2.248)\tData  2.107 ( 2.107)\tLoss 3.0862e+00 (3.0862e+00)\tAcc@1  36.33 ( 36.33)\tAcc@5  63.67 ( 63.67)\n",
      "Epoch: [6][1001/5005]\tTime  0.390 ( 0.392)\tData  0.000 ( 0.003)\tLoss 3.1230e+00 (3.1800e+00)\tAcc@1  40.62 ( 35.35)\tAcc@5  61.33 ( 59.19)\n",
      "Epoch: [6][2002/5005]\tTime  0.388 ( 0.391)\tData  0.000 ( 0.002)\tLoss 3.1589e+00 (3.1843e+00)\tAcc@1  33.20 ( 35.22)\tAcc@5  59.38 ( 59.16)\n",
      "Epoch: [6][3003/5005]\tTime  0.390 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.1210e+00 (3.1765e+00)\tAcc@1  41.02 ( 35.34)\tAcc@5  62.50 ( 59.29)\n",
      "Epoch: [6][4004/5005]\tTime  0.381 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.9502e+00 (3.1695e+00)\tAcc@1  40.23 ( 35.46)\tAcc@5  60.94 ( 59.43)\n",
      " * Acc@1 37.277 Acc@5 62.442\n",
      "************train_loss 3.1620732058297385 val_acc 37.27652359008789*************\n",
      "Epoch: [7][   0/5005]\tTime  2.183 ( 2.183)\tData  2.033 ( 2.033)\tLoss 3.1044e+00 (3.1044e+00)\tAcc@1  33.59 ( 33.59)\tAcc@5  59.38 ( 59.38)\n",
      "Epoch: [7][1001/5005]\tTime  0.388 ( 0.392)\tData  0.000 ( 0.002)\tLoss 3.2810e+00 (3.0744e+00)\tAcc@1  33.98 ( 36.93)\tAcc@5  58.59 ( 61.04)\n",
      "Epoch: [7][2002/5005]\tTime  0.390 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.9639e+00 (3.0825e+00)\tAcc@1  40.62 ( 36.86)\tAcc@5  62.50 ( 60.93)\n",
      "Epoch: [7][3003/5005]\tTime  0.392 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.0327e+00 (3.0793e+00)\tAcc@1  33.98 ( 36.93)\tAcc@5  64.45 ( 60.99)\n",
      "Epoch: [7][4004/5005]\tTime  0.388 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.7959e+00 (3.0732e+00)\tAcc@1  39.45 ( 37.01)\tAcc@5  67.58 ( 61.09)\n",
      " * Acc@1 35.934 Acc@5 61.301\n",
      "************train_loss 3.069625977869634 val_acc 35.93416213989258*************\n",
      "Epoch: [8][   0/5005]\tTime  2.491 ( 2.491)\tData  2.335 ( 2.335)\tLoss 2.9617e+00 (2.9617e+00)\tAcc@1  37.50 ( 37.50)\tAcc@5  64.06 ( 64.06)\n",
      "Epoch: [8][1001/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.003)\tLoss 3.0188e+00 (2.9971e+00)\tAcc@1  37.11 ( 38.22)\tAcc@5  61.33 ( 62.29)\n",
      "Epoch: [8][2002/5005]\tTime  0.388 ( 0.391)\tData  0.000 ( 0.002)\tLoss 3.1090e+00 (3.0007e+00)\tAcc@1  36.72 ( 38.21)\tAcc@5  59.77 ( 62.24)\n",
      "Epoch: [8][3003/5005]\tTime  0.389 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.7963e+00 (3.0020e+00)\tAcc@1  42.58 ( 38.20)\tAcc@5  64.06 ( 62.28)\n",
      "Epoch: [8][4004/5005]\tTime  0.385 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.1963e+00 (2.9996e+00)\tAcc@1  34.77 ( 38.24)\tAcc@5  57.81 ( 62.35)\n",
      " * Acc@1 38.199 Acc@5 63.710\n",
      "************train_loss 2.9995626851157113 val_acc 38.199398040771484*************\n",
      "Epoch: [9][   0/5005]\tTime  2.349 ( 2.349)\tData  2.202 ( 2.202)\tLoss 3.1932e+00 (3.1932e+00)\tAcc@1  38.67 ( 38.67)\tAcc@5  59.38 ( 59.38)\n",
      "Epoch: [9][1001/5005]\tTime  0.391 ( 0.392)\tData  0.000 ( 0.003)\tLoss 2.7958e+00 (2.9371e+00)\tAcc@1  39.06 ( 39.25)\tAcc@5  66.41 ( 63.38)\n",
      "Epoch: [9][2002/5005]\tTime  0.391 ( 0.391)\tData  0.000 ( 0.002)\tLoss 3.1401e+00 (2.9411e+00)\tAcc@1  33.20 ( 39.14)\tAcc@5  62.50 ( 63.33)\n",
      "Epoch: [9][3003/5005]\tTime  0.383 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.6548e+00 (2.9443e+00)\tAcc@1  42.97 ( 39.10)\tAcc@5  70.31 ( 63.25)\n",
      "Epoch: [9][4004/5005]\tTime  0.388 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.1693e+00 (2.9451e+00)\tAcc@1  30.08 ( 39.10)\tAcc@5  56.64 ( 63.27)\n",
      " * Acc@1 40.710 Acc@5 66.387\n",
      "************train_loss 2.9433994672872448 val_acc 40.71033477783203*************\n",
      "Epoch: [10][   0/5005]\tTime  2.456 ( 2.456)\tData  2.313 ( 2.313)\tLoss 2.7446e+00 (2.7446e+00)\tAcc@1  38.67 ( 38.67)\tAcc@5  64.84 ( 64.84)\n",
      "Epoch: [10][1001/5005]\tTime  0.389 ( 0.393)\tData  0.000 ( 0.003)\tLoss 3.1715e+00 (2.8978e+00)\tAcc@1  32.42 ( 39.88)\tAcc@5  59.38 ( 64.07)\n",
      "Epoch: [10][2002/5005]\tTime  0.393 ( 0.391)\tData  0.000 ( 0.002)\tLoss 2.6961e+00 (2.9020e+00)\tAcc@1  45.31 ( 39.81)\tAcc@5  69.53 ( 64.00)\n",
      "Epoch: [10][3003/5005]\tTime  0.380 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.8841e+00 (2.9049e+00)\tAcc@1  37.89 ( 39.78)\tAcc@5  61.33 ( 63.96)\n",
      "Epoch: [10][4004/5005]\tTime  0.390 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.7234e+00 (2.9057e+00)\tAcc@1  45.70 ( 39.79)\tAcc@5  64.84 ( 63.95)\n",
      " * Acc@1 40.223 Acc@5 66.177\n",
      "************train_loss 2.904777197737794 val_acc 40.222930908203125*************\n",
      "Epoch: [11][   0/5005]\tTime  2.370 ( 2.370)\tData  2.228 ( 2.228)\tLoss 2.9135e+00 (2.9135e+00)\tAcc@1  39.45 ( 39.45)\tAcc@5  61.72 ( 61.72)\n",
      "Epoch: [11][1001/5005]\tTime  0.386 ( 0.393)\tData  0.000 ( 0.003)\tLoss 3.1687e+00 (2.8531e+00)\tAcc@1  36.72 ( 40.63)\tAcc@5  59.77 ( 64.72)\n",
      "Epoch: [11][2002/5005]\tTime  0.389 ( 0.391)\tData  0.000 ( 0.002)\tLoss 2.9606e+00 (2.8608e+00)\tAcc@1  39.06 ( 40.52)\tAcc@5  64.84 ( 64.64)\n",
      "Epoch: [11][3003/5005]\tTime  0.392 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.7583e+00 (2.8635e+00)\tAcc@1  41.41 ( 40.53)\tAcc@5  65.62 ( 64.61)\n",
      "Epoch: [11][4004/5005]\tTime  0.387 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.7128e+00 (2.8662e+00)\tAcc@1  41.41 ( 40.48)\tAcc@5  66.02 ( 64.57)\n",
      " * Acc@1 39.905 Acc@5 64.955\n",
      "************train_loss 2.8678498216204114 val_acc 39.90531539916992*************\n",
      "Epoch: [12][   0/5005]\tTime  2.429 ( 2.429)\tData  2.286 ( 2.286)\tLoss 2.9089e+00 (2.9089e+00)\tAcc@1  39.45 ( 39.45)\tAcc@5  64.06 ( 64.06)\n",
      "Epoch: [12][1001/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.003)\tLoss 2.7177e+00 (2.8291e+00)\tAcc@1  44.53 ( 40.92)\tAcc@5  66.41 ( 65.17)\n",
      "Epoch: [12][2002/5005]\tTime  0.390 ( 0.391)\tData  0.000 ( 0.002)\tLoss 2.9108e+00 (2.8334e+00)\tAcc@1  42.58 ( 40.84)\tAcc@5  63.28 ( 65.12)\n",
      "Epoch: [12][3003/5005]\tTime  0.388 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.7448e+00 (2.8341e+00)\tAcc@1  39.45 ( 40.93)\tAcc@5  67.58 ( 65.13)\n",
      "Epoch: [12][4004/5005]\tTime  0.388 ( 0.390)\tData  0.000 ( 0.001)\tLoss 3.0507e+00 (2.8375e+00)\tAcc@1  37.11 ( 40.91)\tAcc@5  65.23 ( 65.10)\n",
      " * Acc@1 43.257 Acc@5 68.768\n",
      "************train_loss 2.840689935503187 val_acc 43.25722885131836*************\n",
      "Epoch: [13][   0/5005]\tTime  2.375 ( 2.375)\tData  2.232 ( 2.232)\tLoss 2.6651e+00 (2.6651e+00)\tAcc@1  43.75 ( 43.75)\tAcc@5  67.97 ( 67.97)\n",
      "Epoch: [13][1001/5005]\tTime  0.391 ( 0.393)\tData  0.000 ( 0.003)\tLoss 2.8015e+00 (2.7994e+00)\tAcc@1  38.67 ( 41.63)\tAcc@5  64.45 ( 65.66)\n",
      "Epoch: [13][2002/5005]\tTime  0.391 ( 0.391)\tData  0.000 ( 0.002)\tLoss 2.6687e+00 (2.8054e+00)\tAcc@1  43.75 ( 41.50)\tAcc@5  65.23 ( 65.60)\n",
      "Epoch: [13][3003/5005]\tTime  0.390 ( 0.391)\tData  0.000 ( 0.001)\tLoss 3.0328e+00 (2.8097e+00)\tAcc@1  38.28 ( 41.40)\tAcc@5  64.45 ( 65.54)\n",
      "Epoch: [13][4004/5005]\tTime  0.394 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.9035e+00 (2.8116e+00)\tAcc@1  42.19 ( 41.35)\tAcc@5  64.45 ( 65.51)\n",
      " * Acc@1 43.990 Acc@5 69.807\n",
      "************train_loss 2.8150607459671373 val_acc 43.990333557128906*************\n",
      "Epoch: [14][   0/5005]\tTime  2.293 ( 2.293)\tData  2.148 ( 2.148)\tLoss 2.6566e+00 (2.6566e+00)\tAcc@1  41.41 ( 41.41)\tAcc@5  66.80 ( 66.80)\n",
      "Epoch: [14][1001/5005]\tTime  0.381 ( 0.393)\tData  0.000 ( 0.003)\tLoss 2.8721e+00 (2.7821e+00)\tAcc@1  41.02 ( 41.83)\tAcc@5  64.45 ( 66.06)\n",
      "Epoch: [14][2002/5005]\tTime  0.381 ( 0.391)\tData  0.000 ( 0.002)\tLoss 2.4850e+00 (2.7875e+00)\tAcc@1  47.27 ( 41.75)\tAcc@5  69.92 ( 65.96)\n",
      "Epoch: [14][3003/5005]\tTime  0.381 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.7198e+00 (2.7915e+00)\tAcc@1  46.09 ( 41.71)\tAcc@5  68.75 ( 65.91)\n",
      "Epoch: [14][4004/5005]\tTime  0.392 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.7089e+00 (2.7928e+00)\tAcc@1  41.41 ( 41.69)\tAcc@5  65.62 ( 65.87)\n",
      " * Acc@1 43.575 Acc@5 69.134\n",
      "************train_loss 2.7945123260433262 val_acc 43.5748405456543*************\n",
      "Epoch: [15][   0/5005]\tTime  2.206 ( 2.206)\tData  2.045 ( 2.045)\tLoss 2.7389e+00 (2.7389e+00)\tAcc@1  42.58 ( 42.58)\tAcc@5  66.80 ( 66.80)\n",
      "Epoch: [15][1001/5005]\tTime  0.388 ( 0.393)\tData  0.000 ( 0.002)\tLoss 2.7663e+00 (2.7531e+00)\tAcc@1  42.19 ( 42.28)\tAcc@5  64.45 ( 66.37)\n",
      "Epoch: [15][2002/5005]\tTime  0.379 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.5411e+00 (2.7649e+00)\tAcc@1  45.31 ( 42.12)\tAcc@5  70.31 ( 66.23)\n",
      "Epoch: [15][3003/5005]\tTime  0.390 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.7360e+00 (2.7731e+00)\tAcc@1  41.02 ( 42.00)\tAcc@5  65.23 ( 66.10)\n",
      "Epoch: [15][4004/5005]\tTime  0.391 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.5622e+00 (2.7753e+00)\tAcc@1  50.78 ( 41.98)\tAcc@5  68.36 ( 66.07)\n",
      " * Acc@1 44.392 Acc@5 70.158\n",
      "************train_loss 2.776426762753314 val_acc 44.391841888427734*************\n",
      "Epoch: [16][   0/5005]\tTime  2.275 ( 2.275)\tData  2.124 ( 2.124)\tLoss 2.9462e+00 (2.9462e+00)\tAcc@1  35.94 ( 35.94)\tAcc@5  63.28 ( 63.28)\n",
      "Epoch: [16][1001/5005]\tTime  0.391 ( 0.393)\tData  0.000 ( 0.003)\tLoss 2.6412e+00 (2.7500e+00)\tAcc@1  44.53 ( 42.26)\tAcc@5  67.19 ( 66.49)\n",
      "Epoch: [16][2002/5005]\tTime  0.391 ( 0.391)\tData  0.000 ( 0.002)\tLoss 2.6237e+00 (2.7544e+00)\tAcc@1  41.80 ( 42.19)\tAcc@5  67.19 ( 66.44)\n",
      "Epoch: [16][3003/5005]\tTime  0.388 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.7703e+00 (2.7589e+00)\tAcc@1  39.06 ( 42.16)\tAcc@5  63.28 ( 66.35)\n",
      "Epoch: [16][4004/5005]\tTime  0.389 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.7259e+00 (2.7604e+00)\tAcc@1  41.02 ( 42.15)\tAcc@5  65.62 ( 66.35)\n",
      " * Acc@1 41.967 Acc@5 67.673\n",
      "************train_loss 2.7621834119478543 val_acc 41.966800689697266*************\n",
      "Epoch: [17][   0/5005]\tTime  2.435 ( 2.435)\tData  2.274 ( 2.274)\tLoss 2.5442e+00 (2.5442e+00)\tAcc@1  46.09 ( 46.09)\tAcc@5  67.97 ( 67.97)\n",
      "Epoch: [17][1001/5005]\tTime  0.391 ( 0.393)\tData  0.000 ( 0.003)\tLoss 2.7816e+00 (2.7289e+00)\tAcc@1  36.72 ( 42.72)\tAcc@5  69.14 ( 66.82)\n",
      "Epoch: [17][2002/5005]\tTime  0.392 ( 0.391)\tData  0.000 ( 0.002)\tLoss 2.9421e+00 (2.7375e+00)\tAcc@1  36.72 ( 42.52)\tAcc@5  62.89 ( 66.70)\n",
      "Epoch: [17][3003/5005]\tTime  0.387 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.7277e+00 (2.7439e+00)\tAcc@1  41.80 ( 42.48)\tAcc@5  63.67 ( 66.64)\n",
      "Epoch: [17][4004/5005]\tTime  0.391 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.9409e+00 (2.7461e+00)\tAcc@1  43.75 ( 42.43)\tAcc@5  68.36 ( 66.63)\n",
      " * Acc@1 43.853 Acc@5 69.439\n",
      "************train_loss 2.7505254923642335 val_acc 43.852500915527344*************\n",
      "Epoch: [18][   0/5005]\tTime  1.852 ( 1.852)\tData  1.712 ( 1.712)\tLoss 2.4570e+00 (2.4570e+00)\tAcc@1  45.70 ( 45.70)\tAcc@5  69.14 ( 69.14)\n",
      "Epoch: [18][1001/5005]\tTime  0.391 ( 0.392)\tData  0.000 ( 0.002)\tLoss 2.9419e+00 (2.7230e+00)\tAcc@1  43.75 ( 42.84)\tAcc@5  64.84 ( 66.98)\n",
      "Epoch: [18][2002/5005]\tTime  0.390 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.5550e+00 (2.7236e+00)\tAcc@1  44.53 ( 42.84)\tAcc@5  67.19 ( 66.98)\n",
      "Epoch: [18][3003/5005]\tTime  0.378 ( 0.391)\tData  0.000 ( 0.001)\tLoss 3.0773e+00 (2.7289e+00)\tAcc@1  36.72 ( 42.71)\tAcc@5  62.11 ( 66.94)\n",
      "Epoch: [18][4004/5005]\tTime  0.392 ( 0.390)\tData  0.000 ( 0.001)\tLoss 2.8800e+00 (2.7350e+00)\tAcc@1  42.58 ( 42.62)\tAcc@5  65.62 ( 66.83)\n",
      " * Acc@1 45.015 Acc@5 70.778\n",
      "************train_loss 2.7366016838576765 val_acc 45.01508331298828*************\n",
      "Epoch: [19][   0/5005]\tTime  2.465 ( 2.465)\tData  2.322 ( 2.322)\tLoss 2.7905e+00 (2.7905e+00)\tAcc@1  41.80 ( 41.80)\tAcc@5  67.97 ( 67.97)\n",
      "Epoch: [19][1001/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.003)\tLoss 2.6204e+00 (2.7075e+00)\tAcc@1  46.09 ( 42.96)\tAcc@5  70.31 ( 67.29)\n",
      "Epoch: [19][2002/5005]\tTime  0.389 ( 0.392)\tData  0.000 ( 0.002)\tLoss 3.0561e+00 (2.7116e+00)\tAcc@1  37.11 ( 42.94)\tAcc@5  62.89 ( 67.20)\n",
      "Epoch: [19][3003/5005]\tTime  0.392 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.7688e+00 (2.7190e+00)\tAcc@1  42.97 ( 42.88)\tAcc@5  63.67 ( 67.06)\n",
      "Epoch: [19][4004/5005]\tTime  0.389 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.8512e+00 (2.7224e+00)\tAcc@1  40.62 ( 42.85)\tAcc@5  67.58 ( 67.03)\n",
      " * Acc@1 45.413 Acc@5 71.097\n",
      "************train_loss 2.726197893064577 val_acc 45.41259765625*************\n",
      "Epoch: [20][   0/5005]\tTime  2.338 ( 2.338)\tData  2.190 ( 2.190)\tLoss 2.8017e+00 (2.8017e+00)\tAcc@1  39.45 ( 39.45)\tAcc@5  66.41 ( 66.41)\n",
      "Epoch: [20][1001/5005]\tTime  0.389 ( 0.394)\tData  0.000 ( 0.003)\tLoss 2.8116e+00 (2.6965e+00)\tAcc@1  44.92 ( 43.27)\tAcc@5  66.02 ( 67.49)\n",
      "Epoch: [20][2002/5005]\tTime  0.392 ( 0.392)\tData  0.000 ( 0.002)\tLoss 2.8475e+00 (2.6968e+00)\tAcc@1  38.28 ( 43.28)\tAcc@5  66.80 ( 67.45)\n",
      "Epoch: [20][3003/5005]\tTime  0.393 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.4882e+00 (2.7057e+00)\tAcc@1  44.53 ( 43.13)\tAcc@5  72.27 ( 67.30)\n",
      "Epoch: [20][4004/5005]\tTime  0.391 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.5424e+00 (2.7107e+00)\tAcc@1  48.83 ( 43.06)\tAcc@5  71.88 ( 67.23)\n",
      " * Acc@1 46.891 Acc@5 71.848\n",
      "************train_loss 2.7147942404885153 val_acc 46.89079666137695*************\n",
      "Epoch: [21][   0/5005]\tTime  2.282 ( 2.282)\tData  2.137 ( 2.137)\tLoss 2.6601e+00 (2.6601e+00)\tAcc@1  42.58 ( 42.58)\tAcc@5  66.02 ( 66.02)\n",
      "Epoch: [21][1001/5005]\tTime  0.391 ( 0.394)\tData  0.000 ( 0.003)\tLoss 2.5636e+00 (2.6791e+00)\tAcc@1  41.41 ( 43.53)\tAcc@5  70.31 ( 67.77)\n",
      "Epoch: [21][2002/5005]\tTime  0.388 ( 0.392)\tData  0.000 ( 0.002)\tLoss 2.9899e+00 (2.6936e+00)\tAcc@1  39.45 ( 43.32)\tAcc@5  60.55 ( 67.51)\n",
      "Epoch: [21][3003/5005]\tTime  0.385 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.6213e+00 (2.7001e+00)\tAcc@1  47.27 ( 43.21)\tAcc@5  67.97 ( 67.41)\n",
      "Epoch: [21][4004/5005]\tTime  0.392 ( 0.391)\tData  0.000 ( 0.001)\tLoss 2.6673e+00 (2.7052e+00)\tAcc@1  41.80 ( 43.14)\tAcc@5  69.53 ( 67.32)\n",
      " * Acc@1 44.913 Acc@5 70.444\n",
      "************train_loss 2.707239945094426 val_acc 44.9132080078125*************\n",
      "Epoch: [22][   0/5005]\tTime  2.456 ( 2.456)\tData  2.311 ( 2.311)\tLoss 2.7600e+00 (2.7600e+00)\tAcc@1  42.58 ( 42.58)\tAcc@5  67.19 ( 67.19)\n",
      "Epoch: [22][1001/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.003)\tLoss 2.3485e+00 (2.6859e+00)\tAcc@1  49.22 ( 43.53)\tAcc@5  74.61 ( 67.49)\n",
      "Epoch: [22][2002/5005]\tTime  0.393 ( 0.392)\tData  0.000 ( 0.002)\tLoss 2.8370e+00 (2.6931e+00)\tAcc@1  44.53 ( 43.37)\tAcc@5  65.23 ( 67.44)\n",
      "Epoch: [22][3003/5005]\tTime  0.391 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.9112e+00 (2.7000e+00)\tAcc@1  41.41 ( 43.27)\tAcc@5  64.06 ( 67.37)\n",
      "Epoch: [22][4004/5005]\tTime  0.392 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.6729e+00 (2.7017e+00)\tAcc@1  44.53 ( 43.24)\tAcc@5  70.31 ( 67.36)\n",
      " * Acc@1 44.779 Acc@5 70.402\n",
      "************train_loss 2.703896752556602 val_acc 44.77936935424805*************\n",
      "Epoch: [23][   0/5005]\tTime  1.893 ( 1.893)\tData  1.749 ( 1.749)\tLoss 2.7436e+00 (2.7436e+00)\tAcc@1  41.80 ( 41.80)\tAcc@5  67.97 ( 67.97)\n",
      "Epoch: [23][1001/5005]\tTime  0.394 ( 0.394)\tData  0.000 ( 0.003)\tLoss 2.6016e+00 (2.6712e+00)\tAcc@1  45.31 ( 43.72)\tAcc@5  69.92 ( 67.86)\n",
      "Epoch: [23][2002/5005]\tTime  0.387 ( 0.393)\tData  0.000 ( 0.002)\tLoss 2.6916e+00 (2.6797e+00)\tAcc@1  43.36 ( 43.59)\tAcc@5  68.75 ( 67.72)\n",
      "Epoch: [23][3003/5005]\tTime  0.389 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.3936e+00 (2.6846e+00)\tAcc@1  50.00 ( 43.49)\tAcc@5  71.09 ( 67.65)\n",
      "Epoch: [23][4004/5005]\tTime  0.392 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.7794e+00 (2.6897e+00)\tAcc@1  45.70 ( 43.41)\tAcc@5  66.41 ( 67.58)\n",
      " * Acc@1 46.463 Acc@5 71.641\n",
      "************train_loss 2.692817587285609 val_acc 46.46331787109375*************\n",
      "Epoch: [24][   0/5005]\tTime  2.312 ( 2.312)\tData  2.158 ( 2.158)\tLoss 3.0700e+00 (3.0700e+00)\tAcc@1  43.36 ( 43.36)\tAcc@5  64.06 ( 64.06)\n",
      "Epoch: [24][1001/5005]\tTime  0.392 ( 0.394)\tData  0.000 ( 0.003)\tLoss 2.7473e+00 (2.6634e+00)\tAcc@1  45.70 ( 43.86)\tAcc@5  67.58 ( 67.94)\n",
      "Epoch: [24][2002/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.002)\tLoss 2.8063e+00 (2.6736e+00)\tAcc@1  44.53 ( 43.70)\tAcc@5  65.23 ( 67.83)\n",
      "Epoch: [24][3003/5005]\tTime  0.390 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.5931e+00 (2.6783e+00)\tAcc@1  46.09 ( 43.62)\tAcc@5  68.36 ( 67.74)\n",
      "Epoch: [24][4004/5005]\tTime  0.391 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.5983e+00 (2.6842e+00)\tAcc@1  44.53 ( 43.53)\tAcc@5  66.02 ( 67.67)\n",
      " * Acc@1 44.256 Acc@5 69.723\n",
      "************train_loss 2.6872817250517578 val_acc 44.25600814819336*************\n",
      "Epoch: [25][   0/5005]\tTime  1.896 ( 1.896)\tData  1.756 ( 1.756)\tLoss 2.7526e+00 (2.7526e+00)\tAcc@1  46.88 ( 46.88)\tAcc@5  65.23 ( 65.23)\n",
      "Epoch: [25][1001/5005]\tTime  0.378 ( 0.394)\tData  0.000 ( 0.002)\tLoss 2.7492e+00 (2.6607e+00)\tAcc@1  47.27 ( 43.82)\tAcc@5  70.70 ( 68.05)\n",
      "Epoch: [25][2002/5005]\tTime  0.394 ( 0.393)\tData  0.000 ( 0.001)\tLoss 2.5763e+00 (2.6700e+00)\tAcc@1  48.44 ( 43.71)\tAcc@5  68.75 ( 67.90)\n",
      "Epoch: [25][3003/5005]\tTime  0.391 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.8889e+00 (2.6758e+00)\tAcc@1  43.75 ( 43.65)\tAcc@5  68.75 ( 67.83)\n",
      "Epoch: [25][4004/5005]\tTime  0.393 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.8253e+00 (2.6799e+00)\tAcc@1  41.41 ( 43.57)\tAcc@5  64.84 ( 67.77)\n",
      " * Acc@1 44.941 Acc@5 70.880\n",
      "************train_loss 2.68168786751045 val_acc 44.9411735534668*************\n",
      "Epoch: [26][   0/5005]\tTime  1.919 ( 1.919)\tData  1.778 ( 1.778)\tLoss 2.8825e+00 (2.8825e+00)\tAcc@1  42.58 ( 42.58)\tAcc@5  62.11 ( 62.11)\n",
      "Epoch: [26][1001/5005]\tTime  0.389 ( 0.394)\tData  0.000 ( 0.002)\tLoss 2.7042e+00 (2.6592e+00)\tAcc@1  44.92 ( 43.83)\tAcc@5  66.80 ( 68.04)\n",
      "Epoch: [26][2002/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.001)\tLoss 2.7627e+00 (2.6691e+00)\tAcc@1  38.28 ( 43.69)\tAcc@5  62.11 ( 67.89)\n",
      "Epoch: [26][3003/5005]\tTime  0.394 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.7588e+00 (2.6705e+00)\tAcc@1  40.62 ( 43.70)\tAcc@5  64.45 ( 67.87)\n",
      "Epoch: [26][4004/5005]\tTime  0.392 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.6885e+00 (2.6753e+00)\tAcc@1  42.19 ( 43.59)\tAcc@5  67.58 ( 67.80)\n",
      " * Acc@1 46.447 Acc@5 71.597\n",
      "************train_loss 2.677659947436292 val_acc 46.44733428955078*************\n",
      "Epoch: [27][   0/5005]\tTime  2.470 ( 2.470)\tData  2.326 ( 2.326)\tLoss 2.5414e+00 (2.5414e+00)\tAcc@1  48.05 ( 48.05)\tAcc@5  73.44 ( 73.44)\n",
      "Epoch: [27][1001/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.003)\tLoss 2.7855e+00 (2.6490e+00)\tAcc@1  41.41 ( 44.15)\tAcc@5  67.19 ( 68.25)\n",
      "Epoch: [27][2002/5005]\tTime  0.389 ( 0.393)\tData  0.000 ( 0.002)\tLoss 2.8284e+00 (2.6592e+00)\tAcc@1  41.02 ( 43.98)\tAcc@5  64.45 ( 68.08)\n",
      "Epoch: [27][3003/5005]\tTime  0.392 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.4838e+00 (2.6664e+00)\tAcc@1  46.88 ( 43.85)\tAcc@5  70.31 ( 67.95)\n",
      "Epoch: [27][4004/5005]\tTime  0.391 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.6512e+00 (2.6693e+00)\tAcc@1  46.88 ( 43.81)\tAcc@5  68.75 ( 67.92)\n",
      " * Acc@1 46.385 Acc@5 71.659\n",
      "************train_loss 2.671857310675241 val_acc 46.38541030883789*************\n",
      "Epoch: [28][   0/5005]\tTime  2.928 ( 2.928)\tData  2.789 ( 2.789)\tLoss 2.6016e+00 (2.6016e+00)\tAcc@1  44.53 ( 44.53)\tAcc@5  67.19 ( 67.19)\n",
      "Epoch: [28][1001/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.003)\tLoss 2.5896e+00 (2.6405e+00)\tAcc@1  44.53 ( 44.21)\tAcc@5  70.31 ( 68.30)\n",
      "Epoch: [28][2002/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.002)\tLoss 2.7304e+00 (2.6460e+00)\tAcc@1  40.62 ( 44.15)\tAcc@5  67.97 ( 68.21)\n",
      "Epoch: [28][3003/5005]\tTime  0.391 ( 0.393)\tData  0.000 ( 0.001)\tLoss 2.5492e+00 (2.6573e+00)\tAcc@1  43.36 ( 43.97)\tAcc@5  69.92 ( 68.05)\n",
      "Epoch: [28][4004/5005]\tTime  0.389 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.3918e+00 (2.6610e+00)\tAcc@1  47.66 ( 43.93)\tAcc@5  73.83 ( 68.01)\n",
      " * Acc@1 46.431 Acc@5 72.166\n",
      "************train_loss 2.6640674506748594 val_acc 46.43135452270508*************\n",
      "Epoch: [29][   0/5005]\tTime  2.647 ( 2.647)\tData  2.507 ( 2.507)\tLoss 2.5706e+00 (2.5706e+00)\tAcc@1  42.58 ( 42.58)\tAcc@5  69.14 ( 69.14)\n",
      "Epoch: [29][1001/5005]\tTime  0.390 ( 0.395)\tData  0.000 ( 0.003)\tLoss 2.7008e+00 (2.6383e+00)\tAcc@1  41.41 ( 44.37)\tAcc@5  66.80 ( 68.41)\n",
      "Epoch: [29][2002/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.002)\tLoss 2.9220e+00 (2.6484e+00)\tAcc@1  38.67 ( 44.12)\tAcc@5  66.80 ( 68.27)\n",
      "Epoch: [29][3003/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.001)\tLoss 2.4302e+00 (2.6578e+00)\tAcc@1  46.09 ( 43.97)\tAcc@5  71.09 ( 68.10)\n",
      "Epoch: [29][4004/5005]\tTime  0.393 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.6841e+00 (2.6617e+00)\tAcc@1  41.80 ( 43.91)\tAcc@5  66.41 ( 68.07)\n",
      " * Acc@1 44.999 Acc@5 70.614\n",
      "************train_loss 2.6631115813355346 val_acc 44.99910354614258*************\n",
      "Epoch: [30][   0/5005]\tTime  2.378 ( 2.378)\tData  2.227 ( 2.227)\tLoss 2.7203e+00 (2.7203e+00)\tAcc@1  43.36 ( 43.36)\tAcc@5  68.36 ( 68.36)\n",
      "Epoch: [30][1001/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.003)\tLoss 2.1519e+00 (2.1813e+00)\tAcc@1  53.12 ( 52.75)\tAcc@5  76.56 ( 75.19)\n",
      "Epoch: [30][2002/5005]\tTime  0.387 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.7135e+00 (2.1121e+00)\tAcc@1  58.98 ( 53.89)\tAcc@5  80.86 ( 76.24)\n",
      "Epoch: [30][3003/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.9476e+00 (2.0733e+00)\tAcc@1  55.86 ( 54.60)\tAcc@5  76.95 ( 76.81)\n",
      "Epoch: [30][4004/5005]\tTime  0.391 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.8211e+00 (2.0471e+00)\tAcc@1  59.38 ( 55.08)\tAcc@5  79.69 ( 77.20)\n",
      " * Acc@1 61.841 Acc@5 83.738\n",
      "************train_loss 2.026921493309242 val_acc 61.84055709838867*************\n",
      "Epoch: [31][   0/5005]\tTime  2.609 ( 2.609)\tData  2.464 ( 2.464)\tLoss 1.5656e+00 (1.5656e+00)\tAcc@1  58.59 ( 58.59)\tAcc@5  84.77 ( 84.77)\n",
      "Epoch: [31][1001/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.9813e+00 (1.8970e+00)\tAcc@1  59.38 ( 57.91)\tAcc@5  78.52 ( 79.33)\n",
      "Epoch: [31][2002/5005]\tTime  0.391 ( 0.394)\tData  0.000 ( 0.002)\tLoss 1.7061e+00 (1.8875e+00)\tAcc@1  62.89 ( 58.00)\tAcc@5  84.38 ( 79.49)\n",
      "Epoch: [31][3003/5005]\tTime  0.395 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.8761e+00 (1.8852e+00)\tAcc@1  56.64 ( 58.06)\tAcc@5  82.81 ( 79.51)\n",
      "Epoch: [31][4004/5005]\tTime  0.392 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.7995e+00 (1.8816e+00)\tAcc@1  58.98 ( 58.15)\tAcc@5  82.81 ( 79.57)\n",
      " * Acc@1 62.897 Acc@5 84.465\n",
      "************train_loss 1.8778742375550095 val_acc 62.89726638793945*************\n",
      "Epoch: [32][   0/5005]\tTime  2.406 ( 2.406)\tData  2.260 ( 2.260)\tLoss 2.1643e+00 (2.1643e+00)\tAcc@1  53.52 ( 53.52)\tAcc@5  73.44 ( 73.44)\n",
      "Epoch: [32][1001/5005]\tTime  0.389 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.9961e+00 (1.8134e+00)\tAcc@1  57.81 ( 59.37)\tAcc@5  78.52 ( 80.57)\n",
      "Epoch: [32][2002/5005]\tTime  0.391 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.8490e+00 (1.8169e+00)\tAcc@1  58.59 ( 59.30)\tAcc@5  79.69 ( 80.50)\n",
      "Epoch: [32][3003/5005]\tTime  0.388 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.5603e+00 (1.8206e+00)\tAcc@1  66.02 ( 59.25)\tAcc@5  82.03 ( 80.44)\n",
      "Epoch: [32][4004/5005]\tTime  0.391 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.5219e+00 (1.8192e+00)\tAcc@1  62.11 ( 59.31)\tAcc@5  88.67 ( 80.46)\n",
      " * Acc@1 63.666 Acc@5 84.819\n",
      "************train_loss 1.8173877413098987 val_acc 63.66632843017578*************\n",
      "Epoch: [33][   0/5005]\tTime  2.491 ( 2.491)\tData  2.347 ( 2.347)\tLoss 1.7626e+00 (1.7626e+00)\tAcc@1  60.55 ( 60.55)\tAcc@5  80.08 ( 80.08)\n",
      "Epoch: [33][1001/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.7560e+00 (1.7719e+00)\tAcc@1  62.50 ( 60.09)\tAcc@5  80.47 ( 81.15)\n",
      "Epoch: [33][2002/5005]\tTime  0.387 ( 0.394)\tData  0.000 ( 0.002)\tLoss 1.7392e+00 (1.7729e+00)\tAcc@1  62.11 ( 60.09)\tAcc@5  82.81 ( 81.12)\n",
      "Epoch: [33][3003/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.8036e+00 (1.7775e+00)\tAcc@1  62.11 ( 60.07)\tAcc@5  77.73 ( 81.05)\n",
      "Epoch: [33][4004/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.6271e+00 (1.7789e+00)\tAcc@1  58.20 ( 60.04)\tAcc@5  83.98 ( 81.00)\n",
      " * Acc@1 63.864 Acc@5 85.060\n",
      "************train_loss 1.7806325817203426 val_acc 63.86408996582031*************\n",
      "Epoch: [34][   0/5005]\tTime  2.078 ( 2.078)\tData  1.915 ( 1.915)\tLoss 1.6976e+00 (1.6976e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  83.98 ( 83.98)\n",
      "Epoch: [34][1001/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.002)\tLoss 1.4968e+00 (1.7423e+00)\tAcc@1  64.06 ( 60.63)\tAcc@5  85.94 ( 81.48)\n",
      "Epoch: [34][2002/5005]\tTime  0.391 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.9575e+00 (1.7485e+00)\tAcc@1  60.16 ( 60.58)\tAcc@5  78.91 ( 81.41)\n",
      "Epoch: [34][3003/5005]\tTime  0.388 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.5886e+00 (1.7503e+00)\tAcc@1  64.06 ( 60.53)\tAcc@5  81.64 ( 81.40)\n",
      "Epoch: [34][4004/5005]\tTime  0.382 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.6975e+00 (1.7520e+00)\tAcc@1  64.84 ( 60.52)\tAcc@5  80.08 ( 81.39)\n",
      " * Acc@1 64.104 Acc@5 85.342\n",
      "************train_loss 1.7517751291915253 val_acc 64.10379791259766*************\n",
      "Epoch: [35][   0/5005]\tTime  2.431 ( 2.431)\tData  2.287 ( 2.287)\tLoss 1.6097e+00 (1.6097e+00)\tAcc@1  65.23 ( 65.23)\tAcc@5  82.42 ( 82.42)\n",
      "Epoch: [35][1001/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.9333e+00 (1.7195e+00)\tAcc@1  57.81 ( 61.09)\tAcc@5  78.12 ( 81.77)\n",
      "Epoch: [35][2002/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.002)\tLoss 1.8031e+00 (1.7270e+00)\tAcc@1  61.72 ( 61.02)\tAcc@5  80.08 ( 81.70)\n",
      "Epoch: [35][3003/5005]\tTime  0.390 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.7000e+00 (1.7271e+00)\tAcc@1  61.72 ( 61.00)\tAcc@5  82.03 ( 81.71)\n",
      "Epoch: [35][4004/5005]\tTime  0.383 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.8653e+00 (1.7300e+00)\tAcc@1  60.16 ( 60.95)\tAcc@5  79.69 ( 81.68)\n",
      " * Acc@1 64.044 Acc@5 85.254\n",
      "************train_loss 1.7343557798183644 val_acc 64.04386901855469*************\n",
      "Epoch: [36][   0/5005]\tTime  2.493 ( 2.493)\tData  2.344 ( 2.344)\tLoss 1.7203e+00 (1.7203e+00)\tAcc@1  59.77 ( 59.77)\tAcc@5  82.81 ( 82.81)\n",
      "Epoch: [36][1001/5005]\tTime  0.394 ( 0.394)\tData  0.000 ( 0.003)\tLoss 1.7015e+00 (1.7030e+00)\tAcc@1  59.38 ( 61.56)\tAcc@5  83.59 ( 82.04)\n",
      "Epoch: [36][2002/5005]\tTime  0.389 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.8425e+00 (1.7121e+00)\tAcc@1  63.28 ( 61.36)\tAcc@5  79.69 ( 81.94)\n",
      "Epoch: [36][3003/5005]\tTime  0.389 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.6301e+00 (1.7157e+00)\tAcc@1  63.67 ( 61.24)\tAcc@5  83.20 ( 81.88)\n",
      "Epoch: [36][4004/5005]\tTime  0.378 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.8550e+00 (1.7196e+00)\tAcc@1  58.98 ( 61.15)\tAcc@5  81.25 ( 81.83)\n",
      " * Acc@1 64.258 Acc@5 85.370\n",
      "************train_loss 1.7228584499387714 val_acc 64.25760650634766*************\n",
      "Epoch: [37][   0/5005]\tTime  2.719 ( 2.719)\tData  2.576 ( 2.576)\tLoss 1.5667e+00 (1.5667e+00)\tAcc@1  64.45 ( 64.45)\tAcc@5  84.38 ( 84.38)\n",
      "Epoch: [37][1001/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.7327e+00 (1.6914e+00)\tAcc@1  57.81 ( 61.68)\tAcc@5  80.86 ( 82.24)\n",
      "Epoch: [37][2002/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.7978e+00 (1.7002e+00)\tAcc@1  61.33 ( 61.45)\tAcc@5  80.08 ( 82.11)\n",
      "Epoch: [37][3003/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.5681e+00 (1.7054e+00)\tAcc@1  66.80 ( 61.37)\tAcc@5  82.81 ( 82.03)\n",
      "Epoch: [37][4004/5005]\tTime  0.417 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.6120e+00 (1.7091e+00)\tAcc@1  60.16 ( 61.27)\tAcc@5  83.98 ( 82.00)\n",
      " * Acc@1 63.712 Acc@5 85.130\n",
      "************train_loss 1.7141876443401798 val_acc 63.71227264404297*************\n",
      "Epoch: [38][   0/5005]\tTime  2.455 ( 2.455)\tData  2.308 ( 2.308)\tLoss 1.9192e+00 (1.9192e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  80.08 ( 80.08)\n",
      "Epoch: [38][1001/5005]\tTime  0.390 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.6113e+00 (1.6888e+00)\tAcc@1  63.67 ( 61.69)\tAcc@5  82.03 ( 82.27)\n",
      "Epoch: [38][2002/5005]\tTime  0.390 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.5863e+00 (1.6932e+00)\tAcc@1  63.67 ( 61.61)\tAcc@5  83.59 ( 82.23)\n",
      "Epoch: [38][3003/5005]\tTime  0.389 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.7464e+00 (1.6992e+00)\tAcc@1  58.98 ( 61.48)\tAcc@5  81.25 ( 82.17)\n",
      "Epoch: [38][4004/5005]\tTime  0.392 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.6700e+00 (1.7024e+00)\tAcc@1  61.72 ( 61.41)\tAcc@5  80.86 ( 82.11)\n",
      " * Acc@1 63.940 Acc@5 85.150\n",
      "************train_loss 1.7044652818323491 val_acc 63.93999481201172*************\n",
      "Epoch: [39][   0/5005]\tTime  2.454 ( 2.454)\tData  2.308 ( 2.308)\tLoss 1.7167e+00 (1.7167e+00)\tAcc@1  57.81 ( 57.81)\tAcc@5  83.98 ( 83.98)\n",
      "Epoch: [39][1001/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.6201e+00 (1.6771e+00)\tAcc@1  63.28 ( 61.85)\tAcc@5  83.59 ( 82.42)\n",
      "Epoch: [39][2002/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.5371e+00 (1.6857e+00)\tAcc@1  61.33 ( 61.67)\tAcc@5  83.59 ( 82.33)\n",
      "Epoch: [39][3003/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.8024e+00 (1.6933e+00)\tAcc@1  55.86 ( 61.56)\tAcc@5  82.42 ( 82.20)\n",
      "Epoch: [39][4004/5005]\tTime  0.394 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.7807e+00 (1.6971e+00)\tAcc@1  57.42 ( 61.49)\tAcc@5  82.42 ( 82.17)\n",
      " * Acc@1 63.982 Acc@5 85.372\n",
      "************train_loss 1.703179334284185 val_acc 63.9819450378418*************\n",
      "Epoch: [40][   0/5005]\tTime  2.103 ( 2.103)\tData  1.959 ( 1.959)\tLoss 1.5671e+00 (1.5671e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  84.38 ( 84.38)\n",
      "Epoch: [40][1001/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.002)\tLoss 1.7508e+00 (1.6683e+00)\tAcc@1  57.42 ( 62.08)\tAcc@5  81.25 ( 82.61)\n",
      "Epoch: [40][2002/5005]\tTime  0.390 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.6090e+00 (1.6843e+00)\tAcc@1  63.67 ( 61.77)\tAcc@5  82.42 ( 82.35)\n",
      "Epoch: [40][3003/5005]\tTime  0.390 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.6715e+00 (1.6916e+00)\tAcc@1  62.50 ( 61.65)\tAcc@5  82.42 ( 82.26)\n",
      "Epoch: [40][4004/5005]\tTime  0.394 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.8385e+00 (1.6952e+00)\tAcc@1  57.81 ( 61.56)\tAcc@5  81.25 ( 82.21)\n",
      " * Acc@1 63.856 Acc@5 85.484\n",
      "************train_loss 1.7002989931420966 val_acc 63.85609817504883*************\n",
      "Epoch: [41][   0/5005]\tTime  2.223 ( 2.223)\tData  2.064 ( 2.064)\tLoss 1.7915e+00 (1.7915e+00)\tAcc@1  62.89 ( 62.89)\tAcc@5  80.86 ( 80.86)\n",
      "Epoch: [41][1001/5005]\tTime  0.391 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.6557e+00 (1.6704e+00)\tAcc@1  62.89 ( 62.04)\tAcc@5  82.42 ( 82.50)\n",
      "Epoch: [41][2002/5005]\tTime  0.382 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.5750e+00 (1.6818e+00)\tAcc@1  64.06 ( 61.85)\tAcc@5  85.55 ( 82.40)\n",
      "Epoch: [41][3003/5005]\tTime  0.389 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.5382e+00 (1.6868e+00)\tAcc@1  64.84 ( 61.75)\tAcc@5  85.94 ( 82.35)\n",
      "Epoch: [41][4004/5005]\tTime  0.390 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.6453e+00 (1.6925e+00)\tAcc@1  62.50 ( 61.63)\tAcc@5  82.03 ( 82.27)\n",
      " * Acc@1 63.225 Acc@5 84.844\n",
      "************train_loss 1.697348190044666 val_acc 63.22486877441406*************\n",
      "Epoch: [42][   0/5005]\tTime  2.218 ( 2.218)\tData  2.051 ( 2.051)\tLoss 1.6261e+00 (1.6261e+00)\tAcc@1  61.72 ( 61.72)\tAcc@5  83.98 ( 83.98)\n",
      "Epoch: [42][1001/5005]\tTime  0.397 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.7923e+00 (1.6690e+00)\tAcc@1  62.50 ( 62.07)\tAcc@5  78.52 ( 82.60)\n",
      "Epoch: [42][2002/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.6257e+00 (1.6795e+00)\tAcc@1  67.19 ( 61.92)\tAcc@5  81.64 ( 82.47)\n",
      "Epoch: [42][3003/5005]\tTime  0.397 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.6461e+00 (1.6890e+00)\tAcc@1  62.50 ( 61.72)\tAcc@5  84.77 ( 82.32)\n",
      "Epoch: [42][4004/5005]\tTime  0.388 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.7435e+00 (1.6935e+00)\tAcc@1  59.38 ( 61.56)\tAcc@5  80.86 ( 82.27)\n",
      " * Acc@1 63.071 Acc@5 84.689\n",
      "************train_loss 1.698609968355962 val_acc 63.0710563659668*************\n",
      "Epoch: [43][   0/5005]\tTime  2.706 ( 2.706)\tData  2.564 ( 2.564)\tLoss 1.5070e+00 (1.5070e+00)\tAcc@1  67.19 ( 67.19)\tAcc@5  85.94 ( 85.94)\n",
      "Epoch: [43][1001/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.8322e+00 (1.6708e+00)\tAcc@1  59.77 ( 61.93)\tAcc@5  81.25 ( 82.53)\n",
      "Epoch: [43][2002/5005]\tTime  0.394 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.8467e+00 (1.6797e+00)\tAcc@1  60.55 ( 61.79)\tAcc@5  81.64 ( 82.43)\n",
      "Epoch: [43][3003/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.6234e+00 (1.6886e+00)\tAcc@1  64.84 ( 61.65)\tAcc@5  82.42 ( 82.30)\n",
      "Epoch: [43][4004/5005]\tTime  0.389 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.4802e+00 (1.6913e+00)\tAcc@1  66.02 ( 61.59)\tAcc@5  83.59 ( 82.28)\n",
      " * Acc@1 63.215 Acc@5 84.693\n",
      "************train_loss 1.6980340793773487 val_acc 63.214881896972656*************\n",
      "Epoch: [44][   0/5005]\tTime  2.456 ( 2.456)\tData  2.312 ( 2.312)\tLoss 1.8989e+00 (1.8989e+00)\tAcc@1  59.77 ( 59.77)\tAcc@5  78.91 ( 78.91)\n",
      "Epoch: [44][1001/5005]\tTime  0.391 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.4912e+00 (1.6689e+00)\tAcc@1  64.06 ( 62.09)\tAcc@5  86.33 ( 82.60)\n",
      "Epoch: [44][2002/5005]\tTime  0.391 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.7800e+00 (1.6785e+00)\tAcc@1  60.16 ( 61.86)\tAcc@5  78.12 ( 82.45)\n",
      "Epoch: [44][3003/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.5853e+00 (1.6880e+00)\tAcc@1  63.28 ( 61.69)\tAcc@5  85.94 ( 82.32)\n",
      "Epoch: [44][4004/5005]\tTime  0.393 ( 0.392)\tData  0.000 ( 0.001)\tLoss 2.0730e+00 (1.6939e+00)\tAcc@1  57.81 ( 61.58)\tAcc@5  75.00 ( 82.25)\n",
      " * Acc@1 63.345 Acc@5 84.916\n",
      "************train_loss 1.6990703018752489 val_acc 63.344722747802734*************\n",
      "Epoch: [45][   0/5005]\tTime  2.493 ( 2.493)\tData  2.323 ( 2.323)\tLoss 1.6482e+00 (1.6482e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  82.42 ( 82.42)\n",
      "Epoch: [45][1001/5005]\tTime  0.391 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.6364e+00 (1.6702e+00)\tAcc@1  61.72 ( 62.06)\tAcc@5  84.77 ( 82.61)\n",
      "Epoch: [45][2002/5005]\tTime  0.394 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.6900e+00 (1.6807e+00)\tAcc@1  62.11 ( 61.84)\tAcc@5  82.03 ( 82.44)\n",
      "Epoch: [45][3003/5005]\tTime  0.391 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.9001e+00 (1.6881e+00)\tAcc@1  56.25 ( 61.69)\tAcc@5  79.30 ( 82.34)\n",
      "Epoch: [45][4004/5005]\tTime  0.394 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.6260e+00 (1.6934e+00)\tAcc@1  66.80 ( 61.57)\tAcc@5  82.42 ( 82.28)\n",
      " * Acc@1 64.280 Acc@5 85.512\n",
      "************train_loss 1.6972925914274706 val_acc 64.27957916259766*************\n",
      "Epoch: [46][   0/5005]\tTime  2.584 ( 2.584)\tData  2.441 ( 2.441)\tLoss 1.5913e+00 (1.5913e+00)\tAcc@1  61.33 ( 61.33)\tAcc@5  83.59 ( 83.59)\n",
      "Epoch: [46][1001/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.7869e+00 (1.6673e+00)\tAcc@1  58.98 ( 61.92)\tAcc@5  80.86 ( 82.64)\n",
      "Epoch: [46][2002/5005]\tTime  0.391 ( 0.394)\tData  0.000 ( 0.002)\tLoss 1.7366e+00 (1.6815e+00)\tAcc@1  58.59 ( 61.74)\tAcc@5  82.81 ( 82.43)\n",
      "Epoch: [46][3003/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.6309e+00 (1.6881e+00)\tAcc@1  59.38 ( 61.65)\tAcc@5  83.98 ( 82.33)\n",
      "Epoch: [46][4004/5005]\tTime  0.388 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.7541e+00 (1.6921e+00)\tAcc@1  61.72 ( 61.57)\tAcc@5  80.47 ( 82.28)\n",
      " * Acc@1 63.638 Acc@5 85.182\n",
      "************train_loss 1.6964885198152981 val_acc 63.638362884521484*************\n",
      "Epoch: [47][   0/5005]\tTime  2.125 ( 2.125)\tData  1.982 ( 1.982)\tLoss 1.4586e+00 (1.4586e+00)\tAcc@1  67.19 ( 67.19)\tAcc@5  84.77 ( 84.77)\n",
      "Epoch: [47][1001/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.6942e+00 (1.6678e+00)\tAcc@1  63.28 ( 62.05)\tAcc@5  82.42 ( 82.60)\n",
      "Epoch: [47][2002/5005]\tTime  0.390 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.7193e+00 (1.6747e+00)\tAcc@1  60.55 ( 61.94)\tAcc@5  80.86 ( 82.53)\n",
      "Epoch: [47][3003/5005]\tTime  0.388 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.6038e+00 (1.6820e+00)\tAcc@1  65.23 ( 61.80)\tAcc@5  82.42 ( 82.44)\n",
      "Epoch: [47][4004/5005]\tTime  0.390 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.9236e+00 (1.6884e+00)\tAcc@1  55.86 ( 61.67)\tAcc@5  83.59 ( 82.36)\n",
      " * Acc@1 64.006 Acc@5 85.298\n",
      "************train_loss 1.6921177054499532 val_acc 64.00591278076172*************\n",
      "Epoch: [48][   0/5005]\tTime  1.909 ( 1.909)\tData  1.742 ( 1.742)\tLoss 1.8322e+00 (1.8322e+00)\tAcc@1  59.77 ( 59.77)\tAcc@5  79.30 ( 79.30)\n",
      "Epoch: [48][1001/5005]\tTime  0.390 ( 0.395)\tData  0.000 ( 0.002)\tLoss 1.6489e+00 (1.6640e+00)\tAcc@1  62.89 ( 62.17)\tAcc@5  83.20 ( 82.69)\n",
      "Epoch: [48][2002/5005]\tTime  0.390 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.7079e+00 (1.6725e+00)\tAcc@1  62.50 ( 61.93)\tAcc@5  81.25 ( 82.58)\n",
      "Epoch: [48][3003/5005]\tTime  0.396 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.8165e+00 (1.6800e+00)\tAcc@1  58.59 ( 61.78)\tAcc@5  82.81 ( 82.51)\n",
      "Epoch: [48][4004/5005]\tTime  0.390 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.7798e+00 (1.6870e+00)\tAcc@1  59.38 ( 61.68)\tAcc@5  80.47 ( 82.39)\n",
      " * Acc@1 63.327 Acc@5 85.152\n",
      "************train_loss 1.6927740896617496 val_acc 63.326744079589844*************\n",
      "Epoch: [49][   0/5005]\tTime  2.441 ( 2.441)\tData  2.297 ( 2.297)\tLoss 1.7208e+00 (1.7208e+00)\tAcc@1  63.67 ( 63.67)\tAcc@5  80.86 ( 80.86)\n",
      "Epoch: [49][1001/5005]\tTime  0.391 ( 0.395)\tData  0.000 ( 0.003)\tLoss 1.5981e+00 (1.6655e+00)\tAcc@1  62.50 ( 62.12)\tAcc@5  87.11 ( 82.59)\n",
      "Epoch: [49][2002/5005]\tTime  0.395 ( 0.394)\tData  0.000 ( 0.002)\tLoss 1.7855e+00 (1.6726e+00)\tAcc@1  60.94 ( 61.93)\tAcc@5  80.08 ( 82.52)\n",
      "Epoch: [49][3003/5005]\tTime  0.389 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.4685e+00 (1.6796e+00)\tAcc@1  66.41 ( 61.77)\tAcc@5  86.33 ( 82.43)\n",
      "Epoch: [49][4004/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.7250e+00 (1.6869e+00)\tAcc@1  61.33 ( 61.63)\tAcc@5  83.20 ( 82.34)\n",
      " * Acc@1 63.754 Acc@5 85.368\n",
      "************train_loss 1.6915123316196057 val_acc 63.75422286987305*************\n"
     ]
    }
   ],
   "source": [
    "best_acc1 = 0\n",
    "acc1 = 0\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, args)\n",
    "\n",
    "    # train for one epoch\n",
    "    epoch_loss = train(train_loader, model, criterion, optimizer, epoch, args)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion, args)  \n",
    "    \n",
    "    train_loss.append(epoch_loss)\n",
    "    val_acc.append(acc1)\n",
    "    print('************train_loss {} val_acc {}*************'.format(epoch_loss, acc1))\n",
    "    \n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "#     if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
    "#             and args.rank % ngpus_per_node == 0):\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': args.arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6de6a0-a209-4b66-b52d-322f333aacae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48e851-7663-40f7-b4d8-417cc3498927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f37a2-26e6-4fef-97b3-feef20c1502c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b8e6c-a3c1-4467-b232-f02c9389b1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_retina",
   "language": "python",
   "name": "pytorch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
