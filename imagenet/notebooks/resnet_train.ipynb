{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ab6313-698a-46cc-9ab4-770b35f86211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "# import torchvision.models as models\n",
    "from resnet import *\n",
    "\n",
    "from main import *\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56508d67-5301-4e1e-9454-1fe41b5f3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args(args=[])\n",
    "# args = parser.parse_args()\n",
    "import easydict \n",
    "args = easydict.EasyDict({ \"batch-size\": 256, \n",
    "                          \"epochs\": 50, \n",
    "                          \"data\": 0, \n",
    "                          'arch':'resnet18',\n",
    "                          'lr':0.1,\n",
    "                         'momentum':0.9,\n",
    "                         'weight_decay':1e-4,\n",
    "                         'start_epoch':0,\n",
    "                         'gpu':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea598abf-d5fe-4f70-8cbf-78ad6038e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "ngpus_per_node = torch.cuda.device_count()\n",
    "print(ngpus_per_node)\n",
    "# device = 'cpu'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b5c549-7611-40a8-a5cd-54451c41e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# imagenet_embeding = np.load('../data/imagenet_embeding.npy')\n",
    "# imagenet_embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5e59a2-80d4-49f7-8998-7b78846b333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet_embeding = torch.tensor(imagenet_embeding)\n",
    "# imagenet_embeding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e9ebbf-4193-412c-b416-9faccf6aac25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model 'resnet18'\n"
     ]
    }
   ],
   "source": [
    "print(\"=> using pre-trained model '{}'\".format('resnet18'))\n",
    "# model = models.__dict__['resnet18'](pretrained=True)\n",
    "# model = models.resnet18(pretrained=False)\n",
    "model = resnet18(pretrained=False)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                            momentum=args.momentum,\n",
    "                            weight_decay=args.weight_decay)\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8910807d-3862-4446-a811-530857664cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict = model.state_dict() \n",
    "# for k in model_dict :\n",
    "#     if 'fc.weight' in k :\n",
    "#         print(model_dict[k].shape)\n",
    "#         print(model_dict[k])\n",
    "#         model_dict[k] = imagenet_embeding\n",
    "# #     if 'fc.bias' in k :\n",
    "# #         print(model_dict[k])        \n",
    "# #     print(model_dict[k].shape)\n",
    "# model.load_state_dict(model_dict)\n",
    "# model_dict = model.state_dict() \n",
    "# for k in model_dict :\n",
    "#     if 'fc.weight' in k :\n",
    "# #         print(model_dict[k].shape)\n",
    "#         print(model_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66335ead-2157-4014-8671-62dbab63a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fc.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77598382-abf2-459f-9457-f8bdd1cf8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters() :\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5493c8ea-4540-4067-8954-9e7a73cb10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6cd832a-5720-4925-9d50-36b16480132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "data_dir = '../ILSVRC/Data/CLS-LOC/'\n",
    "traindir = os.path.join(data_dir, 'train')\n",
    "valdir = os.path.join(data_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb79f3f-e8a8-4e0d-9a55-f7dbf5c08236",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d9c94c-b533-4147-96f0-57afbf42624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4460bdaa-261a-4858-a94c-c70aebd6fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "train_sampler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beece8cc-7ac9-4e30-8cca-0ca4adbd58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=256, shuffle=(train_sampler is None),\n",
    "    num_workers=8, pin_memory=True, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53171b34-4e2f-4b1b-ae3f-d8d2db58b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32, shuffle=False,\n",
    "    num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afed6352-ebb9-4fcf-ade1-12354b4e7755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 384, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(384, 768, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25092250-abbb-482c-bb3a-3c279e593859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][   0/5005]\tTime  2.572 ( 2.572)\tData  2.105 ( 2.105)\tLoss 7.0025e+00 (7.0025e+00)\tAcc@1   0.39 (  0.39)\tAcc@5   1.17 (  1.17)\n",
      "Epoch: [0][1001/5005]\tTime  0.394 ( 0.389)\tData  0.000 ( 0.003)\tLoss 5.5999e+00 (6.1512e+00)\tAcc@1   6.25 (  2.20)\tAcc@5  17.97 (  7.43)\n",
      "Epoch: [0][2002/5005]\tTime  0.397 ( 0.391)\tData  0.000 ( 0.002)\tLoss 4.8257e+00 (5.6935e+00)\tAcc@1  12.50 (  4.67)\tAcc@5  29.30 ( 13.55)\n",
      "Epoch: [0][3003/5005]\tTime  0.393 ( 0.392)\tData  0.000 ( 0.001)\tLoss 4.4676e+00 (5.3463e+00)\tAcc@1  14.06 (  7.35)\tAcc@5  33.59 ( 19.15)\n",
      "Epoch: [0][4004/5005]\tTime  0.387 ( 0.392)\tData  0.000 ( 0.001)\tLoss 4.1200e+00 (5.0676e+00)\tAcc@1  20.31 (  9.94)\tAcc@5  43.75 ( 23.95)\n",
      " * Acc@1 22.219 Acc@5 45.560\n",
      "************train_loss 4.839285239496908 val_acc 22.21889305114746*************\n",
      "Epoch: [1][   0/5005]\tTime  2.472 ( 2.472)\tData  2.325 ( 2.325)\tLoss 3.8029e+00 (3.8029e+00)\tAcc@1  21.09 ( 21.09)\tAcc@5  43.36 ( 43.36)\n",
      "Epoch: [1][1001/5005]\tTime  0.389 ( 0.396)\tData  0.000 ( 0.003)\tLoss 3.5720e+00 (3.6698e+00)\tAcc@1  28.12 ( 25.43)\tAcc@5  51.56 ( 49.03)\n",
      "Epoch: [1][2002/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.002)\tLoss 3.4235e+00 (3.5869e+00)\tAcc@1  25.39 ( 26.73)\tAcc@5  53.52 ( 50.64)\n",
      "Epoch: [1][3003/5005]\tTime  0.392 ( 0.394)\tData  0.000 ( 0.001)\tLoss 3.1639e+00 (3.5112e+00)\tAcc@1  33.98 ( 27.88)\tAcc@5  56.64 ( 52.06)\n",
      "Epoch: [1][4004/5005]\tTime  0.390 ( 0.394)\tData  0.000 ( 0.001)\tLoss 3.3992e+00 (3.4432e+00)\tAcc@1  30.08 ( 28.96)\tAcc@5  55.86 ( 53.35)\n",
      " * Acc@1 35.523 Acc@5 62.114\n",
      "************train_loss 3.386059929821994 val_acc 35.52266311645508*************\n",
      "Epoch: [2][   0/5005]\tTime  1.923 ( 1.923)\tData  1.778 ( 1.778)\tLoss 2.8693e+00 (2.8693e+00)\tAcc@1  40.23 ( 40.23)\tAcc@5  60.55 ( 60.55)\n",
      "Epoch: [2][1001/5005]\tTime  0.393 ( 0.396)\tData  0.000 ( 0.002)\tLoss 2.6743e+00 (3.0433e+00)\tAcc@1  45.70 ( 35.45)\tAcc@5  67.19 ( 60.59)\n",
      "Epoch: [2][2002/5005]\tTime  0.392 ( 0.394)\tData  0.000 ( 0.001)\tLoss 3.1595e+00 (3.0157e+00)\tAcc@1  30.86 ( 35.92)\tAcc@5  58.59 ( 61.11)\n",
      "Epoch: [2][3003/5005]\tTime  0.396 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.7803e+00 (2.9903e+00)\tAcc@1  41.02 ( 36.34)\tAcc@5  64.45 ( 61.57)\n",
      "Epoch: [2][4004/5005]\tTime  0.384 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.9643e+00 (2.9673e+00)\tAcc@1  38.67 ( 36.76)\tAcc@5  58.98 ( 61.97)\n",
      " * Acc@1 40.918 Acc@5 67.885\n",
      "************train_loss 2.9457330739462413 val_acc 40.91808319091797*************\n",
      "Epoch: [3][   0/5005]\tTime  2.005 ( 2.005)\tData  1.860 ( 1.860)\tLoss 3.0956e+00 (3.0956e+00)\tAcc@1  35.55 ( 35.55)\tAcc@5  56.64 ( 56.64)\n",
      "Epoch: [3][1001/5005]\tTime  0.394 ( 0.396)\tData  0.000 ( 0.002)\tLoss 2.5380e+00 (2.7860e+00)\tAcc@1  41.80 ( 39.94)\tAcc@5  68.75 ( 65.24)\n",
      "Epoch: [3][2002/5005]\tTime  0.391 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.7893e+00 (2.7766e+00)\tAcc@1  41.02 ( 40.08)\tAcc@5  61.33 ( 65.32)\n",
      "Epoch: [3][3003/5005]\tTime  0.395 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.7948e+00 (2.7675e+00)\tAcc@1  42.97 ( 40.25)\tAcc@5  64.84 ( 65.48)\n",
      "Epoch: [3][4004/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.7365e+00 (2.7575e+00)\tAcc@1  36.72 ( 40.44)\tAcc@5  66.41 ( 65.65)\n",
      " * Acc@1 43.329 Acc@5 69.178\n",
      "************train_loss 2.7466517454141623 val_acc 43.329139709472656*************\n",
      "Epoch: [4][   0/5005]\tTime  2.361 ( 2.361)\tData  2.200 ( 2.200)\tLoss 2.5842e+00 (2.5842e+00)\tAcc@1  44.14 ( 44.14)\tAcc@5  67.58 ( 67.58)\n",
      "Epoch: [4][1001/5005]\tTime  0.394 ( 0.396)\tData  0.000 ( 0.003)\tLoss 2.7810e+00 (2.6357e+00)\tAcc@1  41.80 ( 42.58)\tAcc@5  62.89 ( 67.77)\n",
      "Epoch: [4][2002/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.7166e+00 (2.6395e+00)\tAcc@1  39.84 ( 42.47)\tAcc@5  66.02 ( 67.70)\n",
      "Epoch: [4][3003/5005]\tTime  0.390 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.7056e+00 (2.6400e+00)\tAcc@1  40.62 ( 42.46)\tAcc@5  64.84 ( 67.70)\n",
      "Epoch: [4][4004/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.4945e+00 (2.6362e+00)\tAcc@1  46.09 ( 42.55)\tAcc@5  69.92 ( 67.75)\n",
      " * Acc@1 45.726 Acc@5 72.180\n",
      "************train_loss 2.632523614995844 val_acc 45.72621536254883*************\n",
      "Epoch: [5][   0/5005]\tTime  2.344 ( 2.344)\tData  2.197 ( 2.197)\tLoss 2.4771e+00 (2.4771e+00)\tAcc@1  46.48 ( 46.48)\tAcc@5  69.53 ( 69.53)\n",
      "Epoch: [5][1001/5005]\tTime  0.395 ( 0.396)\tData  0.000 ( 0.003)\tLoss 2.6194e+00 (2.5607e+00)\tAcc@1  44.14 ( 43.94)\tAcc@5  67.58 ( 68.90)\n",
      "Epoch: [5][2002/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.5816e+00 (2.5630e+00)\tAcc@1  42.19 ( 43.85)\tAcc@5  68.36 ( 68.89)\n",
      "Epoch: [5][3003/5005]\tTime  0.391 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.7564e+00 (2.5609e+00)\tAcc@1  40.23 ( 43.91)\tAcc@5  64.84 ( 68.93)\n",
      "Epoch: [5][4004/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.5860e+00 (2.5607e+00)\tAcc@1  44.53 ( 43.90)\tAcc@5  67.19 ( 68.93)\n",
      " * Acc@1 47.053 Acc@5 73.233\n",
      "************train_loss 2.557957032724813 val_acc 47.05259704589844*************\n",
      "Epoch: [6][   0/5005]\tTime  2.234 ( 2.234)\tData  2.087 ( 2.087)\tLoss 2.4352e+00 (2.4352e+00)\tAcc@1  46.48 ( 46.48)\tAcc@5  71.09 ( 71.09)\n",
      "Epoch: [6][1001/5005]\tTime  0.395 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.4986e+00 (2.4995e+00)\tAcc@1  41.02 ( 44.98)\tAcc@5  73.44 ( 70.06)\n",
      "Epoch: [6][2002/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.5213e+00 (2.5068e+00)\tAcc@1  46.88 ( 44.94)\tAcc@5  66.41 ( 69.93)\n",
      "Epoch: [6][3003/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.3792e+00 (2.5094e+00)\tAcc@1  46.09 ( 44.85)\tAcc@5  73.44 ( 69.85)\n",
      "Epoch: [6][4004/5005]\tTime  0.394 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.7045e+00 (2.5099e+00)\tAcc@1  40.23 ( 44.86)\tAcc@5  68.36 ( 69.84)\n",
      " * Acc@1 46.767 Acc@5 72.881\n",
      "************train_loss 2.510004217688973 val_acc 46.766944885253906*************\n",
      "Epoch: [7][   0/5005]\tTime  1.853 ( 1.853)\tData  1.712 ( 1.712)\tLoss 2.5914e+00 (2.5914e+00)\tAcc@1  44.14 ( 44.14)\tAcc@5  66.80 ( 66.80)\n",
      "Epoch: [7][1001/5005]\tTime  0.380 ( 0.396)\tData  0.000 ( 0.002)\tLoss 2.4191e+00 (2.4526e+00)\tAcc@1  48.05 ( 45.92)\tAcc@5  70.70 ( 70.74)\n",
      "Epoch: [7][2002/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.3964e+00 (2.4572e+00)\tAcc@1  44.53 ( 45.84)\tAcc@5  69.53 ( 70.61)\n",
      "Epoch: [7][3003/5005]\tTime  0.403 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.3393e+00 (2.4652e+00)\tAcc@1  43.36 ( 45.73)\tAcc@5  74.22 ( 70.48)\n",
      "Epoch: [7][4004/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.5567e+00 (2.4675e+00)\tAcc@1  45.31 ( 45.71)\tAcc@5  69.53 ( 70.46)\n",
      " * Acc@1 48.035 Acc@5 74.199\n",
      "************train_loss 2.47025181382567 val_acc 48.035400390625*************\n",
      "Epoch: [8][   0/5005]\tTime  2.247 ( 2.247)\tData  2.080 ( 2.080)\tLoss 2.5020e+00 (2.5020e+00)\tAcc@1  45.31 ( 45.31)\tAcc@5  64.45 ( 64.45)\n",
      "Epoch: [8][1001/5005]\tTime  0.392 ( 0.396)\tData  0.000 ( 0.003)\tLoss 2.3880e+00 (2.4289e+00)\tAcc@1  48.83 ( 46.46)\tAcc@5  69.92 ( 71.15)\n",
      "Epoch: [8][2002/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.6638e+00 (2.4348e+00)\tAcc@1  43.75 ( 46.34)\tAcc@5  66.80 ( 71.06)\n",
      "Epoch: [8][3003/5005]\tTime  0.395 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.3990e+00 (2.4403e+00)\tAcc@1  49.22 ( 46.20)\tAcc@5  71.88 ( 70.94)\n",
      "Epoch: [8][4004/5005]\tTime  0.400 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.2558e+00 (2.4408e+00)\tAcc@1  49.22 ( 46.19)\tAcc@5  76.17 ( 70.94)\n",
      " * Acc@1 49.196 Acc@5 75.132\n",
      "************train_loss 2.443824908330843 val_acc 49.19598388671875*************\n",
      "Epoch: [9][   0/5005]\tTime  2.563 ( 2.563)\tData  2.420 ( 2.420)\tLoss 2.3906e+00 (2.3906e+00)\tAcc@1  44.14 ( 44.14)\tAcc@5  71.48 ( 71.48)\n",
      "Epoch: [9][1001/5005]\tTime  0.394 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.2961e+00 (2.3979e+00)\tAcc@1  47.66 ( 46.99)\tAcc@5  75.00 ( 71.61)\n",
      "Epoch: [9][2002/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.7394e+00 (2.4118e+00)\tAcc@1  39.84 ( 46.69)\tAcc@5  66.41 ( 71.45)\n",
      "Epoch: [9][3003/5005]\tTime  0.394 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.3827e+00 (2.4161e+00)\tAcc@1  48.44 ( 46.63)\tAcc@5  72.27 ( 71.39)\n",
      "Epoch: [9][4004/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.5469e+00 (2.4179e+00)\tAcc@1  44.14 ( 46.57)\tAcc@5  73.05 ( 71.35)\n",
      " * Acc@1 48.295 Acc@5 74.445\n",
      "************train_loss 2.419759335860863 val_acc 48.295082092285156*************\n",
      "Epoch: [10][   0/5005]\tTime  2.331 ( 2.331)\tData  2.186 ( 2.186)\tLoss 2.3819e+00 (2.3819e+00)\tAcc@1  48.83 ( 48.83)\tAcc@5  73.44 ( 73.44)\n",
      "Epoch: [10][1001/5005]\tTime  0.396 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.3772e+00 (2.3787e+00)\tAcc@1  47.66 ( 47.17)\tAcc@5  73.83 ( 72.05)\n",
      "Epoch: [10][2002/5005]\tTime  0.397 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.4007e+00 (2.3938e+00)\tAcc@1  44.14 ( 46.95)\tAcc@5  69.92 ( 71.77)\n",
      "Epoch: [10][3003/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.5921e+00 (2.3973e+00)\tAcc@1  45.31 ( 46.90)\tAcc@5  68.75 ( 71.72)\n",
      "Epoch: [10][4004/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.4812e+00 (2.4007e+00)\tAcc@1  47.66 ( 46.90)\tAcc@5  67.97 ( 71.64)\n",
      " * Acc@1 48.097 Acc@5 74.219\n",
      "************train_loss 2.4018253583889027 val_acc 48.09732437133789*************\n",
      "Epoch: [11][   0/5005]\tTime  2.378 ( 2.378)\tData  2.218 ( 2.218)\tLoss 2.6054e+00 (2.6054e+00)\tAcc@1  45.31 ( 45.31)\tAcc@5  66.80 ( 66.80)\n",
      "Epoch: [11][1001/5005]\tTime  0.395 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.6751e+00 (2.3635e+00)\tAcc@1  39.06 ( 47.48)\tAcc@5  67.58 ( 72.19)\n",
      "Epoch: [11][2002/5005]\tTime  0.390 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.2481e+00 (2.3731e+00)\tAcc@1  48.83 ( 47.32)\tAcc@5  72.66 ( 72.04)\n",
      "Epoch: [11][3003/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.4420e+00 (2.3772e+00)\tAcc@1  46.88 ( 47.26)\tAcc@5  65.62 ( 71.98)\n",
      "Epoch: [11][4004/5005]\tTime  0.395 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.3579e+00 (2.3814e+00)\tAcc@1  47.27 ( 47.20)\tAcc@5  72.66 ( 71.94)\n",
      " * Acc@1 50.055 Acc@5 75.816\n",
      "************train_loss 2.383875805633766 val_acc 50.054935455322266*************\n",
      "Epoch: [12][   0/5005]\tTime  2.541 ( 2.541)\tData  2.398 ( 2.398)\tLoss 2.1304e+00 (2.1304e+00)\tAcc@1  53.52 ( 53.52)\tAcc@5  75.39 ( 75.39)\n",
      "Epoch: [12][1001/5005]\tTime  0.397 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.2389e+00 (2.3561e+00)\tAcc@1  50.39 ( 47.74)\tAcc@5  75.00 ( 72.39)\n",
      "Epoch: [12][2002/5005]\tTime  0.390 ( 0.396)\tData  0.000 ( 0.002)\tLoss 2.2841e+00 (2.3666e+00)\tAcc@1  46.09 ( 47.52)\tAcc@5  72.66 ( 72.13)\n",
      "Epoch: [12][3003/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.5629e+00 (2.3703e+00)\tAcc@1  43.75 ( 47.48)\tAcc@5  72.27 ( 72.10)\n",
      "Epoch: [12][4004/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.4978e+00 (2.3738e+00)\tAcc@1  46.48 ( 47.41)\tAcc@5  70.70 ( 72.04)\n",
      " * Acc@1 50.205 Acc@5 76.013\n",
      "************train_loss 2.3757572669963856 val_acc 50.20475387573242*************\n",
      "Epoch: [13][   0/5005]\tTime  2.365 ( 2.365)\tData  2.218 ( 2.218)\tLoss 2.4677e+00 (2.4677e+00)\tAcc@1  44.14 ( 44.14)\tAcc@5  69.92 ( 69.92)\n",
      "Epoch: [13][1001/5005]\tTime  0.395 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.1977e+00 (2.3404e+00)\tAcc@1  50.39 ( 47.95)\tAcc@5  74.22 ( 72.66)\n",
      "Epoch: [13][2002/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.5629e+00 (2.3467e+00)\tAcc@1  46.09 ( 47.82)\tAcc@5  71.09 ( 72.52)\n",
      "Epoch: [13][3003/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.3557e+00 (2.3543e+00)\tAcc@1  45.70 ( 47.73)\tAcc@5  74.61 ( 72.40)\n",
      "Epoch: [13][4004/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.4278e+00 (2.3585e+00)\tAcc@1  50.00 ( 47.66)\tAcc@5  69.53 ( 72.32)\n",
      " * Acc@1 49.434 Acc@5 75.428\n",
      "************train_loss 2.359844772322671 val_acc 49.433692932128906*************\n",
      "Epoch: [14][   0/5005]\tTime  2.014 ( 2.014)\tData  1.872 ( 1.872)\tLoss 2.3721e+00 (2.3721e+00)\tAcc@1  50.78 ( 50.78)\tAcc@5  73.83 ( 73.83)\n",
      "Epoch: [14][1001/5005]\tTime  0.395 ( 0.397)\tData  0.000 ( 0.002)\tLoss 2.2366e+00 (2.3320e+00)\tAcc@1  50.00 ( 48.15)\tAcc@5  74.22 ( 72.74)\n",
      "Epoch: [14][2002/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.4283e+00 (2.3408e+00)\tAcc@1  48.83 ( 48.05)\tAcc@5  72.66 ( 72.60)\n",
      "Epoch: [14][3003/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.2160e+00 (2.3454e+00)\tAcc@1  53.91 ( 47.96)\tAcc@5  72.27 ( 72.54)\n",
      "Epoch: [14][4004/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.4218e+00 (2.3481e+00)\tAcc@1  44.14 ( 47.91)\tAcc@5  70.70 ( 72.50)\n",
      " * Acc@1 49.168 Acc@5 75.148\n",
      "************train_loss 2.3519458376801574 val_acc 49.16801834106445*************\n",
      "Epoch: [15][   0/5005]\tTime  2.288 ( 2.288)\tData  2.139 ( 2.139)\tLoss 2.4229e+00 (2.4229e+00)\tAcc@1  51.17 ( 51.17)\tAcc@5  71.48 ( 71.48)\n",
      "Epoch: [15][1001/5005]\tTime  0.397 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.3987e+00 (2.3221e+00)\tAcc@1  46.48 ( 48.35)\tAcc@5  71.88 ( 72.86)\n",
      "Epoch: [15][2002/5005]\tTime  0.397 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.5027e+00 (2.3340e+00)\tAcc@1  42.58 ( 48.21)\tAcc@5  69.53 ( 72.71)\n",
      "Epoch: [15][3003/5005]\tTime  0.391 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.3021e+00 (2.3388e+00)\tAcc@1  50.00 ( 48.11)\tAcc@5  76.17 ( 72.60)\n",
      "Epoch: [15][4004/5005]\tTime  0.396 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.2887e+00 (2.3407e+00)\tAcc@1  49.22 ( 48.04)\tAcc@5  75.00 ( 72.58)\n",
      " * Acc@1 51.180 Acc@5 76.535\n",
      "************train_loss 2.345204544257927 val_acc 51.179561614990234*************\n",
      "Epoch: [16][   0/5005]\tTime  2.401 ( 2.401)\tData  2.261 ( 2.261)\tLoss 2.3163e+00 (2.3163e+00)\tAcc@1  50.00 ( 50.00)\tAcc@5  71.48 ( 71.48)\n",
      "Epoch: [16][1001/5005]\tTime  0.389 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.7141e+00 (2.3059e+00)\tAcc@1  42.19 ( 48.57)\tAcc@5  65.62 ( 73.22)\n",
      "Epoch: [16][2002/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.2271e+00 (2.3189e+00)\tAcc@1  51.56 ( 48.39)\tAcc@5  73.83 ( 72.96)\n",
      "Epoch: [16][3003/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.1433e+00 (2.3257e+00)\tAcc@1  48.05 ( 48.29)\tAcc@5  76.95 ( 72.86)\n",
      "Epoch: [16][4004/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.0848e+00 (2.3308e+00)\tAcc@1  49.22 ( 48.21)\tAcc@5  77.73 ( 72.74)\n",
      " * Acc@1 48.401 Acc@5 74.106\n",
      "************train_loss 2.3359204807481566 val_acc 48.40095138549805*************\n",
      "Epoch: [17][   0/5005]\tTime  2.402 ( 2.402)\tData  2.251 ( 2.251)\tLoss 2.5372e+00 (2.5372e+00)\tAcc@1  43.75 ( 43.75)\tAcc@5  69.92 ( 69.92)\n",
      "Epoch: [17][1001/5005]\tTime  0.395 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.3496e+00 (2.3057e+00)\tAcc@1  48.05 ( 48.61)\tAcc@5  71.88 ( 73.06)\n",
      "Epoch: [17][2002/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.5737e+00 (2.3159e+00)\tAcc@1  46.48 ( 48.47)\tAcc@5  64.84 ( 72.92)\n",
      "Epoch: [17][3003/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.2447e+00 (2.3217e+00)\tAcc@1  51.17 ( 48.38)\tAcc@5  74.22 ( 72.85)\n",
      "Epoch: [17][4004/5005]\tTime  0.398 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.5345e+00 (2.3270e+00)\tAcc@1  46.48 ( 48.26)\tAcc@5  69.92 ( 72.76)\n",
      " * Acc@1 51.437 Acc@5 77.104\n",
      "************train_loss 2.3290415674775513 val_acc 51.43724822998047*************\n",
      "Epoch: [18][   0/5005]\tTime  2.338 ( 2.338)\tData  2.182 ( 2.182)\tLoss 2.4640e+00 (2.4640e+00)\tAcc@1  46.88 ( 46.88)\tAcc@5  69.53 ( 69.53)\n",
      "Epoch: [18][1001/5005]\tTime  0.394 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.4535e+00 (2.3020e+00)\tAcc@1  44.53 ( 48.74)\tAcc@5  70.31 ( 73.21)\n",
      "Epoch: [18][2002/5005]\tTime  0.396 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.3835e+00 (2.3099e+00)\tAcc@1  46.48 ( 48.59)\tAcc@5  70.70 ( 73.07)\n",
      "Epoch: [18][3003/5005]\tTime  0.397 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.0008e+00 (2.3137e+00)\tAcc@1  53.91 ( 48.55)\tAcc@5  77.34 ( 73.01)\n",
      "Epoch: [18][4004/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.4293e+00 (2.3186e+00)\tAcc@1  43.75 ( 48.45)\tAcc@5  73.05 ( 72.94)\n",
      " * Acc@1 51.713 Acc@5 77.228\n",
      "************train_loss 2.3213658630788387 val_acc 51.712913513183594*************\n",
      "Epoch: [19][   0/5005]\tTime  2.276 ( 2.276)\tData  2.129 ( 2.129)\tLoss 2.4541e+00 (2.4541e+00)\tAcc@1  47.27 ( 47.27)\tAcc@5  69.92 ( 69.92)\n",
      "Epoch: [19][1001/5005]\tTime  0.391 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.0999e+00 (2.2916e+00)\tAcc@1  50.78 ( 48.90)\tAcc@5  73.05 ( 73.42)\n",
      "Epoch: [19][2002/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.2689e+00 (2.3037e+00)\tAcc@1  46.88 ( 48.74)\tAcc@5  74.22 ( 73.23)\n",
      "Epoch: [19][3003/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.3028e+00 (2.3094e+00)\tAcc@1  46.88 ( 48.60)\tAcc@5  74.22 ( 73.14)\n",
      "Epoch: [19][4004/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.2807e+00 (2.3133e+00)\tAcc@1  50.78 ( 48.55)\tAcc@5  75.78 ( 73.08)\n",
      " * Acc@1 50.902 Acc@5 76.768\n",
      "************train_loss 2.316078154214255 val_acc 50.90190124511719*************\n",
      "Epoch: [20][   0/5005]\tTime  2.402 ( 2.402)\tData  2.259 ( 2.259)\tLoss 2.1488e+00 (2.1488e+00)\tAcc@1  48.05 ( 48.05)\tAcc@5  76.17 ( 76.17)\n",
      "Epoch: [20][1001/5005]\tTime  0.391 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.4082e+00 (2.2882e+00)\tAcc@1  51.95 ( 48.93)\tAcc@5  73.83 ( 73.43)\n",
      "Epoch: [20][2002/5005]\tTime  0.391 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.5239e+00 (2.2956e+00)\tAcc@1  44.92 ( 48.84)\tAcc@5  69.92 ( 73.32)\n",
      "Epoch: [20][3003/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.7117e+00 (2.3038e+00)\tAcc@1  38.67 ( 48.67)\tAcc@5  65.23 ( 73.16)\n",
      "Epoch: [20][4004/5005]\tTime  0.388 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.3423e+00 (2.3088e+00)\tAcc@1  46.48 ( 48.63)\tAcc@5  73.44 ( 73.08)\n",
      " * Acc@1 51.086 Acc@5 76.595\n",
      "************train_loss 2.312812955729611 val_acc 51.08567810058594*************\n",
      "Epoch: [21][   0/5005]\tTime  2.264 ( 2.264)\tData  2.117 ( 2.117)\tLoss 2.2562e+00 (2.2562e+00)\tAcc@1  48.05 ( 48.05)\tAcc@5  74.22 ( 74.22)\n",
      "Epoch: [21][1001/5005]\tTime  0.391 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.1164e+00 (2.2801e+00)\tAcc@1  53.91 ( 49.18)\tAcc@5  77.34 ( 73.57)\n",
      "Epoch: [21][2002/5005]\tTime  0.391 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.2952e+00 (2.2910e+00)\tAcc@1  48.05 ( 48.88)\tAcc@5  73.05 ( 73.40)\n",
      "Epoch: [21][3003/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.9140e+00 (2.3001e+00)\tAcc@1  57.03 ( 48.79)\tAcc@5  79.69 ( 73.25)\n",
      "Epoch: [21][4004/5005]\tTime  0.394 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.3526e+00 (2.3051e+00)\tAcc@1  46.09 ( 48.69)\tAcc@5  72.66 ( 73.14)\n",
      " * Acc@1 51.893 Acc@5 77.503\n",
      "************train_loss 2.3066076380389555 val_acc 51.89269256591797*************\n",
      "Epoch: [22][   0/5005]\tTime  2.590 ( 2.590)\tData  2.447 ( 2.447)\tLoss 2.3035e+00 (2.3035e+00)\tAcc@1  49.61 ( 49.61)\tAcc@5  72.27 ( 72.27)\n",
      "Epoch: [22][1001/5005]\tTime  0.393 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.4578e+00 (2.2858e+00)\tAcc@1  46.88 ( 49.05)\tAcc@5  70.70 ( 73.39)\n",
      "Epoch: [22][2002/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.002)\tLoss 2.4054e+00 (2.2910e+00)\tAcc@1  49.22 ( 48.98)\tAcc@5  70.31 ( 73.35)\n",
      "Epoch: [22][3003/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.4472e+00 (2.2955e+00)\tAcc@1  46.09 ( 48.89)\tAcc@5  70.31 ( 73.31)\n",
      "Epoch: [22][4004/5005]\tTime  0.393 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.3611e+00 (2.3000e+00)\tAcc@1  46.09 ( 48.80)\tAcc@5  74.61 ( 73.26)\n",
      " * Acc@1 51.168 Acc@5 76.660\n",
      "************train_loss 2.304439107307068 val_acc 51.16757583618164*************\n",
      "Epoch: [23][   0/5005]\tTime  2.117 ( 2.117)\tData  1.970 ( 1.970)\tLoss 2.0271e+00 (2.0271e+00)\tAcc@1  50.00 ( 50.00)\tAcc@5  78.91 ( 78.91)\n",
      "Epoch: [23][1001/5005]\tTime  0.396 ( 0.397)\tData  0.000 ( 0.002)\tLoss 2.4010e+00 (2.2732e+00)\tAcc@1  49.22 ( 49.18)\tAcc@5  68.75 ( 73.73)\n",
      "Epoch: [23][2002/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.8895e+00 (2.2864e+00)\tAcc@1  54.30 ( 49.04)\tAcc@5  79.69 ( 73.50)\n",
      "Epoch: [23][3003/5005]\tTime  0.401 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.3221e+00 (2.2898e+00)\tAcc@1  46.48 ( 48.96)\tAcc@5  72.27 ( 73.42)\n",
      "Epoch: [23][4004/5005]\tTime  0.394 ( 0.394)\tData  0.000 ( 0.001)\tLoss 2.2358e+00 (2.2934e+00)\tAcc@1  50.39 ( 48.93)\tAcc@5  75.78 ( 73.35)\n",
      " * Acc@1 52.382 Acc@5 77.803\n",
      "************train_loss 2.2979849542175734 val_acc 52.38209533691406*************\n",
      "Epoch: [24][   0/5005]\tTime  2.720 ( 2.720)\tData  2.577 ( 2.577)\tLoss 2.2341e+00 (2.2341e+00)\tAcc@1  51.56 ( 51.56)\tAcc@5  74.61 ( 74.61)\n",
      "Epoch: [24][1001/5005]\tTime  0.395 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.3858e+00 (2.2700e+00)\tAcc@1  47.66 ( 49.32)\tAcc@5  69.92 ( 73.73)\n",
      "Epoch: [24][2002/5005]\tTime  0.395 ( 0.396)\tData  0.000 ( 0.002)\tLoss 2.3537e+00 (2.2811e+00)\tAcc@1  46.88 ( 49.06)\tAcc@5  73.05 ( 73.55)\n",
      "Epoch: [24][3003/5005]\tTime  0.385 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.1668e+00 (2.2863e+00)\tAcc@1  53.12 ( 49.00)\tAcc@5  76.56 ( 73.45)\n",
      "Epoch: [24][4004/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.1503e+00 (2.2922e+00)\tAcc@1  51.56 ( 48.95)\tAcc@5  77.34 ( 73.37)\n",
      " * Acc@1 52.049 Acc@5 77.717\n",
      "************train_loss 2.296735398633616 val_acc 52.04850387573242*************\n",
      "Epoch: [25][   0/5005]\tTime  2.407 ( 2.407)\tData  2.258 ( 2.258)\tLoss 2.3122e+00 (2.3122e+00)\tAcc@1  48.05 ( 48.05)\tAcc@5  72.66 ( 72.66)\n",
      "Epoch: [25][1001/5005]\tTime  0.391 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.1966e+00 (2.2637e+00)\tAcc@1  50.39 ( 49.39)\tAcc@5  76.56 ( 73.81)\n",
      "Epoch: [25][2002/5005]\tTime  0.390 ( 0.396)\tData  0.000 ( 0.002)\tLoss 2.5417e+00 (2.2706e+00)\tAcc@1  44.14 ( 49.30)\tAcc@5  70.31 ( 73.73)\n",
      "Epoch: [25][3003/5005]\tTime  0.386 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.3601e+00 (2.2837e+00)\tAcc@1  49.61 ( 49.05)\tAcc@5  69.92 ( 73.51)\n",
      "Epoch: [25][4004/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.4314e+00 (2.2861e+00)\tAcc@1  48.83 ( 49.02)\tAcc@5  73.44 ( 73.48)\n",
      " * Acc@1 48.962 Acc@5 75.296\n",
      "************train_loss 2.2900922422285204 val_acc 48.9622688293457*************\n",
      "Epoch: [26][   0/5005]\tTime  2.380 ( 2.380)\tData  2.234 ( 2.234)\tLoss 2.3008e+00 (2.3008e+00)\tAcc@1  49.61 ( 49.61)\tAcc@5  71.88 ( 71.88)\n",
      "Epoch: [26][1001/5005]\tTime  0.397 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.3260e+00 (2.2618e+00)\tAcc@1  48.44 ( 49.47)\tAcc@5  71.48 ( 73.89)\n",
      "Epoch: [26][2002/5005]\tTime  0.398 ( 0.396)\tData  0.000 ( 0.002)\tLoss 2.5274e+00 (2.2746e+00)\tAcc@1  43.75 ( 49.19)\tAcc@5  69.92 ( 73.64)\n",
      "Epoch: [26][3003/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.2740e+00 (2.2802e+00)\tAcc@1  45.31 ( 49.13)\tAcc@5  74.61 ( 73.57)\n",
      "Epoch: [26][4004/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.4797e+00 (2.2853e+00)\tAcc@1  42.58 ( 49.06)\tAcc@5  72.66 ( 73.47)\n",
      " * Acc@1 49.789 Acc@5 75.584\n",
      "************train_loss 2.2890312796229724 val_acc 49.78926086425781*************\n",
      "Epoch: [27][   0/5005]\tTime  2.403 ( 2.403)\tData  2.255 ( 2.255)\tLoss 2.3074e+00 (2.3074e+00)\tAcc@1  47.66 ( 47.66)\tAcc@5  74.61 ( 74.61)\n",
      "Epoch: [27][1001/5005]\tTime  0.394 ( 0.397)\tData  0.000 ( 0.003)\tLoss 2.1132e+00 (2.2611e+00)\tAcc@1  52.34 ( 49.53)\tAcc@5  78.52 ( 73.87)\n",
      "Epoch: [27][2002/5005]\tTime  0.394 ( 0.396)\tData  0.000 ( 0.002)\tLoss 2.4987e+00 (2.2717e+00)\tAcc@1  44.92 ( 49.31)\tAcc@5  71.48 ( 73.72)\n",
      "Epoch: [27][3003/5005]\tTime  0.391 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.2412e+00 (2.2794e+00)\tAcc@1  50.78 ( 49.17)\tAcc@5  75.39 ( 73.58)\n",
      "Epoch: [27][4004/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.5763e+00 (2.2815e+00)\tAcc@1  46.09 ( 49.10)\tAcc@5  70.31 ( 73.54)\n",
      " * Acc@1 50.610 Acc@5 76.589\n",
      "************train_loss 2.2858470994156677 val_acc 50.61025619506836*************\n",
      "Epoch: [28][   0/5005]\tTime  1.972 ( 1.972)\tData  1.806 ( 1.806)\tLoss 2.4270e+00 (2.4270e+00)\tAcc@1  47.66 ( 47.66)\tAcc@5  72.27 ( 72.27)\n",
      "Epoch: [28][1001/5005]\tTime  0.392 ( 0.397)\tData  0.000 ( 0.002)\tLoss 2.2988e+00 (2.2521e+00)\tAcc@1  52.73 ( 49.69)\tAcc@5  74.61 ( 73.96)\n",
      "Epoch: [28][2002/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.2455e+00 (2.2667e+00)\tAcc@1  48.83 ( 49.37)\tAcc@5  75.39 ( 73.76)\n",
      "Epoch: [28][3003/5005]\tTime  0.412 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.0499e+00 (2.2731e+00)\tAcc@1  51.56 ( 49.28)\tAcc@5  78.12 ( 73.68)\n",
      "Epoch: [28][4004/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.2285e+00 (2.2797e+00)\tAcc@1  52.34 ( 49.16)\tAcc@5  72.66 ( 73.60)\n",
      " * Acc@1 51.445 Acc@5 77.080\n",
      "************train_loss 2.282747367426351 val_acc 51.44524002075195*************\n",
      "Epoch: [29][   0/5005]\tTime  2.402 ( 2.402)\tData  2.251 ( 2.251)\tLoss 2.3859e+00 (2.3859e+00)\tAcc@1  48.05 ( 48.05)\tAcc@5  71.88 ( 71.88)\n",
      "Epoch: [29][1001/5005]\tTime  0.396 ( 0.398)\tData  0.000 ( 0.003)\tLoss 2.1401e+00 (2.2527e+00)\tAcc@1  51.56 ( 49.54)\tAcc@5  76.56 ( 73.87)\n",
      "Epoch: [29][2002/5005]\tTime  0.394 ( 0.396)\tData  0.000 ( 0.002)\tLoss 2.0689e+00 (2.2660e+00)\tAcc@1  55.47 ( 49.35)\tAcc@5  78.52 ( 73.75)\n",
      "Epoch: [29][3003/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.5345e+00 (2.2689e+00)\tAcc@1  48.05 ( 49.36)\tAcc@5  68.75 ( 73.74)\n",
      "Epoch: [29][4004/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 2.1272e+00 (2.2730e+00)\tAcc@1  52.73 ( 49.28)\tAcc@5  76.17 ( 73.66)\n",
      " * Acc@1 52.428 Acc@5 77.565\n",
      "************train_loss 2.2788641344179044 val_acc 52.42803955078125*************\n",
      "Epoch: [30][   0/5005]\tTime  2.515 ( 2.515)\tData  2.372 ( 2.372)\tLoss 2.1067e+00 (2.1067e+00)\tAcc@1  55.47 ( 55.47)\tAcc@5  75.78 ( 75.78)\n",
      "Epoch: [30][1001/5005]\tTime  0.396 ( 0.398)\tData  0.000 ( 0.003)\tLoss 1.8055e+00 (1.8839e+00)\tAcc@1  61.72 ( 57.16)\tAcc@5  79.30 ( 79.48)\n",
      "Epoch: [30][2002/5005]\tTime  0.395 ( 0.396)\tData  0.000 ( 0.002)\tLoss 1.6772e+00 (1.8259e+00)\tAcc@1  61.72 ( 58.41)\tAcc@5  81.64 ( 80.31)\n",
      "Epoch: [30][3003/5005]\tTime  0.397 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.7588e+00 (1.7959e+00)\tAcc@1  60.16 ( 59.01)\tAcc@5  82.42 ( 80.72)\n",
      "Epoch: [30][4004/5005]\tTime  0.396 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.7730e+00 (1.7739e+00)\tAcc@1  58.98 ( 59.44)\tAcc@5  82.03 ( 81.03)\n",
      " * Acc@1 65.334 Acc@5 86.588\n",
      "************train_loss 1.75726615263151 val_acc 65.33429718017578*************\n",
      "Epoch: [31][   0/5005]\tTime  2.338 ( 2.338)\tData  2.192 ( 2.192)\tLoss 1.5088e+00 (1.5088e+00)\tAcc@1  63.28 ( 63.28)\tAcc@5  83.98 ( 83.98)\n",
      "Epoch: [31][1001/5005]\tTime  0.396 ( 0.397)\tData  0.000 ( 0.003)\tLoss 1.4946e+00 (1.6510e+00)\tAcc@1  65.62 ( 61.87)\tAcc@5  85.16 ( 82.81)\n",
      "Epoch: [31][2002/5005]\tTime  0.394 ( 0.396)\tData  0.000 ( 0.002)\tLoss 1.8499e+00 (1.6452e+00)\tAcc@1  59.77 ( 61.97)\tAcc@5  82.03 ( 82.85)\n",
      "Epoch: [31][3003/5005]\tTime  0.388 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.5604e+00 (1.6428e+00)\tAcc@1  67.19 ( 62.07)\tAcc@5  83.98 ( 82.86)\n",
      "Epoch: [31][4004/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.7344e+00 (1.6394e+00)\tAcc@1  63.28 ( 62.14)\tAcc@5  81.25 ( 82.92)\n",
      " * Acc@1 66.505 Acc@5 87.353\n",
      "************train_loss 1.6353163902814334 val_acc 66.50486755371094*************\n",
      "Epoch: [32][   0/5005]\tTime  2.041 ( 2.041)\tData  1.897 ( 1.897)\tLoss 1.7189e+00 (1.7189e+00)\tAcc@1  60.55 ( 60.55)\tAcc@5  82.42 ( 82.42)\n",
      "Epoch: [32][1001/5005]\tTime  0.403 ( 0.397)\tData  0.000 ( 0.002)\tLoss 1.5861e+00 (1.5787e+00)\tAcc@1  65.23 ( 63.26)\tAcc@5  84.38 ( 83.72)\n",
      "Epoch: [32][2002/5005]\tTime  0.397 ( 0.396)\tData  0.000 ( 0.001)\tLoss 1.5594e+00 (1.5822e+00)\tAcc@1  62.89 ( 63.24)\tAcc@5  83.59 ( 83.67)\n",
      "Epoch: [32][3003/5005]\tTime  0.396 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.3819e+00 (1.5824e+00)\tAcc@1  64.84 ( 63.22)\tAcc@5  83.98 ( 83.67)\n",
      "Epoch: [32][4004/5005]\tTime  0.396 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.3931e+00 (1.5822e+00)\tAcc@1  69.53 ( 63.27)\tAcc@5  85.94 ( 83.68)\n",
      " * Acc@1 66.818 Acc@5 87.471\n",
      "************train_loss 1.582223160950454 val_acc 66.8184814453125*************\n",
      "Epoch: [33][   0/5005]\tTime  2.568 ( 2.568)\tData  2.423 ( 2.423)\tLoss 1.7243e+00 (1.7243e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  80.47 ( 80.47)\n",
      "Epoch: [33][1001/5005]\tTime  0.393 ( 0.398)\tData  0.000 ( 0.003)\tLoss 1.5870e+00 (1.5506e+00)\tAcc@1  63.28 ( 63.85)\tAcc@5  83.20 ( 84.10)\n",
      "Epoch: [33][2002/5005]\tTime  0.393 ( 0.396)\tData  0.000 ( 0.002)\tLoss 1.4888e+00 (1.5498e+00)\tAcc@1  66.80 ( 63.90)\tAcc@5  83.20 ( 84.10)\n",
      "Epoch: [33][3003/5005]\tTime  0.395 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.6454e+00 (1.5505e+00)\tAcc@1  59.77 ( 63.91)\tAcc@5  83.59 ( 84.13)\n",
      "Epoch: [33][4004/5005]\tTime  0.389 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.4838e+00 (1.5507e+00)\tAcc@1  65.62 ( 63.89)\tAcc@5  83.98 ( 84.13)\n",
      " * Acc@1 66.956 Acc@5 87.559\n",
      "************train_loss 1.551111738403122 val_acc 66.95631408691406*************\n",
      "Epoch: [34][   0/5005]\tTime  2.327 ( 2.327)\tData  2.176 ( 2.176)\tLoss 1.6948e+00 (1.6948e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  82.03 ( 82.03)\n",
      "Epoch: [34][1001/5005]\tTime  0.389 ( 0.398)\tData  0.000 ( 0.003)\tLoss 1.5248e+00 (1.5205e+00)\tAcc@1  64.84 ( 64.47)\tAcc@5  85.16 ( 84.59)\n",
      "Epoch: [34][2002/5005]\tTime  0.395 ( 0.396)\tData  0.000 ( 0.002)\tLoss 1.4041e+00 (1.5251e+00)\tAcc@1  64.84 ( 64.38)\tAcc@5  87.89 ( 84.48)\n",
      "Epoch: [34][3003/5005]\tTime  0.396 ( 0.396)\tData  0.000 ( 0.001)\tLoss 1.4589e+00 (1.5267e+00)\tAcc@1  65.62 ( 64.39)\tAcc@5  87.11 ( 84.46)\n",
      "Epoch: [34][4004/5005]\tTime  0.396 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.5026e+00 (1.5265e+00)\tAcc@1  64.45 ( 64.36)\tAcc@5  84.77 ( 84.48)\n",
      " * Acc@1 67.270 Acc@5 87.805\n",
      "************train_loss 1.5280283092380642 val_acc 67.26993560791016*************\n",
      "Epoch: [35][   0/5005]\tTime  2.504 ( 2.504)\tData  2.361 ( 2.361)\tLoss 1.4176e+00 (1.4176e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  84.38 ( 84.38)\n",
      "Epoch: [35][1001/5005]\tTime  0.395 ( 0.398)\tData  0.000 ( 0.003)\tLoss 1.4641e+00 (1.5043e+00)\tAcc@1  65.23 ( 64.86)\tAcc@5  87.11 ( 84.78)\n",
      "Epoch: [35][2002/5005]\tTime  0.395 ( 0.396)\tData  0.000 ( 0.002)\tLoss 1.4731e+00 (1.5066e+00)\tAcc@1  62.89 ( 64.80)\tAcc@5  85.55 ( 84.74)\n",
      "Epoch: [35][3003/5005]\tTime  0.397 ( 0.396)\tData  0.000 ( 0.001)\tLoss 1.4479e+00 (1.5080e+00)\tAcc@1  65.62 ( 64.76)\tAcc@5  84.38 ( 84.71)\n",
      "Epoch: [35][4004/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.4427e+00 (1.5103e+00)\tAcc@1  67.58 ( 64.72)\tAcc@5  84.77 ( 84.68)\n",
      " * Acc@1 67.326 Acc@5 87.943\n",
      "************train_loss 1.5127318112881152 val_acc 67.32586669921875*************\n",
      "Epoch: [36][   0/5005]\tTime  2.105 ( 2.105)\tData  1.964 ( 1.964)\tLoss 1.6629e+00 (1.6629e+00)\tAcc@1  59.77 ( 59.77)\tAcc@5  81.64 ( 81.64)\n",
      "Epoch: [36][1001/5005]\tTime  0.398 ( 0.397)\tData  0.000 ( 0.003)\tLoss 1.8587e+00 (1.4880e+00)\tAcc@1  61.72 ( 65.17)\tAcc@5  79.30 ( 85.01)\n",
      "Epoch: [36][2002/5005]\tTime  0.396 ( 0.396)\tData  0.000 ( 0.002)\tLoss 1.3320e+00 (1.4894e+00)\tAcc@1  68.36 ( 65.14)\tAcc@5  88.67 ( 84.99)\n",
      "Epoch: [36][3003/5005]\tTime  0.398 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.2719e+00 (1.4949e+00)\tAcc@1  65.62 ( 64.99)\tAcc@5  89.06 ( 84.88)\n",
      "Epoch: [36][4004/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.5584e+00 (1.4991e+00)\tAcc@1  62.11 ( 64.91)\tAcc@5  83.98 ( 84.82)\n",
      " * Acc@1 67.000 Acc@5 87.683\n",
      "************train_loss 1.4999035184080902 val_acc 67.00025939941406*************\n",
      "Epoch: [37][   0/5005]\tTime  1.881 ( 1.881)\tData  1.727 ( 1.727)\tLoss 1.5634e+00 (1.5634e+00)\tAcc@1  62.50 ( 62.50)\tAcc@5  84.77 ( 84.77)\n",
      "Epoch: [37][1001/5005]\tTime  0.389 ( 0.397)\tData  0.000 ( 0.002)\tLoss 1.6472e+00 (1.4787e+00)\tAcc@1  63.28 ( 65.35)\tAcc@5  80.47 ( 85.10)\n",
      "Epoch: [37][2002/5005]\tTime  0.394 ( 0.396)\tData  0.000 ( 0.001)\tLoss 1.3424e+00 (1.4826e+00)\tAcc@1  65.23 ( 65.20)\tAcc@5  88.28 ( 85.04)\n",
      "Epoch: [37][3003/5005]\tTime  0.397 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.6166e+00 (1.4883e+00)\tAcc@1  63.67 ( 65.10)\tAcc@5  84.38 ( 84.98)\n",
      "Epoch: [37][4004/5005]\tTime  0.394 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.4457e+00 (1.4913e+00)\tAcc@1  63.67 ( 65.02)\tAcc@5  86.72 ( 84.94)\n",
      " * Acc@1 67.554 Acc@5 87.913\n",
      "************train_loss 1.4957781901964535 val_acc 67.5535888671875*************\n",
      "Epoch: [38][   0/5005]\tTime  2.600 ( 2.600)\tData  2.458 ( 2.458)\tLoss 1.5308e+00 (1.5308e+00)\tAcc@1  65.23 ( 65.23)\tAcc@5  84.77 ( 84.77)\n",
      "Epoch: [38][1001/5005]\tTime  0.419 ( 0.398)\tData  0.000 ( 0.003)\tLoss 1.4291e+00 (1.4621e+00)\tAcc@1  67.97 ( 65.60)\tAcc@5  86.72 ( 85.31)\n",
      "Epoch: [38][2002/5005]\tTime  0.395 ( 0.396)\tData  0.000 ( 0.002)\tLoss 1.4049e+00 (1.4718e+00)\tAcc@1  68.75 ( 65.43)\tAcc@5  84.77 ( 85.27)\n",
      "Epoch: [38][3003/5005]\tTime  0.392 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.6074e+00 (1.4787e+00)\tAcc@1  64.45 ( 65.27)\tAcc@5  83.59 ( 85.15)\n",
      "Epoch: [38][4004/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.4827e+00 (1.4832e+00)\tAcc@1  67.97 ( 65.18)\tAcc@5  83.98 ( 85.09)\n",
      " * Acc@1 67.268 Acc@5 87.847\n",
      "************train_loss 1.4864424133634233 val_acc 67.26793670654297*************\n",
      "Epoch: [39][   0/5005]\tTime  2.063 ( 2.063)\tData  1.913 ( 1.913)\tLoss 1.4765e+00 (1.4765e+00)\tAcc@1  64.45 ( 64.45)\tAcc@5  84.38 ( 84.38)\n",
      "Epoch: [39][1001/5005]\tTime  0.397 ( 0.398)\tData  0.000 ( 0.002)\tLoss 1.4891e+00 (1.4581e+00)\tAcc@1  64.84 ( 65.64)\tAcc@5  83.20 ( 85.46)\n",
      "Epoch: [39][2002/5005]\tTime  0.396 ( 0.396)\tData  0.000 ( 0.001)\tLoss 1.4700e+00 (1.4682e+00)\tAcc@1  66.80 ( 65.48)\tAcc@5  85.16 ( 85.33)\n",
      "Epoch: [39][3003/5005]\tTime  0.389 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.4579e+00 (1.4748e+00)\tAcc@1  66.80 ( 65.38)\tAcc@5  83.98 ( 85.18)\n",
      "Epoch: [39][4004/5005]\tTime  0.380 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.4508e+00 (1.4797e+00)\tAcc@1  65.23 ( 65.27)\tAcc@5  86.33 ( 85.11)\n",
      " * Acc@1 67.458 Acc@5 87.853\n",
      "************train_loss 1.4848209728608717 val_acc 67.45770263671875*************\n",
      "Epoch: [40][   0/5005]\tTime  2.483 ( 2.483)\tData  2.339 ( 2.339)\tLoss 1.4142e+00 (1.4142e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  85.16 ( 85.16)\n",
      "Epoch: [40][1001/5005]\tTime  0.394 ( 0.398)\tData  0.000 ( 0.003)\tLoss 1.4069e+00 (1.4577e+00)\tAcc@1  64.84 ( 65.70)\tAcc@5  89.06 ( 85.41)\n",
      "Epoch: [40][2002/5005]\tTime  0.394 ( 0.396)\tData  0.000 ( 0.002)\tLoss 1.3585e+00 (1.4650e+00)\tAcc@1  68.75 ( 65.56)\tAcc@5  87.89 ( 85.31)\n",
      "Epoch: [40][3003/5005]\tTime  0.389 ( 0.396)\tData  0.000 ( 0.001)\tLoss 1.5618e+00 (1.4716e+00)\tAcc@1  66.41 ( 65.43)\tAcc@5  83.98 ( 85.25)\n",
      "Epoch: [40][4004/5005]\tTime  0.393 ( 0.395)\tData  0.000 ( 0.001)\tLoss 1.5279e+00 (1.4760e+00)\tAcc@1  62.50 ( 65.36)\tAcc@5  82.42 ( 85.18)\n",
      " * Acc@1 67.494 Acc@5 87.787\n",
      "************train_loss 1.482591616904938 val_acc 67.49365997314453*************\n",
      "Epoch: [41][   0/5005]\tTime  2.511 ( 2.511)\tData  2.367 ( 2.367)\tLoss 1.3447e+00 (1.3447e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  87.11 ( 87.11)\n",
      "Epoch: [41][1001/5005]\tTime  0.392 ( 0.396)\tData  0.000 ( 0.003)\tLoss 1.1462e+00 (1.4607e+00)\tAcc@1  68.36 ( 65.74)\tAcc@5  89.45 ( 85.38)\n",
      "Epoch: [41][2002/5005]\tTime  0.391 ( 0.394)\tData  0.000 ( 0.002)\tLoss 1.5809e+00 (1.4658e+00)\tAcc@1  64.45 ( 65.59)\tAcc@5  82.03 ( 85.33)\n",
      "Epoch: [41][3003/5005]\tTime  0.394 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.4779e+00 (1.4728e+00)\tAcc@1  63.67 ( 65.44)\tAcc@5  85.94 ( 85.23)\n",
      "Epoch: [41][4004/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.001)\tLoss 1.4625e+00 (1.4796e+00)\tAcc@1  63.28 ( 65.30)\tAcc@5  84.77 ( 85.12)\n",
      " * Acc@1 66.932 Acc@5 87.661\n",
      "************train_loss 1.4826880001521612 val_acc 66.93234252929688*************\n",
      "Epoch: [42][   0/5005]\tTime  2.254 ( 2.254)\tData  2.103 ( 2.103)\tLoss 1.4196e+00 (1.4196e+00)\tAcc@1  68.36 ( 68.36)\tAcc@5  86.33 ( 86.33)\n",
      "Epoch: [42][1001/5005]\tTime  0.392 ( 0.394)\tData  0.000 ( 0.003)\tLoss 1.4092e+00 (1.4512e+00)\tAcc@1  66.41 ( 65.89)\tAcc@5  87.50 ( 85.58)\n",
      "Epoch: [42][2002/5005]\tTime  0.389 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.5390e+00 (1.4624e+00)\tAcc@1  64.06 ( 65.68)\tAcc@5  84.38 ( 85.40)\n",
      "Epoch: [42][3003/5005]\tTime  0.388 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.4965e+00 (1.4698e+00)\tAcc@1  62.11 ( 65.49)\tAcc@5  87.11 ( 85.29)\n",
      "Epoch: [42][4004/5005]\tTime  0.379 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.5271e+00 (1.4761e+00)\tAcc@1  65.23 ( 65.36)\tAcc@5  83.98 ( 85.20)\n",
      " * Acc@1 67.050 Acc@5 87.681\n",
      "************train_loss 1.4801727326599867 val_acc 67.05020141601562*************\n",
      "Epoch: [43][   0/5005]\tTime  2.336 ( 2.336)\tData  2.192 ( 2.192)\tLoss 1.2854e+00 (1.2854e+00)\tAcc@1  71.48 ( 71.48)\tAcc@5  87.11 ( 87.11)\n",
      "Epoch: [43][1001/5005]\tTime  0.393 ( 0.393)\tData  0.000 ( 0.003)\tLoss 1.2596e+00 (1.4554e+00)\tAcc@1  71.09 ( 65.76)\tAcc@5  88.28 ( 85.51)\n",
      "Epoch: [43][2002/5005]\tTime  0.390 ( 0.391)\tData  0.000 ( 0.002)\tLoss 1.5411e+00 (1.4665e+00)\tAcc@1  64.45 ( 65.55)\tAcc@5  82.81 ( 85.32)\n",
      "Epoch: [43][3003/5005]\tTime  0.391 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.5413e+00 (1.4712e+00)\tAcc@1  64.45 ( 65.46)\tAcc@5  84.77 ( 85.26)\n",
      "Epoch: [43][4004/5005]\tTime  0.391 ( 0.390)\tData  0.000 ( 0.001)\tLoss 1.3864e+00 (1.4756e+00)\tAcc@1  66.41 ( 65.37)\tAcc@5  84.77 ( 85.21)\n",
      " * Acc@1 67.030 Acc@5 87.871\n",
      "************train_loss 1.4814431327920812 val_acc 67.03022766113281*************\n",
      "Epoch: [44][   0/5005]\tTime  2.429 ( 2.429)\tData  2.282 ( 2.282)\tLoss 1.5100e+00 (1.5100e+00)\tAcc@1  61.72 ( 61.72)\tAcc@5  87.50 ( 87.50)\n",
      "Epoch: [44][1001/5005]\tTime  0.386 ( 0.393)\tData  0.000 ( 0.003)\tLoss 1.4907e+00 (1.4537e+00)\tAcc@1  66.41 ( 65.76)\tAcc@5  85.16 ( 85.53)\n",
      "Epoch: [44][2002/5005]\tTime  0.391 ( 0.392)\tData  0.000 ( 0.002)\tLoss 1.4558e+00 (1.4679e+00)\tAcc@1  65.23 ( 65.48)\tAcc@5  82.42 ( 85.32)\n",
      "Epoch: [44][3003/5005]\tTime  0.385 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.4775e+00 (1.4747e+00)\tAcc@1  64.06 ( 65.36)\tAcc@5  84.38 ( 85.21)\n",
      "Epoch: [44][4004/5005]\tTime  0.390 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.6915e+00 (1.4780e+00)\tAcc@1  60.94 ( 65.26)\tAcc@5  82.42 ( 85.16)\n",
      " * Acc@1 66.926 Acc@5 87.565\n",
      "************train_loss 1.4805309575754446 val_acc 66.92635345458984*************\n",
      "Epoch: [45][   0/5005]\tTime  2.309 ( 2.309)\tData  2.164 ( 2.164)\tLoss 1.3946e+00 (1.3946e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  85.16 ( 85.16)\n",
      "Epoch: [45][1001/5005]\tTime  0.385 ( 0.393)\tData  0.000 ( 0.003)\tLoss 1.5751e+00 (1.4475e+00)\tAcc@1  62.11 ( 65.80)\tAcc@5  84.77 ( 85.60)\n",
      "Epoch: [45][2002/5005]\tTime  0.391 ( 0.391)\tData  0.000 ( 0.002)\tLoss 1.4563e+00 (1.4614e+00)\tAcc@1  66.80 ( 65.56)\tAcc@5  87.50 ( 85.42)\n",
      "Epoch: [45][3003/5005]\tTime  0.390 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.5210e+00 (1.4696e+00)\tAcc@1  63.28 ( 65.41)\tAcc@5  85.16 ( 85.29)\n",
      "Epoch: [45][4004/5005]\tTime  0.392 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.4979e+00 (1.4747e+00)\tAcc@1  65.62 ( 65.26)\tAcc@5  82.42 ( 85.21)\n",
      " * Acc@1 67.170 Acc@5 87.899\n",
      "************train_loss 1.4792201629766337 val_acc 67.17005157470703*************\n",
      "Epoch: [46][   0/5005]\tTime  2.193 ( 2.193)\tData  2.047 ( 2.047)\tLoss 1.2808e+00 (1.2808e+00)\tAcc@1  70.31 ( 70.31)\tAcc@5  87.89 ( 87.89)\n",
      "Epoch: [46][1001/5005]\tTime  0.388 ( 0.393)\tData  0.000 ( 0.002)\tLoss 1.6115e+00 (1.4555e+00)\tAcc@1  65.23 ( 65.78)\tAcc@5  82.03 ( 85.40)\n",
      "Epoch: [46][2002/5005]\tTime  0.391 ( 0.392)\tData  0.000 ( 0.001)\tLoss 1.6163e+00 (1.4597e+00)\tAcc@1  60.55 ( 65.66)\tAcc@5  84.38 ( 85.40)\n",
      "Epoch: [46][3003/5005]\tTime  0.391 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.5635e+00 (1.4692e+00)\tAcc@1  65.62 ( 65.43)\tAcc@5  83.98 ( 85.27)\n",
      "Epoch: [46][4004/5005]\tTime  0.391 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.4089e+00 (1.4749e+00)\tAcc@1  69.53 ( 65.27)\tAcc@5  85.94 ( 85.21)\n",
      " * Acc@1 66.816 Acc@5 87.451\n",
      "************train_loss 1.4798792760569852 val_acc 66.81649017333984*************\n",
      "Epoch: [47][   0/5005]\tTime  2.251 ( 2.251)\tData  2.080 ( 2.080)\tLoss 1.6070e+00 (1.6070e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  81.25 ( 81.25)\n",
      "Epoch: [47][1001/5005]\tTime  0.390 ( 0.393)\tData  0.000 ( 0.003)\tLoss 1.5475e+00 (1.4533e+00)\tAcc@1  66.02 ( 65.78)\tAcc@5  82.42 ( 85.52)\n",
      "Epoch: [47][2002/5005]\tTime  0.392 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.6725e+00 (1.4565e+00)\tAcc@1  59.38 ( 65.71)\tAcc@5  83.20 ( 85.46)\n",
      "Epoch: [47][3003/5005]\tTime  0.391 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.4141e+00 (1.4626e+00)\tAcc@1  61.72 ( 65.54)\tAcc@5  87.89 ( 85.39)\n",
      "Epoch: [47][4004/5005]\tTime  0.388 ( 0.390)\tData  0.000 ( 0.001)\tLoss 1.5877e+00 (1.4701e+00)\tAcc@1  61.72 ( 65.42)\tAcc@5  82.81 ( 85.28)\n",
      " * Acc@1 66.547 Acc@5 87.355\n",
      "************train_loss 1.4758774459897936 val_acc 66.54681396484375*************\n",
      "Epoch: [48][   0/5005]\tTime  2.674 ( 2.674)\tData  2.533 ( 2.533)\tLoss 1.4866e+00 (1.4866e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  84.38 ( 84.38)\n",
      "Epoch: [48][1001/5005]\tTime  0.384 ( 0.393)\tData  0.000 ( 0.003)\tLoss 1.6601e+00 (1.4481e+00)\tAcc@1  62.50 ( 65.92)\tAcc@5  81.25 ( 85.55)\n",
      "Epoch: [48][2002/5005]\tTime  0.378 ( 0.392)\tData  0.000 ( 0.002)\tLoss 1.4882e+00 (1.4553e+00)\tAcc@1  64.84 ( 65.71)\tAcc@5  87.11 ( 85.48)\n",
      "Epoch: [48][3003/5005]\tTime  0.383 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.6032e+00 (1.4633e+00)\tAcc@1  65.62 ( 65.54)\tAcc@5  86.33 ( 85.39)\n",
      "Epoch: [48][4004/5005]\tTime  0.392 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.6698e+00 (1.4688e+00)\tAcc@1  62.11 ( 65.45)\tAcc@5  83.20 ( 85.32)\n",
      " * Acc@1 66.305 Acc@5 87.272\n",
      "************train_loss 1.4730523430503213 val_acc 66.30510711669922*************\n",
      "Epoch: [49][   0/5005]\tTime  2.326 ( 2.326)\tData  2.169 ( 2.169)\tLoss 1.6586e+00 (1.6586e+00)\tAcc@1  64.45 ( 64.45)\tAcc@5  80.08 ( 80.08)\n",
      "Epoch: [49][1001/5005]\tTime  0.392 ( 0.393)\tData  0.000 ( 0.003)\tLoss 1.4205e+00 (1.4422e+00)\tAcc@1  66.02 ( 65.91)\tAcc@5  85.94 ( 85.68)\n",
      "Epoch: [49][2002/5005]\tTime  0.388 ( 0.392)\tData  0.000 ( 0.002)\tLoss 1.2332e+00 (1.4591e+00)\tAcc@1  68.36 ( 65.67)\tAcc@5  88.67 ( 85.44)\n",
      "Epoch: [49][3003/5005]\tTime  0.387 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.3502e+00 (1.4636e+00)\tAcc@1  66.80 ( 65.58)\tAcc@5  87.50 ( 85.38)\n",
      "Epoch: [49][4004/5005]\tTime  0.387 ( 0.391)\tData  0.000 ( 0.001)\tLoss 1.4111e+00 (1.4697e+00)\tAcc@1  64.84 ( 65.45)\tAcc@5  85.16 ( 85.31)\n",
      " * Acc@1 66.801 Acc@5 87.491\n",
      "************train_loss 1.473753260899257 val_acc 66.80050659179688*************\n"
     ]
    }
   ],
   "source": [
    "best_acc1 = 0\n",
    "acc1 = 0\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, args)\n",
    "\n",
    "    # train for one epoch\n",
    "    epoch_loss = train(train_loader, model, criterion, optimizer, epoch, args)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion, args)  \n",
    "    \n",
    "    train_loss.append(epoch_loss)\n",
    "    val_acc.append(acc1)\n",
    "    print('************train_loss {} val_acc {}*************'.format(epoch_loss, acc1))\n",
    "    \n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "#     if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
    "#             and args.rank % ngpus_per_node == 0):\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': args.arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6de6a0-a209-4b66-b52d-322f333aacae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48e851-7663-40f7-b4d8-417cc3498927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_retina",
   "language": "python",
   "name": "pytorch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
