{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d18bb34-2519-4990-ae6c-e96ff5c0df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "  # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "  # PyTorch to build a computational graph, allowing automatic computation of\n",
    "  # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "  # don't need to keep references to intermediate values.\n",
    "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  \n",
    "  # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "  # is a Python number giving its value.\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "#   print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass. This call will compute the\n",
    "  # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "  # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "  # of the loss with respect to w1 and w2 respectively.\n",
    "  loss.backward()\n",
    "\n",
    "  # Update weights using gradient descent. For this step we just want to mutate\n",
    "  # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "  # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "  # to prevent PyTorch from building a computational graph for the updates\n",
    "  with torch.no_grad():\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cdd0802-5511-4566-9923-c3ddf947868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 14655.916015625\n",
      "1 11893.91796875\n",
      "2 9919.2412109375\n",
      "3 8474.69921875\n",
      "4 7393.9248046875\n",
      "5 6570.96435546875\n",
      "6 5933.69091796875\n",
      "7 5433.6708984375\n",
      "8 5036.76611328125\n",
      "9 4716.91845703125\n",
      "10 4455.935546875\n",
      "11 4240.8330078125\n",
      "12 4061.10107421875\n",
      "13 3909.06689453125\n",
      "14 3779.141357421875\n",
      "15 3666.52734375\n",
      "16 3567.622314453125\n",
      "17 3479.63623046875\n",
      "18 3400.296630859375\n",
      "19 3328.031005859375\n",
      "20 3261.56201171875\n",
      "21 3199.891357421875\n",
      "22 3142.35546875\n",
      "23 3088.216796875\n",
      "24 3037.021484375\n",
      "25 2988.2412109375\n",
      "26 2941.449462890625\n",
      "27 2896.419677734375\n",
      "28 2852.98681640625\n",
      "29 2811.013916015625\n",
      "30 2770.353759765625\n",
      "31 2730.810546875\n",
      "32 2692.35400390625\n",
      "33 2654.8671875\n",
      "34 2618.2802734375\n",
      "35 2582.535888671875\n",
      "36 2547.587646484375\n",
      "37 2513.3720703125\n",
      "38 2479.840576171875\n",
      "39 2446.984130859375\n",
      "40 2414.774169921875\n",
      "41 2383.1787109375\n",
      "42 2352.1865234375\n",
      "43 2321.796875\n",
      "44 2291.985595703125\n",
      "45 2262.767578125\n",
      "46 2234.098876953125\n",
      "47 2205.936767578125\n",
      "48 2178.24560546875\n",
      "49 2151.052734375\n",
      "**********************\n",
      "tensor([[ -4.5655,  -9.3042,  -9.9084,  ...,   0.1892,  -7.1532,   6.6757],\n",
      "        [ -4.5332,  -9.4718,  -7.1545,  ...,  -4.7891,  -0.9172,  -5.6986],\n",
      "        [ -5.4886,  -9.1206,  -9.4121,  ...,   0.1109,  -3.2704,  -0.0945],\n",
      "        ...,\n",
      "        [ -1.9943, -10.0116,  -1.3875,  ...,  -0.7201,   5.6202,  -1.4596],\n",
      "        [  6.0329,  -2.4623,  -6.6978,  ...,  -5.0711,   1.1185,   5.2572],\n",
      "        [ -0.0577,   8.5000,  -6.9416,  ...,  -6.0911,  -6.9273,   0.1960]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e5a8eee6c4a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh1_relu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mh2_relu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2_relu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Code in file tensor/two_layer_net_tensor.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H1, H2, D_out = 64, 1000, 500, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H1, device=device)/10\n",
    "w2 = torch.randn(H1, H2, device=device)/10\n",
    "w3 = torch.randn(H2, D_out, device=device)/10\n",
    "\n",
    "w4 = w1.clone()\n",
    "w5 = w2.clone()\n",
    "w6 = w3.clone()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(50):\n",
    "    # Forward pass: compute predicted y\n",
    "    h1 = x.mm(w1)\n",
    "    h1_relu = h1.clamp(min=0)\n",
    "    h2 = h1_relu.mm(w2)\n",
    "    h2_relu = h2.clamp(min=0)\n",
    "    y_pred = h2_relu.mm(w3)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w3 = h2_relu.t().mm(grad_y_pred)\n",
    "    grad_h2_relu = grad_y_pred.mm(w3.t())\n",
    "\n",
    "    grad_h2 = grad_h2_relu.clone()\n",
    "    grad_h2[h2 < 0] = 0\n",
    "\n",
    "    grad_w2 = h1_relu.t().mm(grad_h2)\n",
    "    grad_h1_relu = grad_h2.mm(w2.t())\n",
    "    grad_h1 = grad_h1_relu.clone()\n",
    "    grad_h1[h1 < 0] = 0\n",
    "    \n",
    "    grad_w1 = x.t().mm(grad_h1)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    w3 -= learning_rate * grad_w3\n",
    "    \n",
    "    \n",
    "learning_rate = 1e-6\n",
    "print('**********************')\n",
    "for t in range(50):\n",
    "    # Forward pass: compute predicted y\n",
    "    h1 = x.mm(w4)\n",
    "    h1_relu = h1.clamp(min=0)\n",
    "    h2 = h1_relu.mm(w5)\n",
    "    h2_relu = h2.clamp(min=0)\n",
    "    y_pred = h2_relu.mm(w6)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w3 = h2_relu.t().mm(grad_y_pred)\n",
    "    grad_h2_relu = grad_y_pred.mm(w6.t()) * ((torch.sigmoid(h2)))\n",
    "\n",
    "    grad_h2 = grad_h2_relu.clone()\n",
    "    grad_h2[h2 < 0] = 0\n",
    "\n",
    "    grad_w2 = h1_relu.t().mm(grad_h2)\n",
    "    grad_h1_relu = grad_h2.mm(w5.t()) * ((torch.sigmoid(h1)))\n",
    "    grad_h1 = grad_h1_relu.clone()\n",
    "    grad_h1[h1 < 0] = 0\n",
    "    \n",
    "    grad_w1 = x.t().mm(grad_h1)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w4 -= learning_rate * grad_w1\n",
    "    w5 -= learning_rate * grad_w2\n",
    "    w6 -= learning_rate * grad_w3    \n",
    "    \n",
    "# # Randomly initialize weights\n",
    "# w3 = torch.randn(D_in, H, device=device)\n",
    "# print(w3.shape)\n",
    "# w4 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "# print('***************************')\n",
    "# learning_rate = 1e-6\n",
    "# for t in range(100):\n",
    "#     # Forward pass: compute predicted y\n",
    "#     h = x.mm(w3)\n",
    "#     h_relu = h.clamp(min=0)\n",
    "# #     print(h_relu.shape)\n",
    "#     y_pred = (h_relu.mm(w4))\n",
    "    \n",
    "#     # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "#     # of shape (); we can get its value as a Python number with loss.item().\n",
    "#     loss = (y_pred - y).pow(2).sum()\n",
    "#     print(t, loss.item())\n",
    "\n",
    "#     # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "#     grad_y_pred = 2.0 * (y_pred - y)\n",
    "# #     print(grad_y_pred.shape)\n",
    "#     grad_w4 = h_relu.t().mm(grad_y_pred)\n",
    "# #     print(grad_w4.shape)\n",
    "#     grad_h_relu = grad_y_pred.mm(w4.t()) * ((torch.sigmoid(h_relu) - 0.5)/0.5)\n",
    "# #     print(grad_h_relu.shape)\n",
    "#     grad_h = grad_h_relu.clone()\n",
    "#     grad_h[h < 0] = 0\n",
    "#     grad_w3 = x.t().mm(grad_h)\n",
    "\n",
    "#     # Update weights using gradient descent\n",
    "#     w3 -= learning_rate * grad_w3\n",
    "#     w4 -= learning_rate * grad_w4    \n",
    "# #     adfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d4dd926-6943-4caa-91f7-503dffd0460a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1525, 0.0384]])\n",
      "tensor([[2.1525, 0.0384]])\n",
      "tensor([[2.1525, 0.0384]])\n",
      "tensor([[1.0763, 0.0192]])\n"
     ]
    }
   ],
   "source": [
    "a1  = torch.randn(1, 2, device=device)\n",
    "print(a1)\n",
    "a2 = a1.clone()\n",
    "print(a2)\n",
    "a1 = a1/2\n",
    "print(a2)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f1174-a637-409f-8e93-0b8de8624b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_retina",
   "language": "python",
   "name": "pytorch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
