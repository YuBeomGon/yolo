{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d18bb34-2519-4990-ae6c-e96ff5c0df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "  # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "  # PyTorch to build a computational graph, allowing automatic computation of\n",
    "  # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "  # don't need to keep references to intermediate values.\n",
    "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  \n",
    "  # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "  # is a Python number giving its value.\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "#   print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass. This call will compute the\n",
    "  # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "  # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "  # of the loss with respect to w1 and w2 respectively.\n",
    "  loss.backward()\n",
    "\n",
    "  # Update weights using gradient descent. For this step we just want to mutate\n",
    "  # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "  # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "  # to prevent PyTorch from building a computational graph for the updates\n",
    "  with torch.no_grad():\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cdd0802-5511-4566-9923-c3ddf947868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9318.6474609375\n",
      "1 8259.5078125\n",
      "2 7439.4580078125\n",
      "3 6790.89697265625\n",
      "4 6273.5458984375\n",
      "5 5854.9814453125\n",
      "6 5510.1240234375\n",
      "7 5222.58935546875\n",
      "8 4980.73779296875\n",
      "9 4774.38818359375\n",
      "10 4596.90087890625\n",
      "11 4442.1513671875\n",
      "12 4305.943359375\n",
      "13 4185.001953125\n",
      "14 4076.435302734375\n",
      "15 3978.0517578125\n",
      "16 3888.22509765625\n",
      "17 3805.607666015625\n",
      "18 3728.853271484375\n",
      "19 3657.208984375\n",
      "20 3590.08203125\n",
      "21 3526.553955078125\n",
      "22 3466.2119140625\n",
      "23 3408.604248046875\n",
      "24 3353.420654296875\n",
      "25 3300.495849609375\n",
      "26 3249.51123046875\n",
      "27 3200.123779296875\n",
      "28 3152.37158203125\n",
      "29 3106.15869140625\n",
      "30 3061.234619140625\n",
      "31 3017.554443359375\n",
      "32 2975.029296875\n",
      "33 2933.555419921875\n",
      "34 2893.18212890625\n",
      "35 2853.8427734375\n",
      "36 2815.459228515625\n",
      "37 2777.9833984375\n",
      "38 2741.32080078125\n",
      "39 2705.40087890625\n",
      "40 2670.201904296875\n",
      "41 2635.651123046875\n",
      "42 2601.817138671875\n",
      "43 2568.68310546875\n",
      "44 2536.198486328125\n",
      "45 2504.3642578125\n",
      "46 2473.161376953125\n",
      "47 2442.521484375\n",
      "48 2412.3984375\n",
      "49 2382.8134765625\n",
      "**********************\n",
      "0 9318.6474609375\n",
      "1 8331.767578125\n",
      "2 7556.17431640625\n",
      "3 6936.78125\n",
      "4 6436.3212890625\n",
      "5 6027.298828125\n",
      "6 5688.63134765625\n",
      "7 5405.52734375\n",
      "8 5166.611328125\n",
      "9 4962.7958984375\n",
      "10 4787.3623046875\n",
      "11 4634.77978515625\n",
      "12 4500.916015625\n",
      "13 4382.2998046875\n",
      "14 4276.33447265625\n",
      "15 4180.798828125\n",
      "16 4094.017333984375\n",
      "17 4014.486572265625\n",
      "18 3941.08984375\n",
      "19 3872.92919921875\n",
      "20 3809.218505859375\n",
      "21 3749.234375\n",
      "22 3692.5625\n",
      "23 3638.67822265625\n",
      "24 3587.29443359375\n",
      "25 3538.1728515625\n",
      "26 3490.9609375\n",
      "27 3445.46875\n",
      "28 3401.5224609375\n",
      "29 3358.962646484375\n",
      "30 3317.68505859375\n",
      "31 3277.53857421875\n",
      "32 3238.43896484375\n",
      "33 3200.25146484375\n",
      "34 3162.95458984375\n",
      "35 3126.52587890625\n",
      "36 3090.898193359375\n",
      "37 3056.00732421875\n",
      "38 3021.84423828125\n",
      "39 2988.3720703125\n",
      "40 2955.571533203125\n",
      "41 2923.426025390625\n",
      "42 2891.947265625\n",
      "43 2861.06591796875\n",
      "44 2830.745849609375\n",
      "45 2800.96728515625\n",
      "46 2771.7109375\n",
      "47 2742.95361328125\n",
      "48 2714.654296875\n",
      "49 2686.83251953125\n"
     ]
    }
   ],
   "source": [
    "# Code in file tensor/two_layer_net_tensor.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H1, H2, D_out = 64, 1000, 500, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H1, device=device)/10\n",
    "w2 = torch.randn(H1, H2, device=device)/10\n",
    "w3 = torch.randn(H2, D_out, device=device)/10\n",
    "\n",
    "w4 = w1.clone()\n",
    "w5 = w2.clone()\n",
    "w6 = w3.clone()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(50):\n",
    "    # Forward pass: compute predicted y\n",
    "    h1 = x.mm(w1)\n",
    "    h1_relu = h1.clamp(min=0)\n",
    "    h2 = h1_relu.mm(w2)\n",
    "    h2_relu = h2.clamp(min=0)\n",
    "    y_pred = h2_relu.mm(w3)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w3 = h2_relu.t().mm(grad_y_pred)\n",
    "    grad_h2_relu = grad_y_pred.mm(w3.t())\n",
    "\n",
    "    grad_h2 = grad_h2_relu.clone()\n",
    "    grad_h2[h2 < 0] = 0\n",
    "\n",
    "    grad_w2 = h1_relu.t().mm(grad_h2)\n",
    "    grad_h1_relu = grad_h2.mm(w2.t())\n",
    "    grad_h1 = grad_h1_relu.clone()\n",
    "    grad_h1[h1 < 0] = 0\n",
    "    \n",
    "    grad_w1 = x.t().mm(grad_h1)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    w3 -= learning_rate * grad_w3\n",
    "    \n",
    "    \n",
    "learning_rate = 1e-6\n",
    "print('**********************')\n",
    "for t in range(50):\n",
    "    # Forward pass: compute predicted y\n",
    "    h1 = x.mm(w4)\n",
    "    h1_relu = h1.clamp(min=0)\n",
    "    h2 = h1_relu.mm(w5)\n",
    "    h2_relu = h2.clamp(min=0)\n",
    "    y_pred = h2_relu.mm(w6)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w3 = h2_relu.t().mm(grad_y_pred)\n",
    "    grad_h2_relu = grad_y_pred.mm(w6.t()) * ((torch.sigmoid(h2)-0.5)/0.5)\n",
    "\n",
    "    grad_h2 = grad_h2_relu.clone()\n",
    "    grad_h2[h2 < 0] = 0\n",
    "\n",
    "    grad_w2 = h1_relu.t().mm(grad_h2)\n",
    "    grad_h1_relu = grad_h2.mm(w5.t()) * ((torch.sigmoid(h1)-0.5)/0.5)\n",
    "    grad_h1 = grad_h1_relu.clone()\n",
    "    grad_h1[h1 < 0] = 0\n",
    "    \n",
    "    grad_w1 = x.t().mm(grad_h1)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w4 -= learning_rate * grad_w1\n",
    "    w5 -= learning_rate * grad_w2\n",
    "    w6 -= learning_rate * grad_w3    \n",
    "    \n",
    "# # Randomly initialize weights\n",
    "# w3 = torch.randn(D_in, H, device=device)\n",
    "# print(w3.shape)\n",
    "# w4 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "# print('***************************')\n",
    "# learning_rate = 1e-6\n",
    "# for t in range(100):\n",
    "#     # Forward pass: compute predicted y\n",
    "#     h = x.mm(w3)\n",
    "#     h_relu = h.clamp(min=0)\n",
    "# #     print(h_relu.shape)\n",
    "#     y_pred = (h_relu.mm(w4))\n",
    "    \n",
    "#     # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "#     # of shape (); we can get its value as a Python number with loss.item().\n",
    "#     loss = (y_pred - y).pow(2).sum()\n",
    "#     print(t, loss.item())\n",
    "\n",
    "#     # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "#     grad_y_pred = 2.0 * (y_pred - y)\n",
    "# #     print(grad_y_pred.shape)\n",
    "#     grad_w4 = h_relu.t().mm(grad_y_pred)\n",
    "# #     print(grad_w4.shape)\n",
    "#     grad_h_relu = grad_y_pred.mm(w4.t()) * ((torch.sigmoid(h_relu) - 0.5)/0.5)\n",
    "# #     print(grad_h_relu.shape)\n",
    "#     grad_h = grad_h_relu.clone()\n",
    "#     grad_h[h < 0] = 0\n",
    "#     grad_w3 = x.t().mm(grad_h)\n",
    "\n",
    "#     # Update weights using gradient descent\n",
    "#     w3 -= learning_rate * grad_w3\n",
    "#     w4 -= learning_rate * grad_w4    \n",
    "# #     adfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d4dd926-6943-4caa-91f7-503dffd0460a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 48693164.0\n",
      "1 58294032.0\n",
      "2 63886544.0\n",
      "3 48188224.0\n",
      "4 22351706.0\n",
      "5 7506676.0\n",
      "6 3133998.5\n",
      "7 1974178.875\n",
      "8 1519780.625\n",
      "9 1243464.75\n",
      "10 1038837.875\n",
      "11 877102.75\n",
      "12 746366.625\n",
      "13 639411.625\n",
      "14 551005.8125\n",
      "15 477417.375\n",
      "16 415716.0625\n",
      "17 363589.9375\n",
      "18 319273.3125\n",
      "19 281406.5\n",
      "20 248910.34375\n",
      "21 220886.265625\n",
      "22 196595.203125\n",
      "23 175448.265625\n",
      "24 156995.71875\n",
      "25 140811.609375\n",
      "26 126597.90625\n",
      "27 114061.0390625\n",
      "28 102966.1640625\n",
      "29 93120.421875\n",
      "30 84359.0\n",
      "31 76548.5625\n",
      "32 69569.4921875\n",
      "33 63321.18359375\n",
      "34 57718.796875\n",
      "35 52688.89453125\n",
      "36 48160.58984375\n",
      "37 44078.359375\n",
      "38 40391.92578125\n",
      "39 37055.65625\n",
      "40 34031.0\n",
      "41 31285.09765625\n",
      "42 28788.962890625\n",
      "43 26515.794921875\n",
      "44 24443.375\n",
      "45 22552.43359375\n",
      "46 20824.763671875\n",
      "47 19245.14453125\n",
      "48 17800.16796875\n",
      "49 16475.767578125\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_custom_function.py\n",
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  We can implement our own custom autograd Functions by subclassing\n",
    "  torch.autograd.Function and implementing the forward and backward passes\n",
    "  which operate on Tensors.\n",
    "  \"\"\"\n",
    "  @staticmethod\n",
    "  def forward(ctx, x):\n",
    "    \"\"\"\n",
    "    In the forward pass we receive a context object and a Tensor containing the\n",
    "    input; we must return a Tensor containing the output, and we can use the\n",
    "    context object to cache objects for use in the backward pass.\n",
    "    \"\"\"\n",
    "    ctx.save_for_backward(x)\n",
    "    return x.clamp(min=0)\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive the context object and a Tensor containing\n",
    "    the gradient of the loss with respect to the output produced during the\n",
    "    forward pass. We can retrieve cached data from the context object, and must\n",
    "    compute and return the gradient of the loss with respect to the input to the\n",
    "    forward function.\n",
    "    \"\"\"\n",
    "    x, = ctx.saved_tensors\n",
    "    grad_x = grad_output.clone()\n",
    "    grad_x[x < 0] = 0\n",
    "    return grad_x\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and output\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(50):\n",
    "  # Forward pass: compute predicted y using operations on Tensors; we call our\n",
    "  # custom ReLU implementation using the MyReLU.apply function\n",
    "  y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    " \n",
    "  # Compute and print loss\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass.\n",
    "  loss.backward()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f1174-a637-409f-8e93-0b8de8624b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_retina",
   "language": "python",
   "name": "pytorch_retina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
